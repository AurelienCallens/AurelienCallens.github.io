[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRainfall with a Precision of 1km¬≤‚ÄîA Myth Becoming Reality?\n\n\n\n\n\n\nResearch\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\nDec 17, 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Value of Remote Sensing in Agriculture\n\n\n\n\n\n\nResearch\n\n\nEDA\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nThe Importance of Data Quality Control in Meteorology\n\n\n\n\n\n\nResearch\n\n\nAnomaly detection\n\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing my search for Data scientist jobs by scraping Indeed with R\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\nEDA\n\n\nNLP\n\n\n\n\n\n\n\n\n\nSep 21, 2022\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of the top R packages\n\n\n\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nCan R and Shiny make me a better fisherman? Part 4\n\n\nExploratory analysis on my fishing data (shiny and plotly)\n\n\n\nR\n\n\nShiny\n\n\nEDA\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\nAur√©lien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nThe end of my PhD journey\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nCan R and Shiny make me a better fisherman? Part 3\n\n\nUpdating my application\n\n\n\nR\n\n\nShiny\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\nAur√©lien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping Aliexpress with Rselenium\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\nNov 18, 2020\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nCan R and Shiny make me a better fisherman? Part 2\n\n\nExploratory analysis on my fishing data\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\nAur√©lien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nCan R and Shiny make me a better fisherman? Part 1\n\n\nBuilding a shiny application to store my fishing data\n\n\n\nR\n\n\nShiny\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\nSep 12, 2020\n\n\nAur√©lien Callens\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html",
    "title": "Analysis of physical and climatic forcings impacting the transport of micropollutants in the Adour estuary",
    "section": "",
    "text": "The aim of this work was to provide a descriptive synthesis of the physical and climatic forcings of the Adour (a french river) plume, namely swell, wind and flow. This synthesis aims to estimate the initial parameters of a hydrodynamic model of the Adour plume, which will be used to better better understand the transport of micropollutants in this estuary."
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html#summary",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html#summary",
    "title": "Analysis of physical and climatic forcings impacting the transport of micropollutants in the Adour estuary",
    "section": "",
    "text": "The aim of this work was to provide a descriptive synthesis of the physical and climatic forcings of the Adour (a french river) plume, namely swell, wind and flow. This synthesis aims to estimate the initial parameters of a hydrodynamic model of the Adour plume, which will be used to better better understand the transport of micropollutants in this estuary."
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html#productions",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html#productions",
    "title": "Analysis of physical and climatic forcings impacting the transport of micropollutants in the Adour estuary",
    "section": "Productions",
    "text": "Productions\n\nMy Master Thesis:\n\n Analysis of physical and climatic forcings impacting the transport of micropollutants in the Adour estuary (French version).\n\nShiny application:\n\n Check out here my first amazing (but laggy) Shiny application summarizing a part of my first internship.\n\nA published paper in a reviewed journal:\n\nMorichon, D., de Santiago, I., Delpey, M., Somdecoste, T., Callens, A., Liquet, B., ‚Ä¶ & Arnould, P. (2018). Assessment of flooding hazards at an engineered beach during extreme events: Biarritz, SW France. Journal of Coastal Research, 85(sp1), 801-805, (10.2112/SI85-161.1)."
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.html",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.html",
    "title": "Statistical learning for coastal risks assessment",
    "section": "",
    "text": "The objective of my work was to demonstrate how statistical learning methods can contribute to the improvement of coastal risk assessment tools and to the development of an early warning system which aims to reduce coastal flooding risk. I worked on 4 subjects:\n\nClustering on weather patterns\nMachine learning method to correct wave forecast\nDeep learning to make a wave impact database from wave monitoring images\nBayesian network to improve decision making for coastal risks"
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.html#summary",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.html#summary",
    "title": "Statistical learning for coastal risks assessment",
    "section": "",
    "text": "The objective of my work was to demonstrate how statistical learning methods can contribute to the improvement of coastal risk assessment tools and to the development of an early warning system which aims to reduce coastal flooding risk. I worked on 4 subjects:\n\nClustering on weather patterns\nMachine learning method to correct wave forecast\nDeep learning to make a wave impact database from wave monitoring images\nBayesian network to improve decision making for coastal risks"
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.html#productions",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.html#productions",
    "title": "Statistical learning for coastal risks assessment",
    "section": "Productions",
    "text": "Productions\n\nMy PhD thesis:\n\n Statistical learning for coastal risks assessment\n\nPresentation:\n\nPresentation for my thesis defense\n\nTwo published papers in reviewed journals:\n\nCallens, A., Morichon, D., Liria, P., Epelde, I., & Liquet, B. (2021). Automatic Creation of Storm Impact Database Based on Video Monitoring and Convolutional Neural Networks. Remote Sensing, 13(10), 1933, (10.3390/rs13101933).\nCallens, A., Morichon, D., Abadie, S., Delpey, M., Liquet, B. (2020). Using Random forest and Gradient boosting trees to improve wave forecast at a specific location. Applied Ocean Research, 104, (10.1016/j.apor.2020.102339).\n\nBlog post:\n\nThe end of my PhD journey"
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.html",
    "href": "projects/2018-09-01-robust-regression-time-series/index.html",
    "title": "Robust regression for time series exhibiting heteroscedasticity",
    "section": "",
    "text": "During this internship, I worked on a new statistical method to perform robust regression for time series exhibiting heteroscedasticity. We developed and tested this method on a dataset containing chlorophyll concentration in a small tributary of the Thames River (UK)."
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.html#summary",
    "href": "projects/2018-09-01-robust-regression-time-series/index.html#summary",
    "title": "Robust regression for time series exhibiting heteroscedasticity",
    "section": "",
    "text": "During this internship, I worked on a new statistical method to perform robust regression for time series exhibiting heteroscedasticity. We developed and tested this method on a dataset containing chlorophyll concentration in a small tributary of the Thames River (UK)."
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.html#productions",
    "href": "projects/2018-09-01-robust-regression-time-series/index.html#productions",
    "title": "Robust regression for time series exhibiting heteroscedasticity",
    "section": "Productions",
    "text": "Productions\n\nMy Master 2 Thesis:\n\n Robust regression for time series exhibiting heterogeneity\n\nDevelopement in the rlmDataDriven package:\n\nrlmDD_het.R : this function performs a robust regression which accounts for temporal correlations and heterogeneity.\nwhm.R : this function is the R implementation of the weighted M-estimation.\n\nA published paper in a reviewed journal:\n\nCallens, A., Wang, Y., Fu, L. et al.¬†(2020). Robust Estimation Procedure for Autoregressive Models with Heterogeneity. Environmental Modeling & Assessment, (10.1007/s10666-020-09730-w)"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.html",
    "href": "projects/2022-04-01-my-fishing-application/index.html",
    "title": "My fishing application",
    "section": "",
    "text": "In this project, I made a shiny application to record my fishing session with the aim to find the optimal weather condition to catch more fish. I made several blog post related to this project, don‚Äôt hesitate to check them out!"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.html#summary",
    "href": "projects/2022-04-01-my-fishing-application/index.html#summary",
    "title": "My fishing application",
    "section": "",
    "text": "In this project, I made a shiny application to record my fishing session with the aim to find the optimal weather condition to catch more fish. I made several blog post related to this project, don‚Äôt hesitate to check them out!"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.html#productions",
    "href": "projects/2022-04-01-my-fishing-application/index.html#productions",
    "title": "My fishing application",
    "section": "Productions",
    "text": "Productions\n\nBlog posts:\n\nBuilding a shiny application to store my fishing data\nExploratory analysis on my fishing data\nUpdating my application\nExploratory analysis on my fishing data (shiny and plotly)\n\nShiny applications\n\nDummy example of the fishing application I use \nShiny application presenting the results of my fishing season 2021"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.fr.html",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.fr.html",
    "title": "Est-ce que R et Shiny peuvent faire de moi un meilleur p√™cheur ? Partie 2",
    "section": "",
    "text": "Dans le pr√©c√©dent article de blog, j‚Äôai d√©crit en d√©tail comment j‚Äôai cr√©√© une application Shiny qui stocke les donn√©es de mes sessions de p√™che. Dans cet article, je vais explorer les donn√©es que j‚Äôai collect√©es au cours de l‚Äôann√©e derni√®re.\nPour r√©sumer, mon application stocke les donn√©es dans deux fichiers csv. Le premier contient des variables li√©es aux conditions de p√™che au d√©but et √† la fin de la session, telles que :\nLe second contient des informations sur mes prises :"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.fr.html#importation-et-nettoyage-de-mes-donn√©es-de-p√™che",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.fr.html#importation-et-nettoyage-de-mes-donn√©es-de-p√™che",
    "title": "Est-ce que R et Shiny peuvent faire de moi un meilleur p√™cheur ? Partie 2",
    "section": "Importation et nettoyage de mes donn√©es de p√™che",
    "text": "Importation et nettoyage de mes donn√©es de p√™che\nLa premi√®re √©tape de cette analyse consiste √† importer les deux fichiers csv et √† effectuer quelques transformations.\n\n\nShow the code\n# Change character variables to factor\nsession_data %&lt;&gt;% \nmutate_at(vars(Weather, Tide_status), as.factor)\n\n# Change character variables to factor\ncatch_data %&lt;&gt;% \nmutate_at(vars(species, lure, colour, length_lure), as.factor)\n\n\n¬†\nApr√®s avoir nettoy√© et r√©arrang√© les donn√©es (le code est cach√© ci-dessous), on peut explorer graphiquement les donn√©es !\n\n\nShow the code\n# Compute mean conditions (between beg and end session) \n\nmean_weather_cond &lt;- session_data %&gt;% \ngroup_by(Session) %&gt;% \nselect(-c(Long, Lat, Water_level, Tide_time)) %&gt;% \nsummarise_if(is.numeric, mean) \n\n\n# Extract fixed conditions and comments + join with mean cond \n\nfixed_cond_com &lt;- session_data %&gt;% \ngroup_by(Session) %&gt;% \nselect(Session, Comments, Long, Lat, Weather) %&gt;% \nmutate(Comments_parsed = paste(na.omit(Comments), collapse = \"\")) %&gt;% \nselect(-Comments) %&gt;% \nslice(1) %&gt;% \ninner_join(mean_weather_cond, by = \"Session\")\n\n# Create end and beg variables for WL, Time , Tide_time, Tide_status\n\nbeg_end_vars &lt;- session_data %&gt;% \nselect(Session, Status, Water_level, Time, Tide_time, Tide_status) %&gt;% \npivot_wider(names_from = Status,\nvalues_from = c(Time, Water_level,  Tide_time, Tide_status))\n\n\n# Assemble both file and calculate duration\n\ndat_ses &lt;-  inner_join(beg_end_vars,\nfixed_cond_com,\nby = \"Session\")\n\n# Calculate duration of the sessions\n\ndat_ses %&lt;&gt;% \nmutate(duration = round(difftime(Time_end,  Time_beg,  units = \"hours\"),\ndigits = 1))\n\ncatch_cond &lt;- full_join(dat_ses,\ncatch_data, by = c( \"Session\" = \"n_ses\" )) %&gt;% \nmutate(Session = factor(Session, levels = 1:length(dat_ses$Session)))\n\ncatch_cond %&lt;&gt;%\nmutate(Tide_status_ses = paste0(Tide_status_beg, \"_\", Tide_status_end))\n\n# Simplify the Tide status variable\n\ncatch_cond$Tide_status_ses &lt;- sapply(catch_cond$Tide_status_ses , function(x){switch(x, \n\"Up_Dead\" = \"Up\",\n\"Up_Up\" = \"Up\",\n\"Up_Down\" = \"Dead\",\n\"Down_Dead\" = \"Down\",\n\"Down_Up\" = \"Dead\",\n\"Down_Down\"  = \"Down\",\n\"Dead_Dead\" = \"Dead\",\n\"Dead_Up\" = \"Up\",\n\"Dead_Down\" = \"Down\"\n)}, USE.NAMES = F)"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.fr.html#exploration-graphique",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.fr.html#exploration-graphique",
    "title": "Est-ce que R et Shiny peuvent faire de moi un meilleur p√™cheur ? Partie 2",
    "section": "Exploration graphique",
    "text": "Exploration graphique\n\nMes lieux de p√™che\nOn peut visualiser les endroits o√π j‚Äôai le plus p√™ch√© en utilisant le package leaflet :\n\n\nShow the code\n# Calculate the number of fish caught by session \nfish_number &lt;-  catch_cond  %&gt;% na.omit() %&gt;% group_by(Session) %&gt;%  summarise(nb = length(Session))\n\n# Dataframe with variables we want to show on the map\nmap_data &lt;- catch_cond %&gt;% \ngroup_by(Session) %&gt;%\nselect(Session, Time_beg, Time_end, Long,\nLat, Water_level_beg, Tide_status_beg, Tide_time_beg, duration) \n\nmap_data &lt;- full_join(map_data, fish_number)\n\nmap_data$nb[is.na(map_data$nb)] &lt;- 0\n\n# Interactive map with Popup for each session\nlibrary(leaflet)\n\nleaflet(map_data, width = \"100%\") %&gt;% addTiles() %&gt;%\naddPopups(lng = ~Long, lat = ~Lat, \nwith(map_data, sprintf(\"&lt;b&gt;Session %.0f : %.1f h&lt;/b&gt; &lt;br/&gt; %s &lt;br/&gt; %.0f  fish &lt;br/&gt; Water level: %.0f m, %s, %.0f min since last peak\",                                         Session, duration,  Time_beg, nb, Water_level_beg, Tide_status_beg, Tide_time_beg)), \noptions = popupOptions(maxWidth = 100, minWidth = 50))\n\n\n\n\n\n\nComme vous pouvez le voir, je p√™che principalement dans la Nive, une rivi√®re qui traverse la ville de Bayonne.\n\n\nQuel est le meilleur moment pour p√™cher ?\n\nP√©riode de l‚Äôann√©e\nLe graphique suivant montre le nombre de poissons captur√©s en fonction de la p√©riode de l‚Äôann√©e :\n\n\nShow the code\ncatch_cond %&gt;% \ngroup_by(Session, Time_beg, .drop = F) %&gt;% \nna.omit() %&gt;% \nsummarise(n_catch = n()) %&gt;% \nright_join(unique(catch_cond[, c(\"Session\", \"Time_beg\")])) %&gt;% \nmutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;%\nggplot(aes(y = n_catch, x =Time_beg)) +\ngeom_point( size = 2) + \n  theme_minimal() + labs(x = \"Date\", y = \"Number of catch\") + scale_x_datetime(date_labels = \"%d/%m/%y\", date_breaks = \"3 months\") \n\n\n\n\n\n\n\n\n\nAvec de ce graphique, on constate que je ne suis pas all√© p√™cher durant l‚Äôautomne et l‚Äôhiver 2019, je n‚Äôai donc aucune donn√©e pour ces saisons. Et c‚Äôest b√™te pour moi car l‚Äôautomne est r√©put√© √™tre une excellente p√©riode pour la p√™che au bar! Je dois aller p√™cher cette ann√©e pour compenser ce manque de donn√©es. En hiver, la p√™che est vraiment compliqu√©e, car la grande majorit√© des bars retournent vers l‚Äôoc√©an.\n\n\nHeure de la journ√©e\nCe graphique montre le nombre de poissons captur√©s en fonction de l‚Äôheure de la journ√©e :\n\n\nShow the code\ncatch_cond %&gt;% \ngroup_by(Session, Time_beg, .drop = F) %&gt;% \nna.omit() %&gt;% \nsummarise(n_catch = n()) %&gt;% \nright_join(unique(catch_cond[, c(\"Session\", \"Time_beg\")])) %&gt;% \nmutate(n_catch = ifelse(is.na(n_catch), 0, n_catch ), \nhour = format(Time_beg, \"%H\")) %&gt;%\nggplot(aes(y = n_catch, x =hour)) +\ngeom_point( size = 2)  + labs(x = \"Hour\", y = \"Number of catch\")+\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nJe p√™che principalement apr√®s le travail ou en soir√©e. Pour tirer des conclusions pertinentes sur l‚Äôinfluence de l‚Äôheure de p√™che, je dois aller p√™cher √† diff√©rents moments de la journ√©e (le matin, par exemple).\n\n\nLa mar√©e\nLa mar√©e est un param√®tre important pour la p√™che en estuaire. Voyons l‚Äôeffet du courant de mar√©e sur mes prises :\n\n\nShow the code\nlibrary(ggpubr)\n\ngg1 &lt;- catch_cond %&gt;% \n  group_by(Session, Tide_status_ses, .drop = F)  %&gt;%  \n  drop_na() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Tide_status_ses\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;%\n  ggplot(aes(y = n_catch, x = Tide_status_ses, fill = Tide_status_ses)) +\n  geom_boxplot() +\n  labs(x = \"Status of tide current\", y = \"Number of catch\") +\n  theme_minimal()+ theme(legend.position=\"None\")\n\ngg2 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(y = length,x = Tide_status_ses, fill = Tide_status_ses)) +\n  geom_boxplot()+\n  labs(x = \"Status of tide current\", y = \"Length of the fish\") +\n  theme_minimal()+ theme(legend.position=\"None\")\n\nggarrange(gg1, gg2)\n\n\n\n\n\n\n\n\n\nIl semble que l‚Äô√©tat du courant de mar√©e n‚Äôinfluence pas le nombre de prises, mais qu‚Äôil affecte la taille des poissons. J‚Äôai tendance √† attraper des poissons plus gros lorsque le courant descend.\n\n\n\nLa lune affecte-t-elle mes r√©sultats de p√™che ?\nUne croyance largement r√©pandue chez les p√™cheurs est que la lune influence fortement le comportement des poissons. Les donn√©es sur la phase lunaire √©taient disponibles gr√¢ce √† l‚ÄôAPI m√©t√©o, j‚Äôai donc d√©cid√© d‚Äôenregistrer cette variable pour v√©rifier si cette croyance √©tait fond√©e.\nLes deux graphiques ci-dessous montrent le nombre et la taille des poissons en fonction de la phase de la lune (0 correspondant √† la nouvelle lune et 1 √† la pleine lune) :\n\n\nShow the code\ngg3 &lt;- catch_cond %&gt;% \n  group_by(Session, Moon, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Moon\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Moon)) +\n  geom_point( size = 2) +\n  labs(x = \"Moon phase\", y = \"Number of catch\")+\n  theme_minimal()\n\ngg4 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Moon)) +\n  geom_point( size = 2) +\n  geom_smooth(method=\"lm\", se=T) + \n  labs(x = \"Moon phase\", y = \"Length of the fish\")+\n  theme_minimal()\n\nggarrange(gg3, gg4)\n\n\n\n\n\n\n\n\n\nLa phase de la lune ne semble pas influencer le nombre de poissons que j‚Äôattrape lors d‚Äôune session. Cependant, j‚Äôai tendance √† attraper des poissons plus gros √† mesure que l‚Äôon se rapproche de la pleine lune. Pour confirmer cette observation, je dois continuer √† p√™cher afin de collecter plus de donn√©es !\n\n\nLa m√©t√©o influence-t-elle mes r√©sultats de p√™che ?\nOn peut examiner le nombre de poissons captur√©s dans diff√©rentes conditions m√©t√©orologiques :\n\n\nShow the code\n# precipitation probability \n\ngg5 &lt;- catch_cond %&gt;% \n  group_by(Session, Preci_prob, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Preci_prob\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Preci_prob)) +\n  geom_point()+\n  labs(x = \"Precipitation prob.\", y = \"Number of catch\")+\n  theme_minimal()\n\n# Atm pressure \n\ngg6 &lt;- catch_cond %&gt;% \n  group_by(Session, Atm_pres, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Atm_pres\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Atm_pres)) +\n  geom_point() +\n  labs(x = \"Atm. pressure\", y = \"Number of catch\")+\n  theme_minimal()\n\n#Air temp\n\ngg7 &lt;- catch_cond %&gt;% \n  group_by(Session, Air_temp, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Air_temp\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Air_temp)) +\n  geom_point() +\n  labs(x = \"Air temp.\", y = \"Number of catch\")+\n  theme_minimal()\n\n\n#Cloud cover\n\ngg8 &lt;- catch_cond %&gt;% \n  group_by(Session, Cloud_cover, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Cloud_cover\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Cloud_cover)) +\n  geom_point() +\n  labs(x = \"Cloud cover\", y = \"Number of catchh\")+\n  theme_minimal()\n\nggarrange(gg5, gg6, gg7, gg8)\n\n\n\n\n\n\n\n\n\nEn fonction de leur taille:\n\n\nShow the code\ngg15 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Preci_prob)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\ngg16 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Atm_pres)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\ngg17 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Air_temp)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\n\ngg18  &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Cloud_cover)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\nggarrange(gg15, gg16, gg17, gg18)\n\n\n\n\n\n\n\n\n\n√âtant donn√© que j‚Äôai des donn√©es limit√©es et que toutes les conditions m√©t√©orologiques ne sont pas couvertes, il est difficile de tirer des conclusions.\n\n\nQuels sont les meilleurs leurres pour attraper du poisson ?\n√Ä chaque prise, je remplis un petit formulaire dans mon application Shiny afin d‚Äôenregistrer les caract√©ristiques du leurre utilis√©. Il existe diff√©rents types de leurres ayant des nages sp√©cifiques, des couleurs et des tailles vari√©es. On peut repr√©senter le nombre de poissons captur√©s en fonction des caract√©ristiques du leurre :\n\n\nShow the code\nlevels(catch_cond$colour) &lt;- c(\"clear\", \"natural\", \"dark\")\nlevels(catch_cond$length_lure) &lt;- c(\"large\", \"medium\", \"small\")\n\ngg9 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot( aes(x=lure, fill = lure)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Type of lure\", y = \"\")+\n  theme_minimal()+ \n  theme(legend.position=\"None\")\n\ngg10 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot( aes(x=colour, fill = colour)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Color of the lures\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"BuPu\")+ \n  theme(legend.position=\"None\")\n\ngg11 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot( aes(x=length_lure, fill = length_lure)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Size of the lure\", y = \"\")+\n    scale_fill_brewer(palette=\"Dark2\")+\n  theme_minimal()+ theme(legend.position=\"None\")\n\nannotate_figure(ggarrange(gg9, gg10, gg11, ncol = 3),\n                left = text_grob(\"Number of catch\", rot = 90)\n)\n\n\n\n\n\n\n\n\n\nOn peut faire de m√™me pour la longueur des poissons captur√©s :\n\n\nShow the code\ngg12 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(y = length, x = lure, fill=lure)) +\n  geom_boxplot()+\n  labs(x = \"Type of lure\", y = \"\")+\n  theme_minimal()+ theme(legend.position=\"None\")\n\ngg13 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(y = length, x = colour, fill= colour)) +\n  geom_boxplot()+\n  labs(x = \"Color of the lures\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"BuPu\")+ theme(legend.position=\"None\")\n\ngg14 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(y = length, x = length_lure, fill=length_lure)) +\n  geom_boxplot()+\n  labs(x = \"Size of the lure\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"Dark2\")+ theme(legend.position=\"None\")\n\nannotate_figure(ggarrange(gg12, gg13, gg14, ncol = 3),\n                left = text_grob(\"Length of fish\", rot = 90)\n)\n\n\n\n\n\n\n\n\n\nAvec ces 6 graphiques, on peut voir que les leurres les plus efficaces pour moi sont les types shad et slug. Mention honorable jerkbait : je n‚Äôai attrap√© que 2 poissons avec, mais 2 gros (m√©diane autour de 47 cm). Les couleurs qui ont le mieux fonctionn√© sont les couleurs claires et naturelles. Pour la taille des leurres, les plus grands ont tendance √† attraper des poissons plus gros en moyenne. Ces conclusions doivent √™tre prises avec des pincettes, car je n‚Äôai pas enregistr√© le temps pass√© avec chaque leurre avant d‚Äôattraper un poisson. De plus, j‚Äôai tendance √† utiliser les m√™mes types et couleurs de leurres (par habitude), je vais me forcer √† varier davantage."
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.fr.html#conclusion",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.fr.html#conclusion",
    "title": "Est-ce que R et Shiny peuvent faire de moi un meilleur p√™cheur ? Partie 2",
    "section": "Conclusion",
    "text": "Conclusion\nL‚Äôanalyse de mes donn√©es de p√™che a √©t√© tr√®s int√©ressante et m‚Äôa apport√© des insights sur mon style de p√™che ! J‚Äôai compris que je p√™chais presque toujours de la m√™me mani√®re, avec les m√™mes habitudes. Bien que cela semble fonctionner pour moi, j‚Äôai une vision biais√©e de la fa√ßon d‚Äôattraper le bar europ√©en. Je dois utiliser des leurres plus grands pour attraper de plus gros poissons et varier les types de leurres utilis√©s. En effet, je p√™che la plupart du temps avec des leurres slug ou shad, d‚Äôo√π le plus grand nombre de prises avec ces types de leurres.\nJe vais continuer √† utiliser l‚Äôapplication pour collecter plus de donn√©es et mieux comprendre mes sessions de p√™che. Je vous tiendrai au courant des r√©sultats ! :wink:"
  },
  {
    "objectID": "posts/2024-12-17-rainfall-interpolation-deep-learning/index.fr.html",
    "href": "posts/2024-12-17-rainfall-interpolation-deep-learning/index.fr.html",
    "title": "Pr√©cipitations avec une pr√©cision de 1 km¬≤ : un mythe qui devient r√©alit√© ?",
    "section": "",
    "text": "Une fois de plus, j‚Äôai publi√© un nouvel article sur Medium ! Dans cet article, j‚Äôexplore un d√©fi passionnant en agriculture et en m√©t√©orologie : Peut-on atteindre la pr√©cision des pluviom√®tres tout en b√©n√©ficiant de la large couverture du t√©l√©d√©tection ? üåßüì°\n\nPourquoi les donn√©es spatiales sur les pr√©cipitations sont-elles importantes ?\nSans surprise, les pr√©cipitations influencent tout dans l‚Äôagriculture, de la planification de l‚Äôirrigation √† la gestion de la sant√© des cultures. Alors que les pluviom√®tres traditionnels fournissent des mesures locales tr√®s pr√©cises, ils ne captent pas les ‚Äúsch√©mas‚Äù (pattern) de pr√©cipitations sur de plus grandes r√©gions. Cela cr√©e des zones aveugles pour la prise de d√©cision. Les m√©thodes de t√©l√©detection comme le radar m√©t√©orologique et les satellites aident √† combler ces lacunes, mais leurs estimations des pr√©cipitations au niveau du sol sont beaucoup moins pr√©cise qu‚Äôune mesure directe.\n\n\nLe d√©fi et l‚Äôapproche\nChez Sencrop, on a relev√© ce d√©fi en d√©veloppant une m√©thode bas√©e sur l‚Äôapprentissage profond. Dans cet article, je pr√©sente une m√©thodologie innovante appel√©e densification, qui fusionne : - Des observations de pr√©cipitations (pr√©cises mais localis√©es) provenant de notre r√©seau de stations m√©t√©orologiques - Des estimations de pr√©cipitations (large couverture spatiale mais moins pr√©cises) provenant des radars et des satellites\nL‚Äôobjectif ? Fournir des donn√©es de pr√©cipitations √† haute r√©solution (1 km¬≤) partout en Europe, avec une pr√©cision √©gale ou sup√©rieure √† celle de notre r√©seau de stations.\nCurieux de voir comment on a rendu √ßa possible ? D√©couvre l‚Äôarticle complet ici : Rainfall with a Precision of 1km¬≤‚ÄîA Myth Becoming Reality?\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.fr.html",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.fr.html",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 3",
    "section": "",
    "text": "Dans cet article pr√©c√©dent, j‚Äôai pr√©sent√© l‚Äôapplication Shiny que j‚Äôai d√©velopp√©e pour enregistrer les donn√©es de mes sessions de p√™che. Dans cet article, je vais pr√©senter bri√®vement les modifications et les mises √† jour que j‚Äôai apport√©es pour am√©liorer l‚Äôapplication. Voici les principaux changements :"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.fr.html#api-m√©t√©o",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.fr.html#api-m√©t√©o",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 3",
    "section": "API M√©t√©o",
    "text": "API M√©t√©o\nDe petites modifications ont √©t√© apport√©es pour adapter la fonction m√©t√©o pr√©c√©dente √† la nouvelle API m√©t√©o. Comme cette nouvelle API ne fournit pas de donn√©es sur la phase de la lune, j‚Äôai d√©cid√© de calculer la phase de la lune avec le package oce :\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(oce)\n\nweather &lt;- function(lat, lon, API_key){\n  url &lt;- paste0(\"api.openweathermap.org/data/2.5/weather?lat=\", lat, \"&lon=\", lon, \"&appid=\", API_key, \"&units=metric\")\n  \n  rep &lt;- GET(url)\n  \n  table &lt;- fromJSON(content(rep, \"text\"))\n  \n  # The weather API don't provide moon phase so I compute it with Oce package\n  moon_phase &lt;- round(moonAngle(t = Sys.Date(),\n                                longitude = as.numeric(lon),\n                                latitude = as.numeric(lat))$illuminatedFraction,\n                      3)\n  \n  \n  current.weather.info &lt;- data.frame(Air_temp = table$main$temp,\n                                     Weather = table$weather$main,\n                                     Atm_pres = table$main$pressure,\n                                     Wind_str = table$wind$speed,\n                                     Wind_dir = table$wind$deg,\n                                     Cloud_cover = table$clouds$all,\n                                     PrecipInt = ifelse(is.null(table$rains$`1h`), 0, table$rains$`1h`),  \n                                     Moon = moon_phase)\n  return(current.weather.info)\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.fr.html#d√©bit-de-la-rivi√®re",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.fr.html#d√©bit-de-la-rivi√®re",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 3",
    "section": "D√©bit de la rivi√®re",
    "text": "D√©bit de la rivi√®re\nJ‚Äôai √©crit des fonctions pour scrapper des informations sur les d√©bits des rivi√®res dans lesquelles je p√™che le plus, sur un site web :\n\n# Get and prepare the flow data\nget_Qdata &lt;- function(link){\n  table &lt;- fromJSON(content(GET(link), \"text\"))\n  table &lt;- table$Serie$ObssHydro\n  table &lt;- as.data.frame(table)\n  table$DtObsHydro &lt;- sub(\"T\", \" \", table$DtObsHydro)\n  table$DtObsHydro &lt;- substr(table$DtObsHydro, start = 1, stop = 19)\n  ts &lt;- data.frame(Date = seq.POSIXt(as.POSIXct(range(table$DtObsHydro)[1],'%m/%d/%y %H:%M:%S'), \n                                     as.POSIXct(range(table$DtObsHydro)[2],'%m/%d/%y %H:%M:%S'), by=\"hour\"))\n  \n  table$DtObsHydro &lt;- as.POSIXct(table$DtObsHydro, format = \"%Y-%m-%d %H:%M:%S\")\n  \n  table &lt;- full_join(table, ts, by = c(\"DtObsHydro\" = \"Date\")) %&gt;% arrange(DtObsHydro)\n  return(table)\n}\n\n# Main function to collect river flow \n\nriver_flow &lt;- function(){\n  # Url of website to scrap:\n  url_index &lt;- \"https://www.vigicrues.gouv.fr/services/station.json/index.php\"\n  \n  rep &lt;- GET(url_index)\n  \n  table_index &lt;- fromJSON(content(rep, \"text\"))$Stations%&gt;% \n    na.omit()\n  \n  # I need to add the flow of several rivers to get the flow of the rivers I am interested in:\n  stations &lt;- table_index %&gt;% \n    filter(LbStationHydro %in% c(\"Pontonx-sur-l'Adour\", \"St-Pandelon\", \"Artiguelouve\", \"Escos\",\n                                 \"A√Øcirits [St-Palais]\", \"Cambo-les-Bains\"))\n  \n  base_url &lt;- \"http://www.vigicrues.gouv.fr/services/observations.json?CdStationHydro=\"\n  height_url &lt;- \"&FormatDate=iso\"\n  Q_url &lt;- \"&GrdSerie=Q\"\n  \n  stations &lt;- stations %&gt;% \n    mutate(WL_link = paste0(base_url, CdStationHydro, height_url),\n           Q_link = paste0(WL_link, Q_url))\n  \n  data_Q &lt;- lapply(stations$Q_link, \n                   function(x){get_Qdata(x)})\n  \n  data_Q &lt;- suppressWarnings(Reduce(function(...) merge(..., all = TRUE, by = \"DtObsHydro\"),\n                   data_Q))\n  \n  names(data_Q) &lt;- c(\"Date\", stations$LbStationHydro) \n  \n  data_Q &lt;- data_Q  %&gt;% \n    mutate(hour_of_day = format(Date, \"%Y-%m-%d %H\"))\n  \n  \n  data_Q &lt;- aggregate(.~hour_of_day, data = data_Q, mean, na.rm = TRUE, na.action = na.pass)\n  \n  data_Q &lt;- imputeTS::na_interpolation(data_Q, option = \"linear\")\n  \n  final_data &lt;- data_Q %&gt;% \n    mutate(Adour = `Pontonx-sur-l'Adour` +  `A√Øcirits [St-Palais]` + Artiguelouve + Escos + `St-Pandelon`,\n           Date = as.POSIXct(hour_of_day, tryFormats = \"%Y-%m-%d %H\")) %&gt;% \n    select(Date, `Cambo-les-Bains`, Adour) %&gt;% \n    rename(Nive = `Cambo-les-Bains`)\n  \n  Cur_flow &lt;- data.frame(\"Nive_c\" = final_data[nrow(final_data), 2],\n                         \"Adour_c\" = final_data[nrow(final_data), 3])\n  \n  \n  final_data &lt;- cbind(Cur_flow, final_data) %&gt;% \n    nest(Ts_flow = c(Date, Nive, Adour)) %&gt;% \n    mutate(Ts_flow = paste(Ts_flow))\n\n  return(final_data)\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.fr.html#application-shiny",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.fr.html#application-shiny",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 3",
    "section": "Application Shiny",
    "text": "Application Shiny\nUn graphique simplifi√© de la nouvelle application est montr√© ci-dessous :\n\n\n\nGraphique simplifi√© de la nouvelle application\n\n\n\nC√¥t√© UI\nLe c√¥t√© UI n‚Äôa pas beaucoup chang√©, j‚Äôai seulement supprim√© l‚Äôonglet qui affichait les donn√©es de p√™che sur une carte car je n‚Äôutilisais pas beaucoup cette fonctionnalit√© :\n\n# Load libraries \nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(googlesheets)\nlibrary(miniUI)\nlibrary(leaflet)\nlibrary(rdrop2)\nSys.setenv(TZ=\"Europe/Paris\")\n\n#Import the functions for weather API and webscrapping \nsuppressMessages(source(\"api_functions.R\"))\n\n\n# Load the dropbox token : \ntoken &lt;&lt;- readRDS(\"token.rds\")\n\n# Minipage for small screens\nui &lt;- miniPage(tags$script('$(document).ready(function () {\n                           navigator.geolocation.getCurrentPosition(onSuccess, onError);\n\n                           function onError (err) {\n                           Shiny.onInputChange(\"geolocation\", false);\n                           }\n\n                           function onSuccess (position) {\n                           setTimeout(function () {\n                           var coords = position.coords;\n                           console.log(coords.latitude + \", \" + coords.longitude);\n                           Shiny.onInputChange(\"geolocation\", true);\n                           Shiny.onInputChange(\"lat\", coords.latitude);\n                           Shiny.onInputChange(\"long\", coords.longitude);\n                           }, 1100)\n                           }\n                           });'),\n               \n               gadgetTitleBar(\"Catch them all\", left = NULL, right = NULL),\n               \n               miniTabstripPanel(\n                 \n                 miniTabPanel(\"Session\", icon = icon(\"sliders\"),\n                              \n                              miniContentPanel(uiOutput(\"UI_sess\", align = \"center\"),\n                                               uiOutput(\"UI\", align = \"center\"))\n                              \n                 )\n               )\n               \n)\n\n\n\nC√¥t√© serveur\nPlusieurs changements ont √©t√© apport√©s du c√¥t√© serveur pour collecter des donn√©es sur les leurres que j‚Äôutilise. D√©sormais, chaque fois que je change de leurre, je remplis un petit formulaire pour collecter les caract√©ristiques du leurre et cela ajoute une ligne dans un troisi√®me fichier csv :\n\nserver &lt;- function(input, output, session){\n  \n  observeEvent(input$go ,{\n    \n  # Read the csv file containing information about fishing session. If a session is running,\n  # display the UI that allows the user to input data about the fish caught. If a session is not started,\n  # display a button to start the session and small survey on lure characteristics.\n    \n    dat &lt;&lt;- drop_read_csv(\"/app_peche/session1.csv\", header = T, stringsAsFactors = F, dtoken = token)\n    \n    # Reactive UI\n    \n    output$UI &lt;- renderUI({\n      \n      if(!is.na(rev(dat$End)[1])){\n        # We now indicate what type of lure we use at the beginning of the session:\n        tagList(\n          selectInput(\"lure1\", \n                      label = \"Type de leurre\",\n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"),\n                      selected = \"shad\",\n                      selectize = FALSE),\n          \n          selectInput(\"color_lure1\", \n                      label = \"Couleur du leurre\",\n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ),\n                      selected = \"naturel\",\n                      selectize = FALSE),\n          \n          selectInput(\"length_lure1\",\n                      label = \"Taille du leurre\",\n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"),\n                      selected = \"petit\",\n                      selectize = FALSE),\n          \n          actionButton(\"go\",\"Commencer session !\"))\n      }else{\n        \n        tagList(actionButton(\"go\",\"End session\"))\n      }\n      \n    })\n    \n    output$UI_sess &lt;- renderUI({\n      \n      if(!is.na(rev(dat$End)[1])){\n        \n        tagList(textInput(\"comments\", label = \"Commentaire avant le d√©but?\", value = \"NA\"))\n        \n      }else{\n        input$catch\n        input$lure\n        tagList(\n          \n          selectInput(\"lure_type\", \n                      label = \"Type de leurre\",\n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"),\n                      selected = \"shad\",\n                      selectize = FALSE),\n          \n          selectInput(\"color_lure\", \n                      label = \"Couleur du leurre\",\n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ),\n                      selected = \"naturel\",\n                      selectize = FALSE),\n          \n          selectInput(\"length_lure\",\n                      label = \"Taille du leurre\",\n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"),\n                      selected = \"petit\",\n                      selectize = FALSE),\n          \n          actionButton(\"lure\",\n                       label = \"Changer de leurre!\"),\n          \n          br(), \n          br(), \n          \n          h4(\"Ajouter une capture\"),\n          \n          selectInput(\"species\", \n                      label = \"Esp√®ces\",\n                      choices = list(\"Bar\" = \"bar\",\n                                     \"Bar mouchet√©\" = \"bar_m\",\n                                     \"Alose\" = \"alose\",\n                                     \"Maquereau\" = \"maquereau\",\n                                     \"Chinchard\" = \"chinchard\"),\n                      selected = \"bar\"),\n          \n          sliderInput(\"length\",\n                      label = \"Taille du poisson\",\n                      value = 25, \n                      min = 0, \n                      max = 80, \n                      step = 1),\n          \n          actionButton(\"catch\",\"Rajoutez cette capture aux stats!\"),\n          \n          br(), \n          br(), \n          \n          textInput(\"comments1\", label = h4(\"Commentaire avant la fin ?\"), value = \"NA\")\n        )\n      }\n    })\n  }, ignoreNULL = F)\n  \n  \n  #If the button is pushed, create the line to be added in the csv file. \n  \n  observeEvent(input$go,{\n    \n    # Two outcomes depending if the session starts or ends. This gives the possibility \n    # to the user to add a comment before starting the session or after ending the session\n    \n    if(!is.na(rev(dat$End)[1])){\n      \n      #Tide + geoloc + Weather\n      c_tide &lt;- tide()\n      geoloc &lt;- c(input$lat,input$long)\n      current.weather.info &lt;- weather(lat = geoloc[1], lon = geoloc[2])\n      river.flow &lt;- river_flow()\n      \n      n_ses &lt;- c(rev(dat$Session)[1] + 1)\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment &lt;- input$comments\n      dat.f &lt;&lt;- cbind(data.frame(n_ses,\n                                 time_beg,\n                                 NA,\n                                 geoloc[2],\n                                 geoloc[1]),\n                      current.weather.info,\n                      c_tide,\n                      river.flow,\n                      comment)\n      names(dat.f) &lt;- names(dat)\n      print(dat.f)\n      final_dat &lt;- rbind(dat, dat.f)\n      \n      lure &lt;- drop_read_csv(\"/app_peche/lure.csv\",\n                            header = T,\n                            stringsAsFactors = F,\n                            dtoken = token)\n      \n      new_lure &lt;- data.frame(n_ses = n_ses,\n                             time = as.character(as.POSIXct(Sys.time())),\n                             type_lure = input$lure1,\n                             color_lure = input$color_lure1,\n                             length_lure = input$length_lure1)\n      \n      new_df &lt;- rbind(lure, \n                      new_lure)\n      \n      write_csv(as.data.frame(new_df), \"lure.csv\")\n      drop_upload(\"lure.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n      \n\n    }else{\n      \n      dat$End[nrow(dat)] &lt;- as.character(as.POSIXct(Sys.time()))\n      dat$Comments[nrow(dat)] &lt;- paste(dat$Comments[nrow(dat)], \"/\", input$comments1)\n      final_dat &lt;- dat \n    }\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(final_dat), \"session1.csv\")\n    \n    # Upload it to dropbox account \n    drop_upload(\"session1.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  # Add a line to the catch csv file whenever a fish is caught\n  observeEvent(input$catch,{\n    caugth &lt;- drop_read_csv(\"/app_peche/catch1.csv\", header = T, stringsAsFactors = F, dtoken = token)\n    \n    catch &lt;- data.frame(n_ses = dat$Session[nrow(dat)],\n                        time = as.character(as.POSIXct(Sys.time())),\n                        species = input$species,\n                        length = input$length)\n    \n    b &lt;- rbind(caugth,catch)\n    \n    write_csv(as.data.frame(b), \"catch1.csv\")\n    drop_upload(\"catch1.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  \n  observeEvent(input$lure,{\n    lure &lt;- drop_read_csv(\"/app_peche/lure.csv\",\n                          header = T,\n                          stringsAsFactors = F,\n                          dtoken = token)\n    \n    new_lure &lt;- data.frame(n_ses = dat$Session[nrow(dat)],\n                        time = as.character(as.POSIXct(Sys.time())),\n                        type_lure = input$lure_type,\n                        color_lure = input$color_lure,\n                        length_lure = input$length_lure)\n    \n    new_df &lt;- rbind(lure, \n               new_lure)\n    \n    write_csv(as.data.frame(new_df), \"lure.csv\")\n    drop_upload(\"lure.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.fr.html#conclusion",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.fr.html#conclusion",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 3",
    "section": "Conclusion",
    "text": "Conclusion\nJ‚Äôai test√© cette nouvelle application lors de deux sessions de p√™che et elle fonctionne √† merveille. J‚Äôai h√¢te de vous pr√©senter mes r√©sultats √† la fin de cette saison de p√™che !"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.fr.html",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.fr.html",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 1",
    "section": "",
    "text": "‚ÑπÔ∏è Note: Lorsque j‚Äôai d√©velopp√© cette application, j‚Äô√©tais d√©butant en d√©veloppement web et en gestion des donn√©es. J‚Äôai choisi Shiny car c‚Äô√©tait une solution simple pour moi √† l‚Äô√©poque.\nAvec le recul, si je devais refaire ce projet aujourd‚Äôhui, j‚Äôopterais plut√¥t pour une application Django avec une base de donn√©es d√©di√©e, ce qui offrirait plus de flexibilit√© et de robustesse.\nMon passe-temps favori, en plus de R bien s√ªr, est la p√™che. La plupart du temps, je p√™che le bar (Dicentrarchus labrax) dans les estuaires. Le bar est un pr√©dateur qui a un large √©ventail de proies : crabes, lan√ßons, crevettes, gambas et autres poissons. Pour p√™cher le bar, je n‚Äôutilise pas d‚Äôapp√¢ts vivants, je pr√©f√®re utiliser des leurres artificiels qui imitent une proie sp√©cifique.\nEn th√©orie, attraper un poisson est assez simple :\nEn pratique, c‚Äôest une autre histoire ! En effet, l‚Äôactivit√© alimentaire, la position du bar dans l‚Äôestuaire et ses proies varient en fonction de plusieurs param√®tres :\nComme vous l‚Äôavez compris, de nombreux param√®tres peuvent potentiellement influencer les r√©sultats de mes sessions de p√™che. C‚Äôest pourquoi j‚Äôai d√©cid√© de cr√©er une application Shiny pour augmenter le nombre et la taille des poissons captur√©s durant mes sessions. Pour atteindre cet objectif, je dois mieux comprendre l‚Äôactivit√©, la position et les proies du bar en fonction des param√®tres d√©crits ci-dessus."
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.fr.html#exigences-de-mon-application",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.fr.html#exigences-de-mon-application",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 1",
    "section": "Exigences de mon application",
    "text": "Exigences de mon application\n\nElle doit stocker les donn√©es de mes sessions de p√™che :\n\n\n\n\n\n\n\n\n\nInformations n√©cessaires\nDescription des variables\nSource des donn√©es\n\n\n\n\nTemps\nHeure √† laquelle un poisson est captur√©, dur√©e √©coul√©e depuis le d√©but de la session\nR\n\n\nPrise\nEsp√®ce et taille du poisson captur√©\nG√©olocalisation via smartphone ?\n\n\nLeurres\nType, longueur, couleur du leurre utilis√©\nAPI m√©t√©o\n\n\n\n\nElle doit enregistrer les donn√©es sur mes prises et les leurres artificiels utilis√©s :\n\n\n\n\n\n\n\n\n\nInformations n√©cessaires\nDescription des variables\nSource des donn√©es\n\n\n\n\nTemps\nHeure √† laquelle un poisson est captur√©, dur√©e √©coul√©e depuis le d√©but de la session\nR\n\n\nPrise\nEsp√®ce et taille du poisson captur√©\nSaisie utilisateur\n\n\nLeurres\nType, longueur, couleur du leurre utilis√©\nSaisie utilisateur\n\n\n\n\nElle doit √™tre adapt√©e aux petits √©crans, car je l‚Äôutiliserai toujours sur mon t√©l√©phone.\nElle doit rester gratuite."
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.fr.html#collecte-des-donn√©es",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.fr.html#collecte-des-donn√©es",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 1",
    "section": "Collecte des donn√©es",
    "text": "Collecte des donn√©es\n\nR√©cup√©ration de ma position GPS\nMa position GPS est collect√©e gr√¢ce √† un peu de code Javascript int√©gr√© dans l‚Äôen-t√™te de l‚Äôapplication Shiny. Ce code a √©t√© d√©velopp√© par AugusT et est disponible sur son d√©p√¥t GitHub.\n\n\nAPI m√©t√©o\nPour les donn√©es m√©t√©orologiques, j‚Äôai trouv√© une API gratuite appel√©e Dark Sky. J‚Äôai d√©velopp√© une fonction qui prend en entr√©e les coordonn√©es d‚Äôun lieu ainsi que la cl√© utilisateur de l‚ÄôAPI et retourne les conditions m√©t√©orologiques actuelles sous forme de dataframe :\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(rvest)\n\nweather &lt;- function(x, API_key){\n  url &lt;- paste0(\"https://api.darksky.net/forecast/\",API_key,\n                \"/\", x[1], \",\", x[2],\n                \"?units=ca&exclude=hourly,alerts,flags\")\n  \n  rep &lt;- GET(url)\n  \n  table &lt;- fromJSON(content(rep, \"text\"))\n  \n  current.weather.info &lt;- with(table,\n                               data.frame(Air_temp = currently$temperature,\n                                     Weather = currently$summary,\n                                     Atm_pres = currently$pressure,\n                                     Wind_str = currently$windSpeed,\n                                     Wind_dir = currently$windBearing,\n                                     Cloud_cover = currently$cloudCover,\n                                     PrecipProb = currently$precipProbability,\n                                     PrecipInt = currently$precipIntensity,  \n                                     Moon = daily$data$moonPhase[1]))\n  return(current.weather.info)\n}\n\n\n\nWeb scraping des donn√©es de mar√©e\nJ‚Äôai cr√©√© une fonction pour r√©cup√©rer des informations sur les mar√©es √† partir d‚Äôun site web fran√ßais. La fonction suivante ne prend aucun argument et retourne le niveau d‚Äôeau actuel, l‚Äô√©tat de la mar√©e (montante ou descendante) ainsi que le temps √©coul√© depuis le dernier pic de mar√©e pour le lieu o√π je p√™che.\n\ntide &lt;- function(){\n  \n  # Set the current time and time zone \n  Sys.setenv(TZ=\"Europe/Paris\")\n  time &lt;- as.POSIXct(Sys.time())\n  url &lt;- \"https://services.data.shom.fr/hdm/vignette/grande/BOUCAU-BAYONNE?locale=en\"\n  \n  # Read the web page that contains the tide data \n  text &lt;- url %&gt;% \n    read_html() %&gt;%\n    html_text()\n  \n  # Clean the html data to get a dataframe  with two cols Time and water level: \n\n  text &lt;- as.character(sub(\".*var data = *(.*?) *\\\\;.*\", \"\\\\1\", text))\n  text &lt;- unlist(str_split( substr(text, 1, nchar(text)-2), \"\\\\],\"))\n  tidy_df &lt;- data.frame(hour=NA,Water=NA)\n  \n  for(i in 1:length(text)){\n    text_dat &lt;- unlist(str_split(text[i], '\"'))[c(2,3)]\n    text_dat[1] &lt;- substr(text_dat[1], 1, nchar(text_dat[1])-1)\n    text_dat[2] &lt;- as.numeric(substr(text_dat[2], 2, nchar(text_dat[2])))\n    tidy_df[i,] &lt;- text_dat\n  }\n  \n  tidy_df$hour &lt;- as.POSIXct(paste(format(Sys.time(),\"%Y-%m-%d\"), tidy_df$hour))\n  \n  # Some lines to get the tide status (going down or up) : \n  \n  n_closest &lt;- which(abs(tidy_df$hour - time) == min(abs(tidy_df$hour - time)))\n  \n  water_level &lt;- as.numeric(tidy_df[n_closest, 2])\n  \n  all_decrea &lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==\n                      cummin(tidy_df$Water[(n_closest-6):(n_closest+6)] ))\n  \n  all_increa &lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==\n                      cummax(tidy_df$Water[(n_closest-6):(n_closest+6)] ))\n  \n  maree &lt;- ifelse(all_decrea, \"Down\", ifelse(all_increa, \"Up\", \"Dead\"))\n  \n  \n  # Compute time since the last peak :\n  \n  last_peak &lt;- max(cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &gt; 0)$lengths)\n                   [cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &gt;0)$lengths) &lt; n_closest])\n  \n  \n  time_after &lt;- as.numeric(difftime(tidy_df$hour[n_closest], tidy_df$hour[last_peak], units = \"mins\"))\n  \n  \n  # Return the list with the results :\n  \n  return(list(Water_level = water_level,\n              Maree = maree,\n              Time_peak = time_after))\n  \n}"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.fr.html#lapplication-shiny",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.fr.html#lapplication-shiny",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 1",
    "section": "L‚Äôapplication Shiny",
    "text": "L‚Äôapplication Shiny\nLe principal probl√®me que j‚Äôai rencontr√© lors du d√©veloppement de cette application √©tait le stockage des donn√©es. Shinyapps.io h√©berge gratuitement votre application Shiny, mais j‚Äôai rencontr√© des probl√®mes lorsque j‚Äôai utilis√© l‚Äôapplication pour modifier les fichiers CSV.\nLa solution que j‚Äôai trouv√©e a √©t√© de stocker les donn√©es sur mon compte Dropbox. Vous pouvez trouver ici plus de d√©tails sur le sujet ainsi que des solutions alternatives. J‚Äôai utilis√© le package rdrop2 pour acc√©der et modifier les donn√©es via l‚Äôapplication Shiny.\nVoici les principales √©tapes de cette application :\n\nAu d√©marrage de l‚Äôapplication, un fichier CSV stock√© sur mon Dropbox est lu afin de v√©rifier si une session de p√™che est en cours ou non. Si ce n‚Äôest pas le cas, l‚Äôutilisateur peut d√©marrer une session de p√™che.\nLors du d√©marrage d‚Äôune nouvelle session, une ligne contenant les coordonn√©es, les conditions m√©t√©orologiques et les conditions de mar√©e est ajout√©e au fichier CSV mentionn√© pr√©c√©demment.\nSi un poisson est p√™ch√©, l‚Äôutilisateur peut remplir un formulaire pour enregistrer les donn√©es dans un second fichier CSV. Ce fichier contient : l‚Äôheure, l‚Äôesp√®ce et la longueur du poisson ainsi que des informations sur le leurre utilis√© (type, couleur, longueur).\nL‚Äôutilisateur peut mettre fin √† la session de p√™che en appuyant sur un bouton. Cela enregistre l‚Äôheure de fin, les conditions m√©t√©orologiques et les conditions de mar√©e dans le premier fichier CSV.\n\nUn sch√©ma simplifi√© est pr√©sent√© ci-dessous :\n\n\n\nSimplified workflow of the application\n\n\n\nC√¥t√© interface utilisateur (UI)\nL‚Äôinterface utilisateur de l‚Äôapplication est construite en utilisant le package miniUI. Ce package permet aux utilisateurs de R de d√©velopper des applications Shiny adapt√©es aux petits √©crans.\n\n# Load libraries \nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(googlesheets)\nlibrary(miniUI)\nlibrary(leaflet)\nlibrary(rdrop2)\nSys.setenv(TZ=\"Europe/Paris\")\n\n#Import the functions for weather API and webscrapping \nsuppressMessages(source(\"api_functions.R\"))\n\n\n# Load the dropbox token : \ntoken &lt;&lt;- readRDS(\"token.rds\")\n\n# Minipage for small screens\nui &lt;- miniPage(\n  # Javascript that give user location (input$lat,input$long)\n  tags$script('$(document).ready(function () {\n                           navigator.geolocation.getCurrentPosition(onSuccess, onError);\n                           \n                           function onError (err) {\n                           Shiny.onInputChange(\"geolocation\", false);\n                           }\n                           \n                           function onSuccess (position) {\n                           setTimeout(function () {\n                           var coords = position.coords;\n                           console.log(coords.latitude + \", \" + coords.longitude);\n                           Shiny.onInputChange(\"geolocation\", true);\n                           Shiny.onInputChange(\"lat\", coords.latitude);\n                           Shiny.onInputChange(\"long\", coords.longitude);\n                           }, 1100)\n                           }\n                           });'),\n  \n  gadgetTitleBar(\"Catch them all\", left = NULL, right = NULL),\n  \n  miniTabstripPanel(\n    #First panel depends if a fishing session is started or not \n    miniTabPanel(\"Session\", icon = icon(\"sliders\"),\n                 miniContentPanel(uiOutput(\"UI_sess\", align = \"center\"),\n                                  uiOutput(\"UI\", align = \"center\"))\n    ),\n    # Second panel displays the location of the previous fishing session with the number of fish caught \n    miniTabPanel(\"Map\", icon = icon(\"map-o\"),\n                 miniContentPanel(scrollable = FALSE,padding = 0,\n                                  div(style=\"text-align:center\",\n                                      prettyRadioButtons(\"radio\", inline = TRUE, label = \"\",\n                                                         choices = list(\"3 derni√®res sessions\" = 1,\n                                                                        \"3 Meilleures Sessions\" = 2,\n                                                                        \"Tout afficher\" = 3), \n                                                         selected = 1)),\n                                  leafletOutput(\"map\", height = \"93%\")\n                 ))\n  )\n  \n)\n\n\n\nC√¥t√© serveur\nLe c√¥t√© serveur est principalement compos√© de fonctions observeEvent. L‚Äôutilit√© de chaque observeEvent est indiqu√©e dans le script sous forme de commentaires.\n\nserver &lt;- function(input, output, session){\n  source(\"api_functions.R\")\n  \n  # Read the csv file containing information about fishing session. If a session is running,\n  # display the UI that allows the user to input data about the fish caught. If a session is not started,\n  # display a button to start the session.\n  \n  observeEvent(input$go ,{\n    \n    dat &lt;&lt;- drop_read_csv(\"/app_peche/session.csv\", header = T, stringsAsFactors = F, dtoken = token) \n    \n    output$UI&lt;- renderUI({\n      tagList(\n        if(rev(dat$Status)[1] == \"end\"){\n          actionButton(\"go\",\"Start session\")}\n        else{\n          actionButton(\"go\",\"End session\") \n        }\n      )\n    })\n    \n    output$UI_sess&lt;- renderUI({\n      if(rev(dat$Status)[1] == \"end\"){\n        tagList(textInput(\"comments\", label = h3(\"Commentaires\"), value = \"NA\"))\n      }else{\n        input$catch\n        \n        tagList(\n          selectInput(\"species\", label = h3(\"Esp√®ces\"), \n                      choices = list(\"Bar\" = \"bar\", \n                                     \"Bar mouchet√©\" = \"bar_m\", \n                                     \"Alose\" = \"alose\",\n                                     \"Alose Feinte\" = \"alose_f\",\n                                     \"Maquereau\" = \"maquereau\", \n                                     \"Chinchard\" = \"chinchard\"), selected = \"bar\"),\n          \n          sliderInput(\"length\",label = h3(\"Taille du poisson\"),value=25,min=0,max=80, step=1),\n          \n          selectInput(\"lure\", label = h3(\"Type de leurre\"), \n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"), selectize = FALSE),\n          \n          selectInput(\"color_lure\", label = h3(\"Couleur du leurre\"), \n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ), selectize = FALSE),\n          \n          selectInput(\"length_lure\", label = h3(\"Taille du leurre\"), \n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"), selectize = FALSE),\n          \n          actionButton(\"catch\",\"Rajoutez cette capture aux stats!\"),\n          \n          textInput(\"comments1\", label = h3(\"Commentaire avant la fin ?\"), value = \"NA\")\n          \n          \n        )\n        \n        \n      }\n      \n    })  \n    \n    \n  }, ignoreNULL = F)\n  \n  #If the button is pushed, create the line to be added in the csv file. \n  \n  observeEvent(input$go,{\n    \n    #Tide + geoloc + Weather\n    c_tide &lt;- unlist(tide())\n    geoloc &lt;- c(input$lat,input$long)\n    current.weather.info &lt;- weather(geoloc) \n    \n    # Two outcomes depending if the session starts or ends. This gives the possibility \n    # to the user to add a comment before starting the session or after ending the session\n    \n    if(rev(dat$Status)[1] == \"end\"){\n      \n      n_ses &lt;- c(rev(dat$Session)[1]+1)\n      stat_ses &lt;- c(\"beg\")\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment &lt;- input$comments\n      dat.f &lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment)\n      names(dat.f)&lt;-names(dat)\n      a &lt;- rbind(dat,dat.f)\n      \n    }else{\n      \n      n_ses &lt;- c(rev(dat$Session)[1])\n      stat_ses &lt;- c(\"end\")\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment1 &lt;- input$comments1\n      dat.f&lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment1)\n      names(dat.f)&lt;-names(dat)\n      a &lt;- rbind(dat,dat.f)\n    }\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(a), \"session.csv\")\n    \n    # Upload it to dropbox account \n    drop_upload(\"session.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  \n  # Add a line to the catch csv file whenever a fish is caught\n  observeEvent(input$catch,{\n    caugth &lt;- drop_read_csv(\"/app_peche/catch.csv\", header = T, stringsAsFactors = F, dtoken = token) \n    \n    n_ses &lt;- c(rev(dat$Session)[1])\n    time &lt;- as.POSIXct(Sys.time())\n    time_after_beg &lt;- round(as.numeric(difftime(time, rev(dat$Time)[1], units = \"mins\")), digits = 0)\n    \n    catch &lt;- data.frame(n_ses, \n                        time = as.character(time),\n                        min_fishing = as.character(time_after_beg),\n                        species = input$species,\n                        length = input$length,\n                        lure = input$lure,\n                        colour = input$color_lure,\n                        length_lure = input$length_lure)\n    \n    b &lt;- rbind(caugth,catch)\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(b), \"catch.csv\")\n    # Upload it to dropbox account \n    drop_upload(\"catch.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  # Create the map with the results of previous session depending on the choice of the user :\n  \n  observeEvent(input$radio,{\n    \n    output$map &lt;- renderLeaflet({\n      map_data &lt;- map_choice(input$radio)\n      leaflet(map_data) %&gt;% addTiles() %&gt;%\n        addPopups(lng = ~Long,\n                  lat = ~Lat, \n                  with(map_data,\n                       sprintf(\"&lt;b&gt;Session %.0f : %.1f h&lt;/b&gt; &lt;br/&gt; %s &lt;br/&gt; %.0f  poissons &lt;br/&gt; hauteur d'eau: %.0f m, %s, %.0f min apr√®s l'√©tal\",\n                               n_ses,\n                               duration,\n                               Time,\n                               nb,\n                               Water_level,\n                               Tide_status,\n                               Tide_time)),\n                  options = popupOptions(maxWidth = 100, minWidth = 50))\n    })\n    \n  })\n  \n}"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.fr.html#conclusion-et-am√©liorations-futures",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.fr.html#conclusion-et-am√©liorations-futures",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 1",
    "section": "Conclusion et am√©liorations futures",
    "text": "Conclusion et am√©liorations futures\nVous pouvez trouver un exemple de d√©monstration de cette application (non connect√©e au compte Dropbox)\nici.\nJ‚Äôutilise cette application depuis un an sans aucun probl√®me ! Les donn√©es que j‚Äôai collect√©es seront pr√©sent√©es dans le prochain article.\nDans les mois √† venir, je dois trouver une nouvelle API gratuite pour remplacer l‚Äôactuelle. En effet, l‚ÄôAPI m√©t√©o que j‚Äôutilise a √©t√© rachet√©e par Apple et les requ√™tes gratuites seront arr√™t√©es l‚Äôann√©e prochaine."
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.fr.html",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.fr.html",
    "title": "La fin de ma th√®se",
    "section": "",
    "text": "Il y a deux semaines, j‚Äôai d√©fendu ma th√®se intitul√©e ‚ÄòApprentissage statistique pour l‚Äô√©valuation des risques c√¥tiers‚Äô devant un jury acad√©mique. La soutenance s‚Äôest tr√®s bien pass√©e et le jury ainsi que l‚Äôauditoire √©taient r√©ellement int√©ress√©s par le travail que j‚Äôai accompli durant ma th√®se. Cette th√®se a √©t√© un voyage incroyable et enrichissant tant en termes de connaissances et de recherche, que de collaboration et d‚Äô√©changes. Je suis maintenant impatient de commencer un nouveau chapitre et de d√©couvrir de nouveaux sujets de travail !\nVous pouvez trouver le manuscrit de ma th√®se ici :  http://www.theses.fr/2021PAUU3016. Si vous n‚Äôavez pas le courage de lire le manuscrit entier (je comprends :wink:), vous trouverez ci-dessous un court r√©sum√© de ma th√®se ainsi que les diapositives que j‚Äôai pr√©sent√©es lors de ma soutenance de doctorat."
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.fr.html#r√©sum√©-de-ma-th√®se",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.fr.html#r√©sum√©-de-ma-th√®se",
    "title": "La fin de ma th√®se",
    "section": "R√©sum√© de ma th√®se",
    "text": "R√©sum√© de ma th√®se\nAu cours des derni√®res d√©cennies, la quantit√© de donn√©es li√©es aux risques c√¥tiers a consid√©rablement augment√© avec l‚Äôinstallation de nombreux r√©seaux de surveillance. √Ä cette √©poque de big data, l‚Äôutilisation des m√©thodes d‚Äôapprentissage statistique (MAS) dans le d√©veloppement de mod√®les pr√©dictifs locaux devient de plus en plus l√©gitime et justifi√©e. L‚Äôobjectif de cette th√®se est de d√©montrer comment les MAS peuvent contribuer √† l‚Äôam√©lioration des outils d‚Äô√©valuation des risques c√¥tiers et au d√©veloppement d‚Äôun syst√®me d‚Äôalerte pr√©coce visant √† r√©duire le risque d‚Äôinondation c√¥ti√®re.\nTrois m√©thodologies ont √©t√© d√©velopp√©es et test√©es sur des sites d‚Äô√©tude r√©els. La premi√®re m√©thodologie vise √† am√©liorer la pr√©vision locale des vagues r√©alis√©e par un mod√®le de vagues spectrales avec des m√©thodes d‚Äôapprentissage automatique et des donn√©es provenant des r√©seaux de surveillance. Nous avons montr√© que l‚Äôassimilation de donn√©es avec des m√©thodes d‚Äôapprentissage automatique am√©liore significativement la pr√©vision des param√®tres de vagues, notamment la hauteur et la p√©riode des vagues. La deuxi√®me m√©thodologie concerne la cr√©ation de bases de donn√©es sur les impacts des temp√™tes. Bien que ces bases de donn√©es soient essentielles pour le processus de r√©duction des risques de catastrophe, elles sont rares et dispers√©es. Nous avons donc propos√© une m√©thodologie bas√©e sur une m√©thode d‚Äôapprentissage profond (r√©seaux neuronaux convolutifs) pour g√©n√©rer automatiquement des donn√©es qualitatives sur l‚Äôimpact des temp√™tes √† partir d‚Äôimages fournies par des stations de surveillance vid√©o install√©es sur la c√¥te. La derni√®re m√©thodologie concerne le d√©veloppement d‚Äôun mod√®le d‚Äôimpact des temp√™tes avec une m√©thode statistique (r√©seau bay√©sien) bas√© exclusivement sur les donn√©es acquises par divers r√©seaux de surveillance. Gr√¢ce √† cette m√©thodologie, nous avons pu pr√©dire qualitativement l‚Äôimpact des temp√™tes sur notre site d‚Äô√©tude, la Grande Plage de Biarritz.\n\n\n\nOrganisation du manuscrit de ma th√®se"
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.fr.html#ma-pr√©sentation-pour-la-soutenance-de-th√®se",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.fr.html#ma-pr√©sentation-pour-la-soutenance-de-th√®se",
    "title": "La fin de ma th√®se",
    "section": "Ma pr√©sentation pour la soutenance de th√®se",
    "text": "Ma pr√©sentation pour la soutenance de th√®se\nVous pouvez trouver ma pr√©sentation ici"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "",
    "text": "Il y a quelques semaines, j‚Äôai commenc√© √† chercher un poste de data scientist dans l‚Äôindustrie. Mes premi√®res actions ont √©t√© de :\nApr√®s avoir lu de nombreuses annonces et travaill√© plusieurs heures sur mon CV, je me suis demand√© si je pouvais optimiser ces √©tapes avec R et Data Science. J‚Äôai donc d√©cid√© de scraper Indeed et d‚Äôanalyser les donn√©es des offres de data science pour :"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#chargement-des-biblioth√®ques",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#chargement-des-biblioth√®ques",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Chargement des biblioth√®ques",
    "text": "Chargement des biblioth√®ques\nLa premi√®re √©tape est d‚Äôimporter plusieurs packages :\n\n# General\nlibrary(tidyverse)\n# Webscraping \nlibrary(rvest)\nlibrary(RSelenium)\n# Geo data\nlibrary(tidygeocoder)\nlibrary(leaflet)\nlibrary(rnaturalearth)\nlibrary(sf)\n# NLP\nlibrary(udpipe)\nlibrary(textrank)\nlibrary(wordcloud)\n# Cleaning\nlibrary(stringr)\n# Additional functions presented at the end of the post \nsource('scraping_functions.R')"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#collecter-les-donn√©es-avec-le-web-scraping",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#collecter-les-donn√©es-avec-le-web-scraping",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Collecter les donn√©es avec le web scraping",
    "text": "Collecter les donn√©es avec le web scraping\nAu d√©but de ce projet, j‚Äôutilisais read_html() de rvest pour acc√©der et t√©l√©charger la page web d‚ÄôIndeed. Cependant, les pages Indeed sont prot√©g√©es par un logiciel anti-scraping qui bloquait toutes mes demandes, m√™me si le scraping n‚Äôest pas interdit sur les pages qui m‚Äôint√©ressent (j‚Äôai v√©rifi√© la page robots.txt).\nC‚Äôest pourquoi j‚Äôai d√©cid√© d‚Äôacc√©der aux pages avec Rselenium qui permet d‚Äôex√©cuter un navigateur sans t√™te (‚Äúheadless‚Äù). On commence par naviguer vers la page correspondant aux r√©sultats de recherche des offres de Data Scientist en France :\n\nurl = \"https://fr.indeed.com/jobs?q=data%20scientist&l=France&from=searchOnHP\"\n\n# Headless Firefox browser\nexCap &lt;- list(\"moz:firefoxOptions\" = list(args = list('--headless')))\nrD &lt;- rsDriver(browser = \"firefox\", extraCapabilities = exCap, port=1111L,\n                verbose = F)\nremDr &lt;- rD$client\n\n# Navigate to the url\nremDr$navigate(url)\n\n# Store page source \nweb_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n\nPour scraper une information sp√©cifique sur une page web, voici les √©tapes √† suivre :\n\nTrouver l‚Äô√©l√©ment/texte/donn√©e que vous souhaitez scraper sur la page web.\nTrouver le xpath ou le s√©lecteur CSS associ√© en utilisant l‚Äôoutil de d√©veloppement de Chrome ou Firefox (tutoriel ici !).\nExtraire l‚Äô√©l√©ment avec html_element() en indiquant le xpath ou le s√©lecteur CSS.\nTransformer les donn√©es en texte avec html_text2().\nNettoyer les donn√©es si n√©cessaire.\n\nVoici l‚Äôexemple avec le nombre d‚Äôoffres de Data Scientist list√©es en France :\n\nweb_page %&gt;%\n  html_element(css = \"div.jobsearch-JobCountAndSortPane-jobCount\") %&gt;% # selecting with css \n  html_text2() %&gt;% # Transform to text\n  str_remove_all(\"[^0-9.-]\") %&gt;% # Clean the data to only get numbers\n  substr(start = 2, stop = 8) %&gt;% \n  as.numeric()\n\nPour l‚Äôinstant, on ne peut scraper les donn√©es que de la premi√®re page. Cependant, je suis int√©ress√© par toutes les offres d‚Äôemploi et j‚Äôai besoin d‚Äôacc√©der aux autres pages ! Apr√®s avoir navigu√© sur les 3 premi√®res pages des offres, j‚Äôai remarqu√© un mod√®le dans l‚ÄôURL (valide au moment de l‚Äô√©criture), ce qui signifie qu‚Äôavec une seule ligne de code, je peux produire une liste contenant les URL des 40 premi√®res pages.\nUne fois la liste obtenue, il ne reste plus qu‚Äô√† boucler sur toutes les URL avec un d√©lai (bonne pratique pour le web scraping), collecter les donn√©es et les nettoyer avec des fonctions personnalis√©es (√† la fin de l‚Äôarticle) :\n\n# Creating URL link corresponding to the first 40 pages\nbase_url = \"https://fr.indeed.com/jobs?q=data%20scientist&l=France&start=\"\nurl_list &lt;- c(url, paste0(base_url, as.character(seq(from=10, to=400, by=10))))\n\n# Looping through the URL list\nres &lt;- list()\nfor(i in 1:length(url_list)){\n  # Navigate to the URL\n  remDr$navigate(url_list[i])\n  \n  # Store page source \n  web_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n\n  # Job title \n  job_title &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\") %&gt;%\n    html_elements(css = \".resultContent\") %&gt;%\n    html_element(\"h2\") %&gt;%\n    html_text2() %&gt;%\n    str_replace(\".css.*;\\\\}\", \"\")\n\n  # URL for job post \n  job_url &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_elements(css = \".resultContent\") %&gt;%\n    html_element(\"h2\") %&gt;%\n    html_element(\"a\") %&gt;%\n    html_attr('href') %&gt;%\n    lapply(function(x){paste0(\"https://fr.indeed.com\", x)}) %&gt;%\n    unlist()\n  \n  # Data about company\n  company_info &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_elements(css = \".resultContent\")%&gt;%\n    html_element(css = \".company_location\")%&gt;%\n    html_text2() %&gt;%\n    lapply(FUN = tidy_comploc) %&gt;% # Function to clean the textual data\n    do.call(rbind, .)\n\n  # Data about job description\n  job_desc &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_element(css =\".slider_container .jobCardShelfContainer\")%&gt;%\n    html_text2() %&gt;%\n    tidy_job_desc() # Function to clean the textual data related to job desc.\n\n  # Data about salary (when indicated)\n  salary_hour &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result .resultContent\")%&gt;%\n    html_element(css = \".salaryOnly\") %&gt;%\n    html_text2() %&gt;%\n    lapply(FUN = tidy_salary) %&gt;% # Function to clean the data related to salary\n    do.call(rbind, .)\n  \n  # Job posts in the same format\n  final_df &lt;- cbind(job_title, company_info, salary_hour, job_desc, job_url)\n  colnames(final_df) &lt;- c(\"Job_title\", \"Company\", \"Location\", \"Rating\", \"Low_salary\", \"High_salary\", \"Contract_info\", \"Job_desc\", \"url\")\n  res[[i]] &lt;- final_df\n  \n  # Sleep 5 seconds, good practice for web scraping\n  Sys.sleep(5)\n}\n\n# Gather all the job post in a tibble\nfinal_df &lt;- as_tibble(do.call(\"rbind\", res))\n\n# Final data cleaning\nfinal_df &lt;- final_df %&gt;%\n  mutate_at(c(\"Rating\", \"Low_salary\", \"High_salary\"), as.numeric)\n\n# Clean job title\nfinal_df$Job_title_c &lt;- clean_job_title(final_df$Job_title)  \nfinal_df$Job_title_c &lt;- as.factor(final_df$Job_title_c)\n\nOn a maintenant un jeu de donn√©es propre ! Voici un exemple tronqu√© des 5 premi√®res lignes :\n\n\n\n\n\n\nJob_title\nCompany\nLocation\nRating\nLow_salary\nHigh_salary\nContract_info\nJob_desc\nJob_type\nJob_title_c\n\n\n\n\nData Scientist junior (H/F)\nKea & Partners\n92240 Malakoff\nNA\n3750\n4583\nCDI +2 | Travail en journ√©e +1\nPlusieurs postes √† pourvoirMaitrise de Python et des packages de data science. 1er cabinet europ√©en de conseil en strat√©gie √† devenir Soci√©t√© √† Mission, certifi√©s B-Corp depuis 2021*,‚Ä¶\nPr√©sentiel\ndata scientist junior\n\n\nData Scientist (F ou H)\nSNCF\nSaint-Denis (93)\n3.9\nNA\nNA\nCDI\nLe d√©veloppement informatique (C, C++, Python, Azure, ‚Ä¶). Valider et recetter les phases des projets. Travailler avec des m√©thodes agiles avec les √©quipes et‚Ä¶\nPr√©sentiel\ndata scientist\n\n\nData Scientist (H/F) (IT)\nYzee Services\nParis (75)\n3.3\n2916\n3750\nTemps plein\nRecueillir, structurer et analyser les donn√©es pertinentes pour l'entreprise (activit√© li√©e √† la relation client, conseil en externe).\nPr√©sentiel\ndata scientist\n\n\nData Scientist H/F\nNatan (SSII)\nParis (75)\nNA\n4583\n5833\nCDI +1 | Travail en journ√©e\nPlusieurs postes √† pourvoirVous retrouverez une *ESN ambitieuse port√©e par le go√ªt de l‚Äôexcellence.*. Au sein du d√©partement en charge d'automatisation transverse des besoins de la‚Ä¶\nPr√©sentiel\ndata scientist\n\n\nData Scientist Junior H/F / Freelance\nkarma partners\nRoissy-en-Brie (77)\nNA\n400\n550\nTemps plein +1\nLe profil recherch√© est un profil junior (0-2 ans d'exp√©rience) en data science, avec une app√©tence technique et des notions d'architecture logicielle et de‚Ä¶\nPr√©sentiel\ndata scientist junior\n\n\n\n\n\n\n\n\nVisualisation des salaires propos√©s\nVoyons si on peut obtenir quelques informations sur les offres de jobs en data science en r√©alisant quelques repr√©sentations graphiques. La premi√®re chose que je voulais savoir, c‚Äô√©tait combien les entreprises √©taient pr√™tes √† payer pour recruter un candidat en data science. J‚Äôai donc d√©cid√© de r√©aliser quelques graphiques sur la plage de salaires en fonction de l‚Äôentreprise et du titre du poste.\nAttention !\nLes graphiques suivants doivent √™tre pris avec des pincettes, car ils affichent un petit √©chantillon des donn√©es. En effet, le salaire n‚Äô√©tait list√© que pour 14% des annonces. Les tendances ou informations dans ces graphiques peuvent ne pas √™tre repr√©sentatives des entreprises n‚Äôayant pas indiqu√© leur salaire propos√©.\n\nSalaire par entreprise\nLe graphique suivant montre les revenus mensuels propos√©s par certaines entreprises (toutes les entreprises ne listent pas leur salaire propos√©) :\n\n# Function to make euro X scale \neuro &lt;- scales::label_dollar(\n  prefix = \"\",\n  suffix = \"\\u20ac\",\n  big.mark = \".\",\n  decimal.mark = \",\"\n)\n\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;% # To remove internships and freelance works\n  select(Company, Low_salary, High_salary) %&gt;%\n  group_by(Company) %&gt;%\n  summarize_if(is.numeric, mean) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n           Company = fct_reorder(Company, desc(-Mean_salary))) %&gt;%\n  ggplot(aes(x = Company)) +\n  geom_point(aes(y = Mean_salary), colour = \"#267266\") +\n  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +\n  geom_hline(aes(yintercept = median(Mean_salary)), lty=2, col='red', alpha = 0.7) +\n  scale_y_continuous(labels = euro) +\n  ylab(\"Monthly income\") +\n  xlab(\"\") +\n  coord_flip() +\n  theme_bw(base_size = 8)\n\n\n\n\n\n\n\n\nLa m√©diane des salaires mensuels est d‚Äôenviron 3700 euros. Comme vous pouvez le constater, les salaires peuvent varier consid√©rablement selon l‚Äôentreprise. Cela est en partie d√ª au fait que je n‚Äôai pas fait de distinction entre les diff√©rents types de postes en data science (data scientist, data analyst, data engineer, senior ou lead).\n\n\nSalaire par titre de poste\nOn peut tracer le m√™me graphique, mais au lieu de regrouper par entreprise, on va regrouper par titre de poste :\n\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;%  # To remove internships and freelance works\n  select(Job_title_c, Low_salary, High_salary, Job_type) %&gt;%\n  group_by(Job_title_c) %&gt;%\n  summarize_if(is.numeric, ~ mean(.x, na.rm = TRUE)) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n         Job_title_c = fct_reorder(Job_title_c, desc(-Mean_salary))) %&gt;%\n  ggplot(aes(x = Job_title_c, y = Mean_salary)) +\n  geom_point(aes(y = Mean_salary), colour = \"#267266\") +\n  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +\n  #geom_label(aes(label = n, Job_title_c, y = 1500), data = count_df) + \n  scale_y_continuous(labels = euro) +\n  theme_bw(base_size = 12) +\n  xlab(\"\") +\n  ylab(\"Monthly Income\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nOn remarque clairement les diff√©rences de salaires propos√©s en fonction du titre de poste : les data scientists semblent gagner l√©g√®rement plus en moyenne que les data analysts. Les entreprises semblent √©galement proposer des salaires plus √©lev√©s pour les postes avec plus de responsabilit√©s ou n√©cessitant plus d‚Äôexp√©rience (senior, lead).\n\n\nSalaire en fonction de la localisation : t√©l√©travail complet, hybride, sur site ?\nEnfin, on peut tracer les salaires en fonction de la localisation (t√©l√©travail complet, hybride, sur site) pour voir si cela a un impact :\n\n# Tidy the types and locations of listed jobs\nfinal_df &lt;- tidy_location(final_df)\ncount_df &lt;- count(final_df %&gt;% filter(Low_salary &gt; 1600), Job_type)\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;% \n  drop_na(Location) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n         Job_type = as.factor(Job_type)) %&gt;%\n    ggplot(aes(x = Job_type, y = Mean_salary)) +\n  geom_boxplot(na.rm = TRUE) +\n  geom_label(aes(label = n, Job_type, y = 5500), data = count_df) + \n  scale_y_continuous(labels = euro) + \n  theme_bw(base_size = 12) +\n  xlab(\"Job Type\") +\n  ylab(\"Income\")\n\n\n\n\n\n\n\n\nIl est √† noter que la plupart des emplois propos√©s en France sont des emplois sur site. Le salaire m√©dian pour ce type de postes est l√©g√®rement inf√©rieur √† celui des emplois hybrides. La distribution des salaires pour les emplois en t√©l√©travail complet et hybrides doit √™tre interpr√©t√©e avec prudence car elle ne concerne que 12 offres d‚Äôemploi.\n\n\n\nCartographie des lieux des emplois\nLors de ma recherche d‚Äôemploi, j‚Äô√©tais frustr√© de ne pas voir une carte g√©ographique regroupant les lieux de tous les emplois propos√©s. Une telle carte pourrait m‚Äôaider consid√©rablement dans ma recherche. Faisons-la !\nTout d‚Äôabord, on doit nettoyer et homog√©n√©iser les lieux pour toutes les offres d‚Äôemploi. √Ä cette fin, j‚Äôai cr√©√© une fonction personnalis√©e (tidy_location()) qui inclut plusieurs fonctions de stringr. Vous pouvez trouver plus de d√©tails sur cette fonction √† la fin de ce post. Elle renvoie le lieu sous ce format : [Ville]([Code postal]). M√™me si tous les lieux ont √©t√© homog√©n√©is√©s, ils ne peuvent pas √™tre directement trac√©s sur une carte (on a besoin de la longitude et de la latitude). Pour obtenir la latitude et la longitude √† partir du nom de la ville et du code postal, j‚Äôai utilis√© la fonction geocode() du package tidygeocoder.\n\n# Extract coordinates from town name\nfinal_df &lt;- final_df %&gt;%\n  mutate(Loc_tidy_fr = paste(Loc_tidy, 'France')) %&gt;%\n  geocode(Loc_tidy_fr, method = 'arcgis', lat = latitude , long = longitude) %&gt;%\n  select(- Loc_tidy_fr)\n\n\nDistribution des emplois en Data Science en France\nOn peut maintenant repr√©senter le nombre d‚Äôemplois en Data Science par d√©partement :\n\n# Map of France from rnaturalearth package\nfrance &lt;- ne_states(country = \"France\", returnclass = \"sf\") %&gt;% \n  filter(!name %in% c(\"Guyane fran√ßaise\", \"Martinique\", \"Guadeloupe\", \"La R√©union\", \"Mayotte\"))\n\n# Transform location to st point \ntest &lt;- st_sf(final_df, geom= lapply(1:nrow(final_df), function(x){st_point(c(final_df$longitude[x],final_df$latitude[x]))}))\nst_crs(test) &lt;- 4326\n\n# St_join by departments \njoined &lt;- france %&gt;%\n  st_join(test, left = T)\n\n# Custom breaks for visual representation\nmy_breaks = c(0, 2, 5, 10, 30, 50, 100, 260)\n\njoined %&gt;% \n  mutate(region=as.factor(name)) %&gt;% \n  group_by(region) %&gt;% \n  summarize(Job_number=n()) %&gt;% \n  mutate(Job_number = cut(Job_number, my_breaks)) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill=Job_number), col='grey', lwd=0.2) + \n  scale_fill_brewer(\"Job number\",palette = \"GnBu\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nIl est vraiment int√©ressant de constater que la r√©partition des emplois est assez h√©t√©rog√®ne en France. La majorit√© des emplois sont concentr√©s dans quelques d√©partements qui abritent une grande ville. Cela est attendu, car la plupart des emplois sont propos√©s par de grandes entreprises souvent install√©es √† proximit√© des grandes villes.\n\n\nCarte interactive\nOn peut assii aller plus loin et tracer une carte interactive avec leaflet, √ßa nous permet de rechercher dynamiquement une offre d‚Äôemploi :\n\n# Plot leaflet map\nfinal_df %&gt;%\n  mutate(pop_up_text = sprintf(\"&lt;b&gt;%s&lt;/b&gt; &lt;br/&gt; %s\",\n                                     Job_title, Company)) %&gt;% # Make popup text\n  leaflet() %&gt;%\n  setView(lng = 2.36, lat = 46.31, zoom = 5.2) %&gt;% # Center of France\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addMarkers(\n    popup = ~as.character(pop_up_text),\n    clusterOptions = markerClusterOptions()\n  )"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#analyser-les-descriptions-demploi",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#analyser-les-descriptions-demploi",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Analyser les descriptions d‚Äôemploi",
    "text": "Analyser les descriptions d‚Äôemploi\nDe nos jours, la plupart des CV sont scann√©s et interpr√©t√©s par un syst√®me de suivi des candidatures (ATS). Pour faire simple, ce syst√®me recherche des mots-cl√©s dans votre CV et √©value la correspondance avec l‚Äôoffre d‚Äôemploi pour laquelle vous postulez. Il est donc important de d√©crire vos exp√©riences avec des mots-cl√©s sp√©cifiques pour am√©liorer vos chances d‚Äôacc√©der √† l‚Äô√©tape suivante du processus de recrutement.\nMais quels mots-cl√©s devrait-on inclure dans mon CV ? R√©pondons √† cette question en analysant les descriptions des offres d‚Äôemploi de data scientist.\n\nT√©l√©charger et nettoyer chaque description d‚Äôemploi\nTout d‚Äôabord, on t√©l√©charge la description compl√®te de chaque offre en naviguant √† travers toutes les URL list√©es dans notre tableau. On nettoye et homog√©n√©ise la description avec une fonction personnalis√©e :\n\n# Loop through all the URLs\njob_descriptions &lt;- list()\npb &lt;- txtProgressBar(min = 1, max = length(final_df$url), style = 3)\nfor(i in 1:length(final_df$url)){\n  remDr$navigate(final_df$url[i])\n  web_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n  job_descriptions[[i]] &lt;- web_page %&gt;%\n        html_elements(css = \".jobsearch-JobComponent-description\") %&gt;%\n      html_text2()\n  Sys.sleep(2)\n  setTxtProgressBar(pb, i)\n}\n# Gathering in dataframe\njob_descriptions &lt;- as.data.frame(do.call(\"rbind\", job_descriptions))\nnames(job_descriptions) &lt;- c(\"Description\")\n\n# Binding to same table:\nfinal_df &lt;- cbind(final_df, job_descriptions)\n\n# Homogenize with custom function\nfinal_df$Description_c &lt;- lapply(final_df$Description, function(x){clean_job_desc(x)[[2]]})\nfinal_df$Language &lt;- textcat::textcat(final_df$Description)\n\n\n\nProc√©dure d‚Äôannotation avec le package udpipe\nCette partie est inspir√©e de cet article.\nMaintenant que les descriptions de tous les emplois list√©s sont import√©es et pr√©-nettoy√©es, on peut annoter les donn√©es textuelles avec le package udpipe. Ce package contient des fonctions et des mod√®les qui permettent de r√©aliser la tokenisation, la lemmatisation et l‚Äôextraction de mots-cl√©s.\nOn restreigne d‚Äôabord cette analyse aux offres d‚Äôemploi de data scientist r√©dig√©es en fran√ßais, puis on annote toutes les descriptions :\n\n# Restricting the analysis to Data scientist post written in french\ndesc_data_scientist &lt;- final_df %&gt;%\n  filter((Job_title_c == \"data scientist\") & (Language == \"french\")) %&gt;%\n  select(Description_c)\n\nud_model &lt;- udpipe_download_model(language = \"french\") # Download the model if necessary\nud_model &lt;- udpipe_load_model(ud_model$file_model) \n\n# Annotate the descriptions \nx &lt;- udpipe_annotate(ud_model, x = paste(desc_data_scientist, collapse = \" \"))\nx &lt;- as.data.frame(x)\n\n\n\nLes noms les plus courants\nOn peut visualiser les mots les plus utilis√©s dans les offres d‚Äôemploi de data scientist r√©dig√©es en fran√ßais :\n\nstats &lt;- subset(x, upos %in% \"NOUN\")\nstats &lt;- txt_freq(x = stats$lemma)\n\nstats %&gt;%\n  top_n(50, freq) %&gt;%\n  mutate(key = as.factor(key),\n         key = fct_reorder(key, freq)) %&gt;%\n  ggplot(aes(x = key, y = freq)) +\n  geom_bar(stat = 'identity') +\n  coord_flip() + \n  ylab(\"Most common nouns\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nBien que cela nous donne une id√©e des mots √† inclure, ce n‚Äôest pas tr√®s informatif car les mots-cl√©s sont souvent compos√©s de deux mots ou plus.\n\n\nExtraction des mots-cl√©s pour la r√©daction de CV\nIl existe plusieurs m√©thodes impl√©ment√©es dans udpipe pour extraire les mots-cl√©s d‚Äôun texte. Apr√®s avoir test√© plusieurs m√©thodes, j‚Äôai s√©lectionn√© l‚Äôextraction automatique rapide des mots-cl√©s (RAKE) qui me donne les meilleurs r√©sultats :\n\nstats &lt;- keywords_rake(x = x,\n                       term = \"token\",# Search on token\n                       group = c(\"doc_id\", \"sentence_id\"), # On every post \n                       relevant = x$upos %in% c(\"NOUN\", \"ADJ\"),  # Only among noun and adj.\n                       ngram_max = 2, n_min = 2, sep = \" \")\n\nstats &lt;- subset(stats, stats$freq &gt;= 5 & stats$rake &gt; 3)\n\nstats %&gt;% \n  arrange(desc(rake)) %&gt;% \n  head()\n\n                    keyword ngram freq     rake\n1 intelligence artificielle     2    9 9.368889\n2             tableaux bord     2    5 8.504274\n3      formation sup√©rieure     2    5 8.374725\n4        mod√®les pr√©dictifs     2   15 7.581294\n5         force proposition     2    6 7.190238\n6        production √©chelle     2    5 7.034038\n\nwordcloud(words = stats$keyword, freq = stats$freq, min.freq = 3,\n          max.words=100, random.order=FALSE, rot.per=0.35,\n          colors=brewer.pal(8, \"Dark2\"), scale = c(2.5, .5))\n\n\n\n\n\n\n\n\nOn peut voir que cette m√©thode a s√©lectionn√© des mots-cl√©s importants en fran√ßais li√©s au poste de data scientist ! Dans les premi√®res positions, on trouve les mots-cl√©s : ‚Äúintelligence artificielle‚Äù, ‚Äútableaux de bord‚Äù, ‚Äúenseignement sup√©rieur‚Äù, ‚Äúmod√®le pr√©dictif‚Äù. Il vaut mieux v√©rifier si ces mots apparaissent sur mon CV !"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#conclusion",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#conclusion",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Conclusion",
    "text": "Conclusion\nJ‚Äôesp√®re vous avoir convaincu qu‚Äôil est possible d‚Äôoptimiser votre recherche d‚Äôemploi avec la Data Science !\nSi cet article vous a int√©ress√© et que vous √™tes √† la recherche d‚Äôun nouveau Data Scientist, n‚Äôh√©sitez pas √† me contacter par mail car je suis actuellement en recherche d‚Äôemploi en France (hybride, remote) ou en Europe (remote)."
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#fonctions-personnalis√©es-pour-nettoyer-les-donn√©es-extraites-de-la-page-web",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.fr.html#fonctions-personnalis√©es-pour-nettoyer-les-donn√©es-extraites-de-la-page-web",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Fonctions personnalis√©es pour nettoyer les donn√©es extraites de la page web",
    "text": "Fonctions personnalis√©es pour nettoyer les donn√©es extraites de la page web\nCes fonctions utilisent plusieurs m√©thodes telles que les expressions r√©guli√®res, les mots vides et les instructions conditionnelles pour nettoyer les donn√©es textuelles.\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(httr)\nlibrary(tidystopwords)\nlibrary(textcat)\n\n# Function to tidy the data related to the company\ntidy_comploc &lt;- function(text){\n  lst &lt;- str_split(text, pattern = \"\\n\", simplify =T)\n  ext_str &lt;- substr(lst[1], nchar(lst[1])-2, nchar(lst[1]))\n  res &lt;- suppressWarnings(as.numeric(gsub(',', '.', ext_str)))\n  lst[1] &lt;- ifelse(is.na(res), lst[1], substr(lst[1], 1, nchar(lst[1])-3))\n  lst[3] &lt;- res\n  t(as.matrix(lst))\n}\n\n# Function to tidy the short job description provided with the job post\ntidy_job_desc &lt;- function(text){\n  stopwords &lt;- c(\"Candidature facile\", \"Employeur r√©actif\")\n  text &lt;- str_remove_all(text, paste(stopwords, collapse = \"|\"))\n  stopwords_2 &lt;- \"(Posted|Employer).*\"\n  text &lt;- str_remove_all(text, stopwords_2)\n  text\n}\n\n# Function to tidy the salary data if provided\ntidy_salary &lt;- function(text){\n  if(is.na(text)){\n    others &lt;- NA\n    sal_low &lt;- NA\n    sal_high &lt;- NA\n  }else{\n    text &lt;- str_split(text, \"\\n\", simplify = T)\n    others &lt;- paste(text[str_detect(text, \"‚Ç¨\", negate = T)], collapse = \" | \")\n    sal &lt;- text[str_detect(text, \"‚Ç¨\", negate = F)]\n    if(rlang::is_empty(sal)){\n      sal_low &lt;- NA\n      sal_high &lt;- NA\n    }else{\n      range_sal &lt;- as.numeric(str_split(str_remove_all(str_replace(sal, \"√†\", \"-\"), \"[^0-9.-]\"), \"-\", simplify = TRUE))\n      sal_low &lt;- sort(range_sal)[1]\n      sal_high &lt;- sort(range_sal)[2]\n\n      if(str_detect(sal, \"an\")){\n        sal_low &lt;- floor(sal_low/12)\n        sal_high &lt;- floor(sal_high/12)\n      }\n    }\n  }\n  return(c(as.numeric(sal_low), as.numeric(sal_high), others))\n}\n\n# Function to tidy the location of the job (Remote/Hybrid/Onsite) + homogenize \n# location and zip code\ntidy_location &lt;- function(final_df){\n  final_df$Job_type &lt;- ifelse(final_df$Location == \"T√©l√©travail\", \"Full Remote\", ifelse(str_detect(final_df$Location, \"T√©l√©travail\"), \"Hybrid\", \"On site\"))\n  final_df$Loc_possibility &lt;- ifelse(str_detect(final_df$Location, \"lieu\"), \"Plusieurs lieux\", NA)\n  stopwords &lt;- c(\"T√©l√©travail √†\", \"T√©l√©travail\", \"√†\", \"hybride\")\n  final_df$Loc_tidy &lt;- str_remove_all(final_df$Location, paste(stopwords, collapse = \"|\"))\n  final_df$Loc_tidy &lt;- str_remove_all(final_df$Loc_tidy, \"[+].*\")\n  final_df$Loc_tidy &lt;- str_trim(final_df$Loc_tidy)\n  final_df$Loc_tidy &lt;-  sapply(final_df$Loc_tidy,\n                               function(x){\n                                 if(!is.na(suppressWarnings(as.numeric(substr(x, 1, 5))))){\n                                   return(paste(substr(x, 7, 30), paste0('(', substr(final_df$Loc_tidy[2], 1, 2), ')')))\n                                 }else{\n                                   return(x)\n                                 }})\n  return(final_df)\n}\n\n# Function to keep only certain words in text\nkeep_words &lt;- function(text, keep) {\n  words &lt;- strsplit(text, \" \")[[1]]\n  txt &lt;- paste(words[words %in% keep], collapse = \" \")\n  return(txt)\n}\n\n# Homogenize the job title and class them in a few categories\nclean_job_title &lt;- function(job_titles){\n  job_titles &lt;- tolower(job_titles)\n  job_titles &lt;- gsub(\"[[:punct:]]\", \" \", job_titles, perl=TRUE)\n\n  words_to_keep &lt;- c(\"data\", \"scientist\", \"junior\", \"senior\", \"engineer\", \"nlp\",\n                     \"analyst\", \"analytics\", \"analytic\", \"science\", \"sciences\",\n                     \"computer\", \"vision\", \"ingenieur\", \"donn√©es\", \"analyste\",\n                     \"analyses\", \"lead\", \"leader\", \"dataminer\", \"mining\", \"chief\",\n                     \"miner\", \"analyse\", 'head')\n  job_titles_c &lt;- unlist(sapply(job_titles, function(x){keep_words(x, words_to_keep)}, USE.NAMES = F))\n  job_titles_c &lt;- unlist(sapply(job_titles_c, function(x){paste(unique(unlist(str_split(x, \" \"))), collapse = \" \")}, USE.NAMES = F))\n  table(job_titles_c)\n\n  data_analytics_ind &lt;-  job_titles_c %in% c(\"analyses data\", \"analyst data\", \"analyste data\", \"analyste data scientist\", \"data analyse\",\n                                             \"analyste donn√©es\", \"analytic data scientist\", \"analytics data\", \"analytics data engineer\", \"data analyst engineer\",\n                                             \"data analyst donn√©es\", \"data analyst scientist\", \"data analyst scientist donn√©es\", \"data analyste\", \"data analyst analytics\",\n                                             \"data analytics\", \"data analytics engineer\", \"data engineer analyst\", \"data scientist analyst\", \"data scientist analytics\")\n  job_titles_c[data_analytics_ind] &lt;- \"data analyst\"\n\n  data_analytics_j_ind &lt;-  job_titles_c %in% c(\"junior data analyst\", \"junior data analytics\", \"junior data scientist analyst\")\n  job_titles_c[data_analytics_j_ind] &lt;- \"data analyst junior\"\n\n  data_scientist_ind &lt;- job_titles_c %in% c(\"data computer science\", \"data science\", \"data science scientist\", \"data sciences\",\n                                            \"data sciences scientist\", \"data scientist donn√©es\", \"data scientist sciences\",\n                                            \"donn√©es data scientist\", \"scientist data\", \"science donn√©es\", \"scientist data\",\n                                            \"scientist data science\", \"computer data science\", \"data science donn√©es\", \"data scientist science\")\n  job_titles_c[data_scientist_ind] &lt;- \"data scientist\"\n\n  data_scientist_j_ind &lt;- job_titles_c %in% c(\"junior data scientist\")\n  job_titles_c[data_scientist_j_ind] &lt;- \"data scientist junior\"\n\n  data_engineer_ind &lt;- job_titles_c %in% c(\"data engineer scientist\", \"data science engineer\", \"data miner\", \"data scientist engineer\",\n                                           \"dataminer\", \"engineer data scientist\", \"senior data scientist engineer\", \"ingenieur data scientist\")\n  job_titles_c[data_engineer_ind] &lt;- \"data engineer\"\n\n  nlp_data_scientist_ind &lt;- job_titles_c %in% c(\"data scientist nlp\", \"nlp data science\",\n                                                \"nlp data scientist\", \"senior data scientist nlp\")\n  job_titles_c[nlp_data_scientist_ind] &lt;- \"data scientist NLP\"\n\n  cv_data_scientist_ind &lt;- job_titles_c %in% c(\"computer vision data scientist\", \"data science computer vision\",\n                                               \"data scientist computer vision\")\n  job_titles_c[cv_data_scientist_ind] &lt;- \"data scientist CV\"\n\n  lead_data_scientist_ind &lt;- job_titles_c %in% c(\"chief data\", \"chief data scientist\", \"data scientist leader\", \"lead data scientist\",\n                                                 \"data chief scientist\", \"lead data scientist senior\", \"head data science\")\n  job_titles_c[lead_data_scientist_ind] &lt;- \"data scientist lead or higher\"\n  senior_data_scientist_ind &lt;- job_titles_c %in% c(\"senior data scientist\")\n  job_titles_c[senior_data_scientist_ind] &lt;- \"data scientist senior\"\n\n  senior_data_analytics_ind &lt;- job_titles_c %in% c(\"senior analytics data scientist\", \"senior data analyst\", \"senior data scientist analytics\")\n  job_titles_c[senior_data_analytics_ind] &lt;- \"data analyst senior\"\n\n\n  lead_data_analyst_ind &lt;- job_titles_c %in% c(\"lead data analyst senior\", \"lead data analyst\")\n  job_titles_c[lead_data_analyst_ind] &lt;- \"data analyst lead\"\n  return(job_titles_c)\n}\n\n# Function to clean the full job description before word annotation\nclean_job_desc &lt;- function(text){\n  text &lt;- tolower(text)\n  text &lt;- str_replace_all(text, \"\\n\", \" \")\n  text &lt;- str_remove(text, pattern = \"d√©.*du poste \")\n  text &lt;- str_remove(text, pattern = \"analyse de recr.*\")\n  text &lt;- gsub(\"(?!&)[[:punct:]+‚Äô+‚Ä¶+¬ª+¬´]\", \" \", text, perl=TRUE)\n\n  language &lt;- textcat(text)\n\n  if(language == \"french\"){\n    text &lt;- str_replace_all(text, \"≈ì\", \"oe\")\n    stopwords &lt;- c(\"d√©tails\", \"poste\", \"description\", \"informations\", \"compl√©mentaires\", \"c\", generate_stoplist(language = \"French\"))\n  }else{\n    stopwords &lt;- c(\"description\", generate_stoplist(language = \"English\"))\n  }\n\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n\n  return(c(language, text))\n}"
  },
  {
    "objectID": "posts/2024-07-17-sencrop-data-quality/index.fr.html",
    "href": "posts/2024-07-17-sencrop-data-quality/index.fr.html",
    "title": "L‚Äôimportance du controle de la qualit√© des donn√©es en M√©t√©orologie",
    "section": "",
    "text": "√áa fait longtemps qu‚Äôon ne s‚Äô√©tait pas vus. J‚Äôai √©t√© assez occup√© sur mon nouveau poste, mais je reviens avec un nouvel article sur Medium! üöÄ\nLorsque l‚Äôon travaille avec des donn√©es m√©t√©orologiques, s‚Äôassurer dela qualit√© des donn√©es n‚Äôest pas juste un ‚Äúplus‚Äù ‚Äî c‚Äôest une n√©cessit√©. Les pr√©visions m√©t√©orologiques, les outils d‚Äôaide √† la d√©cision et les analyses climatiques reposent sur des mesures pr√©cises. Mais que se passe-t-il lorsqu‚Äôun capteur tombe en panne, qu‚Äôune station est install√©e de mani√®re incorrecte, ou qu‚Äôun oiseau d√©cide de nidifier dans un collecteur de pluie ? üê¶\nChez Sencrop, le r√©seau de stations m√©t√©orologiques alimente une vari√©t√© de processus en aval, des agr√©gations simples aux outils complexes d‚Äôaide √† la d√©cision en agriculture. Sans une d√©tection robuste des anomalies, ces processus pourraient √™tre perturb√©s par des mesures erron√©es, entra√Ænant des insights inexactes.\nDans cet article, j‚Äôexplore :\n\nPourquoi le contr√¥le de la qualit√© des donn√©es est crucial en m√©t√©orologie\nLes m√©thodes classiques de contr√¥le de la qualit√© des donn√©es et pourquoi elles ne sont pas adapt√©es √† notre cas\nComment on a mis en ≈ìuvre une d√©tection des anomalies innovante pour maintenir la fiabilit√© de nos donn√©es\n\nL‚Äôarticle complet est ici : Sencrop‚Äôs data quality control: Beyond the Z-score\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2024-09-19-exploring-value-remote-sensing-agriculture/index.fr.html",
    "href": "posts/2024-09-19-exploring-value-remote-sensing-agriculture/index.fr.html",
    "title": "Explorer la valeur du t√©l√©d√©tection en agriculture",
    "section": "",
    "text": "Je viens de publier un nouvel article sur Medium ! Cette fois j‚Äôexplore le r√¥le du t√©l√©d√©tection en agriculture et plus sp√©cifiquement comment les donn√©es radar peuvent aider √† estimer les pr√©cipitations plus efficacement ! üì°\nSavoir exactement combien de pluie est tomb√©e sur un champ est crucial pour les agriculteurs. Cela influence les d√©cisions concernant l‚Äôirrigation, les actions de pr√©vention des maladies et de gestion de la sant√© des cultures. Traditionnellement, les pluviom√®tres ont √©t√© la r√©f√©rence pour mesurer les pr√©cipitations, mais leur port√©e est limit√©e √† l‚Äôendroit o√π ils sont physiquement install√©s.\nGr√¢ce aux m√©thodes de t√©l√©d√©tection, telles que le radar m√©t√©o ou les satellites, on peut obtenir des estimations de pr√©cipitations sur de vastes √©tendu, offrant une perspective plus large que ce que les pluviom√®tres peuvent fournir. Mais quelle est leur fiabilit√© pour les applications agricoles ? Chez Sencrop, on a enqu√™t√© pour savoir si les donn√©es radar peuvent :\n\nCompl√©ter notre r√©seau de stations m√©t√©orologiques, surtout dans les zones moins denses\nAm√©liorer la qualit√© des donn√©es de pr√©cipitations par remplissage des trous de donn√©es et d√©tection d‚Äôanomalies\n√ätre utilisable pour l‚Äôirrigation et la pr√©vision des maladies\n\nDans cet article, je compare les donn√©es radar avec les observations des pluviom√®tres pour voir dans quelle mesure elles r√©pondent aux besoins agricoles.\nL‚Äôarticle complet est ici : Exploring the Value of Remote Sensing in Agriculture: Making Sense of Radar Data\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.fr.html",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.fr.html",
    "title": "Webscraping sur Aliexpress avec Rselenium",
    "section": "",
    "text": "Aujourd‚Äôhui, je vais vous montrer comment r√©cup√©rer les prix des produits sur le site Aliexpress."
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.fr.html#quelques-mots-sur-le-web-scraping",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.fr.html#quelques-mots-sur-le-web-scraping",
    "title": "Webscraping sur Aliexpress avec Rselenium",
    "section": "Quelques mots sur le web scraping",
    "text": "Quelques mots sur le web scraping\nAvant de plonger dans le sujet, vous devez savoir que le web scraping n‚Äôest pas autoris√© sur certains sites web. Pour savoir si cela s‚Äôapplique au site que vous souhaitez scraper, je vous invite √† v√©rifier la page robots.txt qui devrait se trouver √† la racine de l‚Äôadresse du site. Pour Aliexpress, cette page se trouve ici : www.aliexpress.com/robots.txt.\nCette page indique que le web scraping et le crawling ne sont pas autoris√©s sur plusieurs cat√©gories de pages telles que /bin/*, /search/*, /wholesale* par exemple. Heureusement pour nous, la cat√©gorie /item/*, o√π les pages des produits sont stock√©es, peut √™tre scrapp√©e."
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.fr.html#rselenium",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.fr.html#rselenium",
    "title": "Webscraping sur Aliexpress avec Rselenium",
    "section": "RSelenium",
    "text": "RSelenium\n\nInstallation pour Ubuntu 18.04 LTS\nL‚Äôinstallation de RSelenium n‚Äôa pas √©t√© aussi simple que pr√©vu et j‚Äôai rencontr√© deux erreurs.\nLa premi√®re erreur que j‚Äôai obtenue apr√®s avoir install√© le package et essay√© la fonction Rsdriver √©tait :\nError in curl::curl_fetch_disk(url, x$path, handle = handle) :\nUnrecognized content encoding type. libcurl understands deflate, gzip content encodings.\nGr√¢ce √† ce post, j‚Äôai install√© le package manquant : stringi.\nUne fois cette erreur corrig√©e, j‚Äôen ai rencontr√© une autre :\nError: Invalid or corrupt jarfile /home/aurelien/.local/share/binman_seleniumserver/generic/4.0.0-alpha-2/selenium-server-standalone-4.0.0-alpha-2.jar\nCette fois-ci, le probl√®me venait d‚Äôun fichier corrompu. Gr√¢ce √† ce post, j‚Äôai su que je devais simplement t√©l√©charger ce fichier selenium-server-standalone-4.0.0-alpha-2.jar depuis le site officiel de Selenium et remplacer le fichier corrompu par celui-ci.\nJ‚Äôesp√®re que cela aidera certains d‚Äôentre vous √† installer RSelenium sur Ubuntu 18.04 LTS !\n\n\nOuverture d‚Äôun navigateur web\nApr√®s avoir corrig√© les erreurs ci-dessus, je peux maintenant ouvrir un navigateur Firefox :\n\nlibrary(RSelenium)\n\n#Open a firefox driver\nrD &lt;- rsDriver(browser = \"firefox\") \nremDr &lt;- rD[[\"client\"]]\n\n\n\nConnexion √† Aliexpress\nLa premi√®re √©tape pour r√©cup√©rer les prix des produits sur Aliexpress est de se connecter √† son compte :\n\nlog_id &lt;- \"Your_mail_adress\"\npassword &lt;- \"Your_password\"\n\n# Navigate to aliexpress login page \nremDr$navigate(\"https://login.aliexpress.com/\")\n\n# Fill the form with mail address\nremDr$findElement(using = \"id\", \"fm-login-id\")$sendKeysToElement(list(log_id))\n\n# Fill the form with password\nremDr$findElement(using = 'id', \"fm-login-password\")$sendKeysToElement(list(password))\n\n#Submit the login form by clicking Submit button\nremDr$findElement(\"class\", \"fm-button\")$clickElement()\n\n\n\nNavigating through the URLs and scraping the prices\nMaintenant, on doit naviguer √† travers un vecteur contenant les URL des produits Aliexpress qui nous int√©ressent. Ensuite, on extrait le prix du produit en utilisant le xpath du prix du produit sur la page web. Le xpath de l‚Äô√©l√©ment que vous voulez scraper peut √™tre trouv√© en utilisant les outils de d√©veloppement de Chrome ou Firefox (tutoriel ici !). Une fois le prix extrait, il faut s‚Äôassurer que ce prix soit sous un format num√©rique en supprimant tout caract√®re sp√©cial (symbole euro ou dollar) et en rempla√ßant la virgule par un point pour le s√©parateur d√©cimal. Voici le code R :\n\n  url_list &lt;- list(\"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Craws-Soft-Fishing-Lures-110mm-11-5g-Artificial-Bait-Soft-Bait-Craws-Lures/406467_32419930548.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-Fishing-Lure-5Pcs-Lot-155mm-7-4g-3-colors-Swimbait-Artificial-Lizard-Soft-Fishing-Lures/406467_32613648610.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Soft-Fishing-Lures-Minnow-Biat-95mm-6g-Jerkbait-Soft-Bait/406467_32419066106.html?spm=a2g0w.12010612.0.0.25fe5872CBqy0m\") \n\n# Allocate a vector to store the price of the products \ncurrentp &lt;- c()\nfor(i in 1:length(url_list)){\n  \n  # Navigate to link [i]\n  remDr$navigate(url_list[i])\n  \n  # Find the price with an xpath selector and findElement.  \n  # Sometimes products can be removed and this could throw an error this is why we are using 'try' to handle the potential errors\n  \n  current &lt;- try(remDr$findElement(using = \"xpath\",'//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"product-price-value\", \" \" ))]'), silent = T)\n  \n  #If error : current price is NA \n  if(class(current) =='try-error'){\n    currentp[i] &lt;- NA\n  }else{\n    # Get the price \n    text &lt;- unlist(current$getElementText())\n    \n    #Remove euro sign\n    text &lt;- gsub(\"[^A-Za-z0-9,;._-]\",\"\",text)\n    \n    #Case when there is a range of price instead of one price + replace comma by point\n    if(grepl(\"-\", text)) {  \n      pe &lt;- sub(\"-.*\",\"\",text) %&gt;% sub(\",\", \".\", ., fixed = TRUE)\n      currentp[i] &lt;-  as.numeric(pe)\n    }else{\n      currentp[i] &lt;- as.numeric(sub(\",\", \".\", text, fixed = TRUE))\n  }\n  }\n  \nSys.sleep(4)\n}\n\nIl est conseill√© d‚Äôattendre quelques secondes entre chaque lien avec Sys.sleep(4) afin d‚Äô√©viter d‚Äô√™tre mis sur liste noire par le site web.\n\n\nVersion Phantomjs\nSi vous ex√©cutez le code ci-dessus, vous devriez voir un navigateur Firefox s‚Äôouvrir et naviguer √† travers la liste que vous avez fournie. Dans le cas o√π vous ne souhaitez pas une fen√™tre active, vous pouvez remplacer Firefox par le navigateur phantomjs, qui est un navigateur sans interface graphique (headless).\nJe ne sais pas pourquoi, mais l‚Äôutilisation de rsDriver(browser = \"phantomjs\") ne fonctionne pas pour moi. J‚Äôai trouv√© cet article qui propose de d√©marrer le navigateur phantomjs avec le package wdman :\n\nlibrary(wdman)\nlibrary(RSelenium)\n# start phantomjs instance\nrPJS &lt;- wdman::phantomjs(port = 4680L)\n\n# is it alive?\nrPJS$process$is_alive()\n\n#connect selenium to it?\nremDr &lt;-  RSelenium::remoteDriver(browserName=\"phantomjs\", port=4680L)\n\n# open a browser\nremDr$open()\n\nremDr$navigate(\"http://www.google.com/\")\n\n# Screenshot of the headless browser to check if everything is working\nremDr$screenshot(display = TRUE)\n\n# Don't forget to close the browser when you are finished ! \nremDr$close()\n\n\n\nConclusion\nUne fois que l‚Äôon comprend les bases de RSelenium et comment s√©lectionner des √©l√©ments dans des pages HTML, c‚Äôest assez facile d‚Äô√©crire un script pour extraire des donn√©es sur le web. Cet article est un exemple simple d‚Äôextraction du prix des produits sur les pages Aliexpress, mais le script peut √™tre √©tendu pour extraire plus de donn√©es sur chaque page, telles que le nom de l‚Äôarticle, sa note, etc. Il est m√™me possible d‚Äôautomatiser ce script pour qu‚Äôil s‚Äôex√©cute quotidiennement afin de suivre l‚Äô√©volution des prix au fil du temps. Les possibilit√©s sont infinies !"
  },
  {
    "objectID": "posts/2022-07-19-top-r-packages/index.fr.html",
    "href": "posts/2022-07-19-top-r-packages/index.fr.html",
    "title": "Analyse des packages R les plus t√©l√©charg√©s",
    "section": "",
    "text": "Apr√®s un moment √† coder en Python tous les jours pour mon travail, j‚Äôavais besoin de faire une pause et d‚Äôeffectuer quelques analyses en R ! Depuis le d√©but de mon postdoc, je n‚Äôavais pas suivi les derni√®res tendances concernant les packages R. Dans cet article, je vais analyser des donn√©es sur les packages R pour voir quels sont les packages les plus t√©l√©charg√©s au cours des derni√®res semaines. Je vais aussi visualiser toutes les relations entre les packages R en examinant leurs d√©pendances requises.\nCommen√ßons par importer les packages n√©cessaires pour cette analyse :\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cranlogs)\nlibrary(igraph)\nlibrary(visNetwork)\n\n\nComment trouver les packages R les plus populaires ?\nLa premi√®re √©tape consiste √† collecter les donn√©es sur le nombre de t√©l√©chargements pour chaque package. Heureusement pour nous, il existe un package appel√© cranlogs qui fait exactement ce dont on a besoin ! Avec une simple ligne de commande, on peut collecter les donn√©es sur les 50 packages ayant eu le plus de t√©l√©chargements au cours du dernier mois, puis on peut tracer le r√©sultat :\n\npopular_pckg &lt;- cran_top_downloads(\"last-month\", 50)\n\npopular_pckg %&gt;%\n  mutate(package = fct_reorder(package, desc(count))) %&gt;% \n  ggplot(aes(x = package, y = count)) +\n  geom_bar(stat=\"identity\") + \n  scale_x_discrete(expand = expansion(mult = c(0, 0.02))) +\n  theme_bw() +\n  xlab(\"\") +\n  ylab(\"Downloads\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 14),\n        legend.position = \"none\") +labs(x = NULL)\n\n\n\n\n\n\n\n\nJe pensais que ce graphique serait plus difficile √† r√©aliser √† cause de la disponibilit√© des donn√©es, mais avec le bon package, c‚Äôest facile !\n\n\nVisualisation des d√©pendances entre les packages\nUne fois que j‚Äôai vu le graphique ci-dessus, je me suis demand√© quelles √©taient toutes les d√©pendances entre ces packages et je voulais savoir lequel √©tait le plus ‚Äúconnect√©‚Äù. Pour r√©pondre √† cette question, j‚Äôavais besoin de plus de donn√©es, notamment sur les d√©pendances requises de chaque package. Apr√®s quelques recherches, j‚Äôai d√©couvert que les donn√©es sur les packages (y compris la description et les d√©pendances) pouvaient √™tre extraites avec une fonction du package tools :\n\ndf_pkg &lt;- tools::CRAN_package_db()[, c('Package', 'Imports')]\ndf_pkg &lt;- df_pkg %&gt;% filter(if_any(\"Package\", ~.x %in% popular_pckg[[\"package\"]]))\n\nCependant, cette fonction extrait les donn√©es pour tous les packages, et je souhaite effectuer l‚Äôanalyse uniquement sur les 50 packages les plus populaires. J‚Äôai donc d√©cid√© de coupler la fonction du package cranlog avec la base de donn√©es que j‚Äôai collect√©e avec la fonction CRAN_package_db() :\n\n# Can be quite long hence the parallel map\n#plan(multisession, workers = 12)\nmonthly_dl &lt;- map(list(df_pkg$Package), function(x){sum(cran_downloads(x, 'last-month')$count)})\ndf_pkg$monthly_dl &lt;- unlist(monthly_dl)\n# write_csv(df_pkg, 'R_pkg_dl.csv')\n\nOn peut ensuite filtrer par nombre de t√©l√©chargements :\n\ndf_pkg &lt;- df_pkg %&gt;% \n  distinct(Package, .keep_all= TRUE) %&gt;% \n  arrange(\"monthly_dl\") \n\nMaintenant, il est temps de pr√©parer les donn√©es pour une visualisation graphique. Pour cr√©er un graphique, on a besoin de deux tableaux. Le premier doit contenir toutes les relations entre les n≈ìuds (dans notre cas, les n≈ìuds sont des packages), il a deux colonnes : ‚Äòfrom‚Äô et ‚Äòto‚Äô. Le deuxi√®me tableau contient une seule colonne avec les noms des n≈ìuds.\n\nimport_cleaning &lt;- function(text){\n  text &lt;- gsub('\\\\s*\\\\([^\\\\)]+\\\\)', '', text)\n  text &lt;- gsub('\\\\n', ' ', text)\n  text &lt;- gsub(' ', '', text, fixed = TRUE)\n  text &lt;- str_split(text, ',')\n  return(text)\n}\n\nimport_cleaning(df_pkg$Imports[2])\n\ntest &lt;- df_pkg %&gt;% \n  mutate(cleaned_imports = import_cleaning(Imports))\n\ndf_target &lt;- function(x,y){\n  df &lt;- expand.grid(from=x, to=unlist(y))\n  return(df)}\n\nfor(i in 1:nrow(test)){\n  if(i == 1){\n    df_res = df_target(test$Package[i], test$cleaned_imports[i])\n  }else{\n      df_res = rbind(df_res, df_target(test$Package[i], test$cleaned_imports[i]))\n  }\n}\n\nlinks &lt;- df_res %&gt;% \n  filter(!is.na(to) | (to == \"\"))\n\nnodes &lt;- tibble(id=as.character(unique(unlist(df_res))))\n\nUne fois les deux matrices cr√©√©es, on peut visualiser interactivement le r√©seau graphique avec le package visNetwork :\n\nvisNetwork(nodes, links) %&gt;%\n    visIgraphLayout(type = \"full\") %&gt;%\n    visNodes(\n        shape = \"dot\",\n        color = list(\n            background = \"#0085AF\",\n            border = \"#013848\",\n            highlight = \"#FF8000\"\n        ),\n        scaling = list(min=2,\n                       max = 10),\n        shadow = list(enabled = TRUE, size = 10)\n    ) %&gt;%\n    visEdges(\n      arrows='to',\n        shadow = FALSE,\n        color = list(color = \"#0085AF\", highlight = \"#C62F4B\")\n    ) %&gt;%\n    visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T)) %&gt;% \n    visLayout(randomSeed = 11)\n\n\n\n\n\nN‚Äôh√©sitez pas √† d√©placer, zoomer ou s√©lectionner des packages pour voir leurs d√©pendances !\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.fr.html",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.fr.html",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 4",
    "section": "",
    "text": "Dans ce post, j‚Äôexplore les donn√©es que j‚Äôai collect√©es au cours de l‚Äôann√©e derni√®re avec la version mise √† jour de l‚Äôapplication (pr√©sent√©e ici). Cette rapide analyse exploratoire est r√©alis√©e avec deux packages que j‚Äôappr√©cie particuli√®rement : Plotly et shiny.\nPour rappel, ma nouvelle application stocke les donn√©es dans trois fichiers csv. Le premier contient les variables li√©es aux conditions de p√™che. Le deuxi√®me contient des informations sur mes prises et enfin le troisi√®me contient des informations sur les caract√©ristiques des leurres que j‚Äôai utilis√©s pendant la session."
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.fr.html#shiny-pour-explorer-les-donn√©es-de-p√™che-par-session",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.fr.html#shiny-pour-explorer-les-donn√©es-de-p√™che-par-session",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 4",
    "section": "Shiny pour explorer les donn√©es de p√™che par session",
    "text": "Shiny pour explorer les donn√©es de p√™che par session\nJ‚Äôai cod√© une petite application shiny qui fournit un r√©sum√© des conditions de mar√©e et de d√©bit de la rivi√®re, des changements de leurres et des prises pour chaque session. N‚Äôh√©sitez pas √† explorer mes donn√©es de p√™che !"
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.fr.html#code-de-lapplication-shiny",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.fr.html#code-de-lapplication-shiny",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 4",
    "section": "Code de l‚Äôapplication shiny",
    "text": "Code de l‚Äôapplication shiny\nVoici le code des graphiques plotly dans l‚Äôapplication :\n\nlibrary(plotly)\nlibrary(tidyverse)\n\n\n#' For the tide plot\n#' \n#' @param dat first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @param temporal_range number of hours to display (before and after the session)\n#' @return A plotly object\nplot_tide_ses &lt;- function(dat, n_ses, temporal_range = 4){\n\n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))\n  dat_tide &lt;- as.data.frame(dat_t$Tide_ts)\n  dat_tide$hour &lt;- as.POSIXct(dat_tide$hour, origin = \"1970-01-01\")\n  dat_tide$Water &lt;- as.numeric(as.character(dat_tide$Water))\n  \n  plot_ly(data = dat_tide, \n          x = ~ hour, \n          y = ~ Water, \n          mode = 'lines') %&gt;%  \n    layout(shapes = list(\n      list(type = 'line',\n           x0 = as.POSIXct(dat_t$Beg),\n           x1 = as.POSIXct(dat_t$Beg),\n           y0 = min(dat_tide$Water),\n           y1 = max(dat_tide$Water),\n           line = list(dash = 'dot', width = 1)),\n      list(type = 'line',\n           x0 =  as.POSIXct(dat_t$End),\n           x1 = as.POSIXct(dat_t$End),\n           y0 = min(dat_tide$Water),\n           y1 = max(dat_tide$Water),\n           line = list(dash = 'dot', width = 1))),\n      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*temporal_range ,\n                                        as.POSIXct(dat_t$End) + 3600*temporal_range )),\n                   title = \"\"),\n      yaxis = list(title = \"Tide level\"))\n}\n\n#' For the river flow plot\n#' \n#' @param dat first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @param past_days number of previous to display (before the session)\n#' @return A plotly object\nplot_flow_ses &lt;- function(dat, n_ses, past_days = 4){\n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Flow_ts = list(eval(parse(text = Ts_flow))))\n  \n  dat_flow &lt;- as.data.frame(dat_t$Flow_ts)\n  dat_flow$Date &lt;- as.POSIXct(dat_flow$Date, origin = \"1970-01-01\")\n  dat_flow$Nive &lt;- as.numeric(as.character(dat_flow$Nive))\n  dat_flow$Adour &lt;- as.numeric(as.character(dat_flow$Adour))\n  \n  \n  dat_flow &lt;- dat_flow %&gt;% \n    pivot_longer(cols = c(Nive, Adour), \n                 names_to = \"River\",\n                 values_to = \"Flow\")\n  \n  plot_ly(data = dat_flow, \n          x = ~ Date,\n          y = ~ Flow, \n          color = ~ River, \n          mode = 'lines') %&gt;%  \n    layout(shapes = list(\n      list(type='line',\n           x0 = as.POSIXct(dat_t$Beg),\n           x1 = as.POSIXct(dat_t$Beg),\n           y0 = min(dat_flow$Flow),\n           y1 = max(dat_flow$Flow),\n           line = list(dash = 'dot', width = 1))),\n      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*24*past_days,\n                                        as.POSIXct(dat_t$End) )),\n                   title = \"\"))\n}\n\n#' Function to prepare the dataset for the plot of lure change and catch\n#' \n#' @param lure third dataframe with lure changes (hours) and characteristics\n#' @param session first dataframe with session characteristics\n#' @param ses_n the id (number) of the session\n#' @return A dataframe\nstart_end_fonction &lt;- function(lure, session, ses_n){\n  dat_ses &lt;- session %&gt;% \n    filter(Session == ses_n)\n  \n  dat_lure &lt;- lure %&gt;% \n    filter(n_ses == ses_n)\n  \n  startdates &lt;- dat_lure$time\n  enddates &lt;- c(startdates[-1], dat_ses$End)\n  \n  data.frame(change = length(startdates):1, \n             start = as.POSIXct(startdates),\n             end = as.POSIXct(enddates),\n             type = dat_lure$type_lure,\n             text = paste(dat_lure$color_lure, dat_lure$length_lure))\n}\n\n#' For the plot of lure change and catch\n#' \n#' @param lure third dataframe with lure changes (hours) and characteristics\n#' @param caught second dataframe with fish caught characteristics\n#' @param session first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @return A plotly object\nlure_change &lt;- function(lure, caught, dat, n_ses){\n  \n  df &lt;- start_end_fonction(lure, dat, n_ses)\n  \n  catch &lt;- caught %&gt;% \n    filter(n_ses == n_ses)\n  \n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))\n  dat_tide &lt;- as.data.frame(dat_t$Tide_ts)\n  dat_tide$hour &lt;- as.POSIXct(dat_tide$hour, origin = \"1970-01-01\")\n  dat_tide$Water &lt;- as.numeric(as.character(dat_tide$Water))\n  \n  plot_ly() %&gt;% \n    add_segments(data = df,\n                 x = ~ start,\n                 xend = ~ end,\n                 y = ~ change,\n                 yend = ~ change,\n                 color = ~ type,\n                 #text = ~ text,\n                 size = I(5),\n                 alpha = 0.8) %&gt;%\n    add_segments(x = as.POSIXct(catch$time),\n                 xend = as.POSIXct(catch$time),\n                 y = min(df$change),\n                 yend = max(df$change),\n                 line = list(color = \"red\", dash = \"dash\"),\n                 name = 'Fish caught') %&gt;%\n    add_trace(data = dat_tide, \n              x = ~ hour, \n              y = ~ Water, \n              mode = 'lines', \n              yaxis = \"y2\",\n              name = \"Water level\",\n              alpha = 0.4,\n              hoverinfo = 'skip'\n    ) %&gt;% \n    layout(xaxis = list(range = c(df$start[1] - 1000 , df$end[nrow(df)] + 1000),\n                        title = \"\"),\n           yaxis = list(title = \"\", zeroline = FALSE, showline = FALSE,\n                        showticklabels = FALSE, showgrid = FALSE ),\n           yaxis2 = list(overlaying = \"y\", side = \"right\"))\n}\n\nVoici le code de cette application :\n\nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(shinydashboard)\nlibrary(plotly)\nlibrary(tidyverse)\nsource('plot_functions.R')\ndat &lt;- read_csv(\"session1.csv\")\ncaught &lt;- read_csv(\"catch1.csv\")\nlure &lt;- read_csv(\"lure.csv\")\n\n# In order to save the tide and flow time series I parse the data in the dataframe\n# The following line is used to transform the parsed text into usable values\ndat_t &lt;- dat %&gt;% \n  mutate(Tide_ts = list(eval(parse(text = Ts_tide))),\n         Flow_ts = list(eval(parse(text = Ts_flow))))\n\nbody &lt;- dashboardBody(fluidPage(\n  # Application title\n  h1(\"Exploratory analysis of fishing data\",\n  align = \"center\",\n  style = \"padding: 40px;  text-align: center;  background: #605ca8;  color: white;  font-size: 40px;\"),\n  br(),\n  # Dropdown menu to select the fishing session\n  fluidRow(align = \"center\",\n           pickerInput(inputId = 'Ses',\n                       label = h3('Select a fishing session:'),\n                       choices = unique(dat$Session[-1]),\n                       options = list(\n                         style = \"btn-primary\"),\n                       choicesOpt = list(\n                         style = rep_len(\"font-size: 75%; line-height: 1.6;\", 4)\n                       ))),\n  br(),\n  br(),\n  # Key figures of the session\n  fluidRow(\n    valueBoxOutput(\"progressD\", width = 4),\n    valueBoxOutput(\"progressF\", width = 4),\n    valueBoxOutput(\"progressL\", width = 4)),\n  br(),\n  \n  br(),\n  # Graphs of the tide and river flow of recent days\n  fluidRow(\n    box(title = \"Tidal water level\", status = \"primary\", \n        plotlyOutput(\"TidePlot\"), width = 6),\n    box(title = \"River flow\", status = \"primary\",\n        plotlyOutput(\"FlowPlot\"), width = 6)),\n  br(),\n  # Graph lure changes during the session + catch\n  fluidRow(\n    box(title = \"Lures tested and fish capture\", status = \"warning\", \n        plotlyOutput(\"LurePlot\"), width=12))\n))\n\nui &lt;- dashboardPage(\n  \n  dashboardHeader(disable = TRUE),\n  \n  dashboardSidebar(disable = TRUE),\n  \n  body\n)\n\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  # Duration\n  output$progressD &lt;- renderValueBox({\n    Duration = as.integer(difftime(as.POSIXct(dat$End[dat$Session == input$Ses]), as.POSIXct(dat$Beg[dat$Session == input$Ses]), units = 'mins'))\n    valueBox(tags$p(\"Duration\", style = \"font-size: 80%;\"),\n             tags$p(paste(Duration, \"min\"), style = \"font-size: 150%; font-weight: bold;\"),\n             icon = icon(\"clock\"), color = \"purple\")\n  })\n  \n  # Number of fish\n  \n  output$progressF &lt;- renderValueBox({\n    fish_caught = as.integer(caught %&gt;% filter(n_ses == input$Ses) %&gt;% nrow())\n    valueBox(tags$p(\"Fish caught\", style = \"font-size: 80%;\"), tags$p(fish_caught, style = \"font-size: 150%;font-weight: bold;\"),\n             icon = icon(\"trophy\"), color = \"purple\")\n  })\n  \n  # Number of lures tried\n  \n  output$progressL &lt;- renderValueBox({\n    Lure = as.integer(lure %&gt;% filter(n_ses == input$Ses) %&gt;% nrow())\n    valueBox(tags$p(\"Lure tried\", style = \"font-size: 80%;\"), tags$p(Lure, style = \"font-size: 150%;font-weight: bold;\"),\n             icon = icon(\"fish\"), color = \"purple\")\n  })\n  \n  output$TidePlot &lt;- renderPlotly({\n    # generate plot depending on session\n    plot_tide_ses(dat, input$Ses, 4)\n  })\n  output$FlowPlot &lt;- renderPlotly({\n    # generate plot depending on session\n    plot_flow_ses(dat_t, input$Ses, 4)\n  })\n  output$LurePlot &lt;- renderPlotly({\n    # generate plot depending on session\n    lure_change(lure, caught, dat, input$Ses)\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.html",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.html",
    "title": "Can R and Shiny make me a better fisherman? Part 4",
    "section": "",
    "text": "In this post, I explore the data I have collected during the last year with the updated version of the application (presented here). This quick exploratory analysis is performed with two packages I really enjoy: Plotly and shiny.\nFor reminder, my new application store the data in three csv files. The first one contains variables related to the fishing conditions. The second one contains information about my catches and finally the third one contains information about the characteristics of the lures I used during the session."
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.html#shiny-to-explore-fishing-data-by-session",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.html#shiny-to-explore-fishing-data-by-session",
    "title": "Can R and Shiny make me a better fisherman? Part 4",
    "section": "Shiny to explore fishing data by session",
    "text": "Shiny to explore fishing data by session\nI coded a small shiny application that provide a summary of the tide and river flow conditions, the lure changes and catches for each session. Don‚Äôt hesitate to explore my fishing data!"
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.html#code-of-the-shiny-application",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.html#code-of-the-shiny-application",
    "title": "Can R and Shiny make me a better fisherman? Part 4",
    "section": "Code of the shiny application",
    "text": "Code of the shiny application\nHere is the code of the plotly graphs in the application:\n\nlibrary(plotly)\nlibrary(tidyverse)\n\n\n#' For the tide plot\n#' \n#' @param dat first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @param temporal_range number of hours to display (before and after the session)\n#' @return A plotly object\nplot_tide_ses &lt;- function(dat, n_ses, temporal_range = 4){\n\n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))\n  dat_tide &lt;- as.data.frame(dat_t$Tide_ts)\n  dat_tide$hour &lt;- as.POSIXct(dat_tide$hour, origin = \"1970-01-01\")\n  dat_tide$Water &lt;- as.numeric(as.character(dat_tide$Water))\n  \n  plot_ly(data = dat_tide, \n          x = ~ hour, \n          y = ~ Water, \n          mode = 'lines') %&gt;%  \n    layout(shapes = list(\n      list(type = 'line',\n           x0 = as.POSIXct(dat_t$Beg),\n           x1 = as.POSIXct(dat_t$Beg),\n           y0 = min(dat_tide$Water),\n           y1 = max(dat_tide$Water),\n           line = list(dash = 'dot', width = 1)),\n      list(type = 'line',\n           x0 =  as.POSIXct(dat_t$End),\n           x1 = as.POSIXct(dat_t$End),\n           y0 = min(dat_tide$Water),\n           y1 = max(dat_tide$Water),\n           line = list(dash = 'dot', width = 1))),\n      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*temporal_range ,\n                                        as.POSIXct(dat_t$End) + 3600*temporal_range )),\n                   title = \"\"),\n      yaxis = list(title = \"Tide level\"))\n}\n\n#' For the river flow plot\n#' \n#' @param dat first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @param past_days number of previous to display (before the session)\n#' @return A plotly object\nplot_flow_ses &lt;- function(dat, n_ses, past_days = 4){\n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Flow_ts = list(eval(parse(text = Ts_flow))))\n  \n  dat_flow &lt;- as.data.frame(dat_t$Flow_ts)\n  dat_flow$Date &lt;- as.POSIXct(dat_flow$Date, origin = \"1970-01-01\")\n  dat_flow$Nive &lt;- as.numeric(as.character(dat_flow$Nive))\n  dat_flow$Adour &lt;- as.numeric(as.character(dat_flow$Adour))\n  \n  \n  dat_flow &lt;- dat_flow %&gt;% \n    pivot_longer(cols = c(Nive, Adour), \n                 names_to = \"River\",\n                 values_to = \"Flow\")\n  \n  plot_ly(data = dat_flow, \n          x = ~ Date,\n          y = ~ Flow, \n          color = ~ River, \n          mode = 'lines') %&gt;%  \n    layout(shapes = list(\n      list(type='line',\n           x0 = as.POSIXct(dat_t$Beg),\n           x1 = as.POSIXct(dat_t$Beg),\n           y0 = min(dat_flow$Flow),\n           y1 = max(dat_flow$Flow),\n           line = list(dash = 'dot', width = 1))),\n      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*24*past_days,\n                                        as.POSIXct(dat_t$End) )),\n                   title = \"\"))\n}\n\n#' Function to prepare the dataset for the plot of lure change and catch\n#' \n#' @param lure third dataframe with lure changes (hours) and characteristics\n#' @param session first dataframe with session characteristics\n#' @param ses_n the id (number) of the session\n#' @return A dataframe\nstart_end_fonction &lt;- function(lure, session, ses_n){\n  dat_ses &lt;- session %&gt;% \n    filter(Session == ses_n)\n  \n  dat_lure &lt;- lure %&gt;% \n    filter(n_ses == ses_n)\n  \n  startdates &lt;- dat_lure$time\n  enddates &lt;- c(startdates[-1], dat_ses$End)\n  \n  data.frame(change = length(startdates):1, \n             start = as.POSIXct(startdates),\n             end = as.POSIXct(enddates),\n             type = dat_lure$type_lure,\n             text = paste(dat_lure$color_lure, dat_lure$length_lure))\n}\n\n#' For the plot of lure change and catch\n#' \n#' @param lure third dataframe with lure changes (hours) and characteristics\n#' @param caught second dataframe with fish caught characteristics\n#' @param session first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @return A plotly object\nlure_change &lt;- function(lure, caught, dat, n_ses){\n  \n  df &lt;- start_end_fonction(lure, dat, n_ses)\n  \n  catch &lt;- caught %&gt;% \n    filter(n_ses == n_ses)\n  \n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))\n  dat_tide &lt;- as.data.frame(dat_t$Tide_ts)\n  dat_tide$hour &lt;- as.POSIXct(dat_tide$hour, origin = \"1970-01-01\")\n  dat_tide$Water &lt;- as.numeric(as.character(dat_tide$Water))\n  \n  plot_ly() %&gt;% \n    add_segments(data = df,\n                 x = ~ start,\n                 xend = ~ end,\n                 y = ~ change,\n                 yend = ~ change,\n                 color = ~ type,\n                 #text = ~ text,\n                 size = I(5),\n                 alpha = 0.8) %&gt;%\n    add_segments(x = as.POSIXct(catch$time),\n                 xend = as.POSIXct(catch$time),\n                 y = min(df$change),\n                 yend = max(df$change),\n                 line = list(color = \"red\", dash = \"dash\"),\n                 name = 'Fish caught') %&gt;%\n    add_trace(data = dat_tide, \n              x = ~ hour, \n              y = ~ Water, \n              mode = 'lines', \n              yaxis = \"y2\",\n              name = \"Water level\",\n              alpha = 0.4,\n              hoverinfo = 'skip'\n    ) %&gt;% \n    layout(xaxis = list(range = c(df$start[1] - 1000 , df$end[nrow(df)] + 1000),\n                        title = \"\"),\n           yaxis = list(title = \"\", zeroline = FALSE, showline = FALSE,\n                        showticklabels = FALSE, showgrid = FALSE ),\n           yaxis2 = list(overlaying = \"y\", side = \"right\"))\n}\n\nHere is the code of this simple yet informative application:\n\nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(shinydashboard)\nlibrary(plotly)\nlibrary(tidyverse)\nsource('plot_functions.R')\ndat &lt;- read_csv(\"session1.csv\")\ncaught &lt;- read_csv(\"catch1.csv\")\nlure &lt;- read_csv(\"lure.csv\")\n\n# In order to save the tide and flow time series I parse the data in the dataframe\n# The following line is used to transform the parsed text into usable values\ndat_t &lt;- dat %&gt;% \n  mutate(Tide_ts = list(eval(parse(text = Ts_tide))),\n         Flow_ts = list(eval(parse(text = Ts_flow))))\n\nbody &lt;- dashboardBody(fluidPage(\n  # Application title\n  h1(\"Exploratory analysis of fishing data\",\n  align = \"center\",\n  style = \"padding: 40px;  text-align: center;  background: #605ca8;  color: white;  font-size: 40px;\"),\n  br(),\n  # Dropdown menu to select the fishing session\n  fluidRow(align = \"center\",\n           pickerInput(inputId = 'Ses',\n                       label = h3('Select a fishing session:'),\n                       choices = unique(dat$Session[-1]),\n                       options = list(\n                         style = \"btn-primary\"),\n                       choicesOpt = list(\n                         style = rep_len(\"font-size: 75%; line-height: 1.6;\", 4)\n                       ))),\n  br(),\n  br(),\n  # Key figures of the session\n  fluidRow(\n    valueBoxOutput(\"progressD\", width = 4),\n    valueBoxOutput(\"progressF\", width = 4),\n    valueBoxOutput(\"progressL\", width = 4)),\n  br(),\n  \n  br(),\n  # Graphs of the tide and river flow of recent days\n  fluidRow(\n    box(title = \"Tidal water level\", status = \"primary\", \n        plotlyOutput(\"TidePlot\"), width = 6),\n    box(title = \"River flow\", status = \"primary\",\n        plotlyOutput(\"FlowPlot\"), width = 6)),\n  br(),\n  # Graph lure changes during the session + catch\n  fluidRow(\n    box(title = \"Lures tested and fish capture\", status = \"warning\", \n        plotlyOutput(\"LurePlot\"), width=12))\n))\n\nui &lt;- dashboardPage(\n  \n  dashboardHeader(disable = TRUE),\n  \n  dashboardSidebar(disable = TRUE),\n  \n  body\n)\n\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  # Duration\n  output$progressD &lt;- renderValueBox({\n    Duration = as.integer(difftime(as.POSIXct(dat$End[dat$Session == input$Ses]), as.POSIXct(dat$Beg[dat$Session == input$Ses]), units = 'mins'))\n    valueBox(tags$p(\"Duration\", style = \"font-size: 80%;\"),\n             tags$p(paste(Duration, \"min\"), style = \"font-size: 150%; font-weight: bold;\"),\n             icon = icon(\"clock\"), color = \"purple\")\n  })\n  \n  # Number of fish\n  \n  output$progressF &lt;- renderValueBox({\n    fish_caught = as.integer(caught %&gt;% filter(n_ses == input$Ses) %&gt;% nrow())\n    valueBox(tags$p(\"Fish caught\", style = \"font-size: 80%;\"), tags$p(fish_caught, style = \"font-size: 150%;font-weight: bold;\"),\n             icon = icon(\"trophy\"), color = \"purple\")\n  })\n  \n  # Number of lures tried\n  \n  output$progressL &lt;- renderValueBox({\n    Lure = as.integer(lure %&gt;% filter(n_ses == input$Ses) %&gt;% nrow())\n    valueBox(tags$p(\"Lure tried\", style = \"font-size: 80%;\"), tags$p(Lure, style = \"font-size: 150%;font-weight: bold;\"),\n             icon = icon(\"fish\"), color = \"purple\")\n  })\n  \n  output$TidePlot &lt;- renderPlotly({\n    # generate plot depending on session\n    plot_tide_ses(dat, input$Ses, 4)\n  })\n  output$FlowPlot &lt;- renderPlotly({\n    # generate plot depending on session\n    plot_flow_ses(dat_t, input$Ses, 4)\n  })\n  output$LurePlot &lt;- renderPlotly({\n    # generate plot depending on session\n    lure_change(lure, caught, dat, input$Ses)\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2022-07-19-top-r-packages/index.html",
    "href": "posts/2022-07-19-top-r-packages/index.html",
    "title": "Analysis of the top R packages",
    "section": "",
    "text": "After a little while coding in Python every day for my work, I needed to make a break and perform some R analysis! Since the beginning of my postdoc, I haven‚Äôt followed the last trends concerning R packages. In this post, I am going to analyze some data about R packages to see what are the most downloaded packages during the past weeks. I will also visualize all the relationships between the R packages by looking at their required dependencies.\nLet‚Äôs import the packages required for this analysis:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cranlogs)\nlibrary(igraph)\nlibrary(visNetwork)\n\n\nHow to find the most popular R packages ?\nThe first thing is to gather the data about the number of downloads for each package. Luckily for us, there is a package called cranlogs that does just what we need ! With a simple line of command we can collect data about the 50 packages with the most downloads in the last month, we can then plot the result:\n\npopular_pckg &lt;- cran_top_downloads(\"last-month\", 50)\n\npopular_pckg %&gt;%\n  mutate(package = fct_reorder(package, desc(count))) %&gt;% \n  ggplot(aes(x = package, y = count)) +\n  geom_bar(stat=\"identity\") + \n  scale_x_discrete(expand = expansion(mult = c(0, 0.02))) +\n  theme_bw() +\n  xlab(\"\") +\n  ylab(\"Downloads\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 14),\n        legend.position = \"none\") +labs(x = NULL)\n\n\n\n\n\n\n\n\nI thought this graph will be harder to make because of the availability of the data but with the right package everything can be done !\n\n\nVisualisation of the dependencies between packages\nOnce I saw the above graph, I was wondering about all the dependencies between these packages and I wanted to know which one was the most ‚Äúconnected‚Äù. To answer this question, I need more data, especially about the required dependencies of each package. After some research, I found out that data about package (including description and dependencies) can be extracted with a function in the tools package:\n\ndf_pkg &lt;- tools::CRAN_package_db()[, c('Package', 'Imports')]\ndf_pkg &lt;- df_pkg %&gt;% filter(if_any(\"Package\", ~.x %in% popular_pckg[[\"package\"]]))\n\nHowever, this function extract the data for all the packages and I want to perform the analysis only on the top 50 popular packages. So I decided to couple the function of the cranlog package with the database I collected with the CRAN_package_db() function:\n\n# Can be quite long hence the parallel map\n#plan(multisession, workers = 12)\nmonthly_dl &lt;- map(list(df_pkg$Package), function(x){sum(cran_downloads(x, 'last-month')$count)})\ndf_pkg$monthly_dl &lt;- unlist(monthly_dl)\n# write_csv(df_pkg, 'R_pkg_dl.csv')\n\nWe can then filter by number of downloads:\n\ndf_pkg &lt;- df_pkg %&gt;% \n  distinct(Package, .keep_all= TRUE) %&gt;% \n  arrange(\"monthly_dl\") \n\nNow, it is time to prepare the data for a graph visualization. To make a graph, we need two tables. The first one must contain all the relationships between nodes (in our case nodes are packages), it has two columns : ‚Äòfrom‚Äô and ‚Äòto‚Äô. The second table contains only one column with the names of the nodes.\n\nimport_cleaning &lt;- function(text){\n  text &lt;- gsub('\\\\s*\\\\([^\\\\)]+\\\\)', '', text)\n  text &lt;- gsub('\\\\n', ' ', text)\n  text &lt;- gsub(' ', '', text, fixed = TRUE)\n  text &lt;- str_split(text, ',')\n  return(text)\n}\n\nimport_cleaning(df_pkg$Imports[2])\n\ntest &lt;- df_pkg %&gt;% \n  mutate(cleaned_imports = import_cleaning(Imports))\n\ndf_target &lt;- function(x,y){\n  df &lt;- expand.grid(from=x, to=unlist(y))\n  return(df)}\n\nfor(i in 1:nrow(test)){\n  if(i == 1){\n    df_res = df_target(test$Package[i], test$cleaned_imports[i])\n  }else{\n      df_res = rbind(df_res, df_target(test$Package[i], test$cleaned_imports[i]))\n  }\n}\n\nlinks &lt;- df_res %&gt;% \n  filter(!is.na(to) | (to == \"\"))\n\nnodes &lt;- tibble(id=as.character(unique(unlist(df_res))))\n\nOnce the two matrices are made, we can interactively visualize the graph network with visNetwork package:\n\nvisNetwork(nodes, links) %&gt;%\n    visIgraphLayout(type = \"full\") %&gt;%\n    visNodes(\n        shape = \"dot\",\n        color = list(\n            background = \"#0085AF\",\n            border = \"#013848\",\n            highlight = \"#FF8000\"\n        ),\n        scaling = list(min=2,\n                       max = 10),\n        shadow = list(enabled = TRUE, size = 10)\n    ) %&gt;%\n    visEdges(\n      arrows='to',\n        shadow = FALSE,\n        color = list(color = \"#0085AF\", highlight = \"#C62F4B\")\n    ) %&gt;%\n    visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T)) %&gt;% \n    visLayout(randomSeed = 11)\n\n\n\n\n\nDo not hesitate to move, zoom in or select packages to see their dependencies !\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html",
    "title": "Webscraping Aliexpress with Rselenium",
    "section": "",
    "text": "Today, I am going to show you how to scrape product prices from Aliexpress website."
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html#a-few-words-on-web-scraping",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html#a-few-words-on-web-scraping",
    "title": "Webscraping Aliexpress with Rselenium",
    "section": "A few words on web scraping",
    "text": "A few words on web scraping\nBefore diving into the subject, you should be aware that web scraping is not allowed on certain websites. To know if it is the case for the website you want to scrape, I invit you to check the robots.txt page which should be located at the root of the website address. For Aliexpress this page is located here : www.aliexpress.com/robots.txt .\nThis page indicates that webscrapping and crawling are not allowed on several page categories such as /bin/*, /search/*, /wholesale* for example. Fortunately for us, the /item/* category, where the product pages are stored, can be scraped."
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html#rselenium",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html#rselenium",
    "title": "Webscraping Aliexpress with Rselenium",
    "section": "RSelenium",
    "text": "RSelenium\n\nInstallation for Ubuntu 18.04 LTS\nThe installation for RSelenium was not as easy as expected and I encountered two errors.\nThe first error I got after I installed the package and tried the function Rsdriver was :\nError in curl::curl_fetch_disk(url, x$path, handle = handle) :\nUnrecognized content encoding type. libcurl understands deflate, gzip content encodings.\nThanks to this post, I installed the missing package : stringi.\nOnce this error was addressed, I had a different one :\nError: Invalid or corrupt jarfile /home/aurelien/.local/share/binman_seleniumserver/generic/4.0.0-alpha-2/selenium-server-standalone-4.0.0-alpha-2.jar\nThis time the problem came from a corrupted file. Thanks to this post, I knew that I just had to download this file selenium-server-standalone-4.0.0-alpha-2.jar from the official selenium website and replace the corrupted file with it.\nI hope this will help some of you to install RSelenium with Ubuntu 18.04 LTS !\n\n\nOpening a web browser\nAfter addressing the errors above, I can now open a firefox browser :\n\nlibrary(RSelenium)\n\n#Open a firefox driver\nrD &lt;- rsDriver(browser = \"firefox\") \nremDr &lt;- rD[[\"client\"]]\n\n\n\nLogging in Aliexpress\nThe first step to scrape product prices on Aliexpress is to log in into your account:\n\nlog_id &lt;- \"Your_mail_adress\"\npassword &lt;- \"Your_password\"\n\n# Navigate to aliexpress login page \nremDr$navigate(\"https://login.aliexpress.com/\")\n\n# Fill the form with mail address\nremDr$findElement(using = \"id\", \"fm-login-id\")$sendKeysToElement(list(log_id))\n\n# Fill the form with password\nremDr$findElement(using = 'id', \"fm-login-password\")$sendKeysToElement(list(password))\n\n#Submit the login form by clicking Submit button\nremDr$findElement(\"class\", \"fm-button\")$clickElement()\n\n\n\nNavigating through the URLs and scraping the prices\nNow we have to navigate through a vector containing the URL of the aliexpress products we are interested in. Then we extract the price of the product by using the xpath of the product price of the webpage. The xpath of the element you want to scrape can be found by using the developer tool of chrome or firefox ( tutorial here ! ). Once the price is extracted we have to ensure this price is in numerical format by removing any special character (euro or dollar sign) and replace the comma by a point for the decimal separator. Here is the R code:\n\n  url_list &lt;- list(\"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Craws-Soft-Fishing-Lures-110mm-11-5g-Artificial-Bait-Soft-Bait-Craws-Lures/406467_32419930548.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-Fishing-Lure-5Pcs-Lot-155mm-7-4g-3-colors-Swimbait-Artificial-Lizard-Soft-Fishing-Lures/406467_32613648610.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Soft-Fishing-Lures-Minnow-Biat-95mm-6g-Jerkbait-Soft-Bait/406467_32419066106.html?spm=a2g0w.12010612.0.0.25fe5872CBqy0m\") \n\n# Allocate a vector to store the price of the products \ncurrentp &lt;- c()\nfor(i in 1:length(url_list)){\n  \n  # Navigate to link [i]\n  remDr$navigate(url_list[i])\n  \n  # Find the price with an xpath selector and findElement.  \n  # Sometimes products can be removed and this could throw an error this is why we are using 'try' to handle the potential errors\n  \n  current &lt;- try(remDr$findElement(using = \"xpath\",'//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"product-price-value\", \" \" ))]'), silent = T)\n  \n  #If error : current price is NA \n  if(class(current) =='try-error'){\n    currentp[i] &lt;- NA\n  }else{\n    # Get the price \n    text &lt;- unlist(current$getElementText())\n    \n    #Remove euro sign\n    text &lt;- gsub(\"[^A-Za-z0-9,;._-]\",\"\",text)\n    \n    #Case when there is a range of price instead of one price + replace comma by point\n    if(grepl(\"-\", text)) {  \n      pe &lt;- sub(\"-.*\",\"\",text) %&gt;% sub(\",\", \".\", ., fixed = TRUE)\n      currentp[i] &lt;-  as.numeric(pe)\n    }else{\n      currentp[i] &lt;- as.numeric(sub(\",\", \".\", text, fixed = TRUE))\n  }\n  }\n  \nSys.sleep(4)\n}\n\nBetween each link it is advised to wait a few seconds with Sys.sleep(4) to avoid being black-listed by the website.\n\n\nPhantomjs version\nIf you execute the code above, you should see a firefox browser open and navigate through the list you provided. In case you don‚Äôt want an active window, you can replace firefox by phantomjs browser which is a headless browser (without a window).\nI don‚Äôt know why but using rsDriver(browser = \"phantomjs\") does not work for me. I found this post which propose to start the phantomjs browser with the wdman package:\n\nlibrary(wdman)\nlibrary(RSelenium)\n# start phantomjs instance\nrPJS &lt;- wdman::phantomjs(port = 4680L)\n\n# is it alive?\nrPJS$process$is_alive()\n\n#connect selenium to it?\nremDr &lt;-  RSelenium::remoteDriver(browserName=\"phantomjs\", port=4680L)\n\n# open a browser\nremDr$open()\n\nremDr$navigate(\"http://www.google.com/\")\n\n# Screenshot of the headless browser to check if everything is working\nremDr$screenshot(display = TRUE)\n\n# Don't forget to close the browser when you are finished ! \nremDr$close()\n\n\n\nConclusion\nOnce you have understand the basics of RSelenium and how to select elements inside HTML pages, it is really easy to write a script to scrape data on the web. This post was a short example to scrape the product price on Aliexpress pages but the script can be extended to scrape more data on each page such as the name of the item, its rating etc‚Ä¶ It is even possible to automate this script to run daily in order to see price changes over time. As you see possibilities are endless!"
  },
  {
    "objectID": "posts/2024-09-19-exploring-value-remote-sensing-agriculture/index.html",
    "href": "posts/2024-09-19-exploring-value-remote-sensing-agriculture/index.html",
    "title": "Exploring the Value of Remote Sensing in Agriculture",
    "section": "",
    "text": "I just published a new article on Medium exploring the role of remote sensing in agriculture, specifically how radar data can help estimate precipitation more effectively ! üì°\nKnowing exactly how much rain has fallen on a field is critical for farmers. It impacts decisions on irrigation, disease prevention, and overall crop health. Traditionally, rain gauges have been the gold standard for measuring precipitation, but their reach is limited to where they are physically installed.\nWith remote sensing methods such weather radars and satellites, we can have spatially continuous precipitation estimates, offering a broader perspective that ground stations alone cannot match. But how reliable are they for agriculture-specific applications? At Sencrop, we‚Äôre investigating whether radar data can:\n\nComplement our network of weather stations, especially in less dense areas\nImprove the quality of precipitation data through gap filling and anomaly detection\nMeet the precision required for hourly rainfall estimates, essential for irrigation and disease forecasting\n\nIn this post, I compare radar data with traditional weather station measurements to see how well it aligns with agricultural needs.\nCurious about how radar can enhance weather-based decision-making in farming? Read the full article here: Exploring the Value of Remote Sensing in Agriculture: Making Sense of Radar Data\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-07-17-sencrop-data-quality/index.html",
    "href": "posts/2024-07-17-sencrop-data-quality/index.html",
    "title": "The Importance of Data Quality Control in Meteorology",
    "section": "",
    "text": "Long time no see! I‚Äôve been busy settling into my new position, but I‚Äôm back with a fresh article on Medium. üöÄ\nWhen working with weather data, ensuring data quality is not just a nice-to-have‚Äîit‚Äôs a necessity. Weather forecasts, decision-support tools, and climate analyses all rely on accurate measurements. But what happens when a sensor malfunctions, a station is installed incorrectly, or‚Äîbelieve it or not‚Äîa bird decides to nest in a rain collector? üê¶\nAt Sencrop, the network of weather stations fuels a variety of downstream processes, from simple aggregations to complex agricultural decision-making tools. Without robust anomaly detection, these processes could be thrown off by faulty measurements, leading to inaccurate insights.\nIn this article, I explore:\n\nWhy data quality control matters in meteorology\nCommon data quality control methods and why they are not adapted to our case\nHow we implement an inovative anomaly detection to keep our data reliable\n\nIf you‚Äôre curious about how to keep your data clean and meaningful, check it out here: Sencrop‚Äôs data quality control: Beyond the Z-score\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "",
    "text": "A few weeks ago, I started looking for a data scientist position in industry. My first moves were:\nAfter reading numerous job posts and work several hours on my resume, I wondered if I could optimize these steps with R and Data Science. I therefore decided to scrape Indeed and analyze the data about data science jobs to:"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#loading-libraries",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#loading-libraries",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Loading libraries",
    "text": "Loading libraries\nThe first step is to import several packages:\n\n# General\nlibrary(tidyverse)\n# Webscraping \nlibrary(rvest)\nlibrary(RSelenium)\n# Geo data\nlibrary(tidygeocoder)\nlibrary(leaflet)\nlibrary(rnaturalearth)\nlibrary(sf)\n# NLP\nlibrary(udpipe)\nlibrary(textrank)\nlibrary(wordcloud)\n# Cleaning\nlibrary(stringr)\n# Additional functions presented at the end of the post \nsource('scraping_functions.R')"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#collect-the-data-with-web-scraping",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#collect-the-data-with-web-scraping",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Collect the data with web scraping",
    "text": "Collect the data with web scraping\nIn the beginning of this project, I was using read_html() from rvest to access and download the webpage from Indeed. However, Indeed pages are protected by an anti-scrapping software that blocked any of my requests even though scraping is not forbidden on the pages I am interested in (I checked the robots.txt page).\nThis is why I decided to access the pages with Rselenium which allows to run an headless browser. We first navigate to the page corresponding to the search results of data scientist jobs in France:\n\nurl = \"https://fr.indeed.com/jobs?q=data%20scientist&l=France&from=searchOnHP\"\n\n# Headless Firefox browser\nexCap &lt;- list(\"moz:firefoxOptions\" = list(args = list('--headless')))\nrD &lt;- rsDriver(browser = \"firefox\", extraCapabilities = exCap, port=1111L,\n                verbose = F)\nremDr &lt;- rD$client\n\n# Navigate to the url\nremDr$navigate(url)\n\n# Store page source \nweb_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n\nTo scrape a specific information on a webpage you need to follow these steps:\n\nFind on the web page the element/text/data you want to scrape\nFind the associated xpath or css selector with the developer tool of chrome or firefox ( tutorial here ! )\nExtract the element with hmtl_element() by indicating the xpath or css selector\nTransform the data to text with html_text2()\nClean the data if necessary\n\nHere is the example with the number of listed data science jobs in France:\n\nweb_page %&gt;%\n  html_element(css = \"div.jobsearch-JobCountAndSortPane-jobCount\") %&gt;% # selecting with css \n  html_text2() %&gt;% # Transform to text\n  str_remove_all(\"[^0-9.-]\") %&gt;% # Clean the data to only get numbers\n  substr(start = 2, stop = 8) %&gt;% \n  as.numeric()\n\nFor now, we can only scrape the data from the first page. However, I am interested in all the job posts and I need to access the other pages ! After navigating through the first 3 pages of listed jobs, I remarked a pattern in the URL address (valid at the time of writing), this means that with a line of code, I can produce a list containing the URLs for the first 40 pages.\nOnce I have the list, the only thing left is to loop over all the URLs with some delay (good practice for web-scraping), collect the data and clean it with custom functions (at the end of the post):\n\n# Creating URL link corresponding to the first 40 pages\nbase_url = \"https://fr.indeed.com/jobs?q=data%20scientist&l=France&start=\"\nurl_list &lt;- c(url, paste0(base_url, as.character(seq(from=10, to=400, by=10))))\n\n# Looping through the URL list\nres &lt;- list()\nfor(i in 1:length(url_list)){\n  # Navigate to the URL\n  remDr$navigate(url_list[i])\n  \n  # Store page source \n  web_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n\n  # Job title \n  job_title &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\") %&gt;%\n    html_elements(css = \".resultContent\") %&gt;%\n    html_element(\"h2\") %&gt;%\n    html_text2() %&gt;%\n    str_replace(\".css.*;\\\\}\", \"\")\n\n  # URL for job post \n  job_url &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_elements(css = \".resultContent\") %&gt;%\n    html_element(\"h2\") %&gt;%\n    html_element(\"a\") %&gt;%\n    html_attr('href') %&gt;%\n    lapply(function(x){paste0(\"https://fr.indeed.com\", x)}) %&gt;%\n    unlist()\n  \n  # Data about company\n  company_info &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_elements(css = \".resultContent\")%&gt;%\n    html_element(css = \".company_location\")%&gt;%\n    html_text2() %&gt;%\n    lapply(FUN = tidy_comploc) %&gt;% # Function to clean the textual data\n    do.call(rbind, .)\n\n  # Data about job description\n  job_desc &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_element(css =\".slider_container .jobCardShelfContainer\")%&gt;%\n    html_text2() %&gt;%\n    tidy_job_desc() # Function to clean the textual data related to job desc.\n\n  # Data about salary (when indicated)\n  salary_hour &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result .resultContent\")%&gt;%\n    html_element(css = \".salaryOnly\") %&gt;%\n    html_text2() %&gt;%\n    lapply(FUN = tidy_salary) %&gt;% # Function to clean the data related to salary\n    do.call(rbind, .)\n  \n  # Job posts in the same format\n  final_df &lt;- cbind(job_title, company_info, salary_hour, job_desc, job_url)\n  colnames(final_df) &lt;- c(\"Job_title\", \"Company\", \"Location\", \"Rating\", \"Low_salary\", \"High_salary\", \"Contract_info\", \"Job_desc\", \"url\")\n  res[[i]] &lt;- final_df\n  \n  # Sleep 5 seconds, good practice for web scraping\n  Sys.sleep(5)\n}\n\n# Gather all the job post in a tibble\nfinal_df &lt;- as_tibble(do.call(\"rbind\", res))\n\n# Final data cleaning\nfinal_df &lt;- final_df %&gt;%\n  mutate_at(c(\"Rating\", \"Low_salary\", \"High_salary\"), as.numeric)\n\n# Clean job title\nfinal_df$Job_title_c &lt;- clean_job_title(final_df$Job_title)  \nfinal_df$Job_title_c &lt;- as.factor(final_df$Job_title_c)\n\nWe have now a tidy data set! Here is a truncated example of the 5 first rows:\n\n\n\n\n\n\nJob_title\nCompany\nLocation\nRating\nLow_salary\nHigh_salary\nContract_info\nJob_desc\nJob_type\nJob_title_c\n\n\n\n\nData Scientist junior (H/F)\nKea & Partners\n92240 Malakoff\nNA\n3750\n4583\nCDI +2 | Travail en journ√©e +1\nPlusieurs postes √† pourvoirMaitrise de Python et des packages de data science. 1er cabinet europ√©en de conseil en strat√©gie √† devenir Soci√©t√© √† Mission, certifi√©s B-Corp depuis 2021*,‚Ä¶\nPr√©sentiel\ndata scientist junior\n\n\nData Scientist (F ou H)\nSNCF\nSaint-Denis (93)\n3.9\nNA\nNA\nCDI\nLe d√©veloppement informatique (C, C++, Python, Azure, ‚Ä¶). Valider et recetter les phases des projets. Travailler avec des m√©thodes agiles avec les √©quipes et‚Ä¶\nPr√©sentiel\ndata scientist\n\n\nData Scientist (H/F) (IT)\nYzee Services\nParis (75)\n3.3\n2916\n3750\nTemps plein\nRecueillir, structurer et analyser les donn√©es pertinentes pour l'entreprise (activit√© li√©e √† la relation client, conseil en externe).\nPr√©sentiel\ndata scientist\n\n\nData Scientist H/F\nNatan (SSII)\nParis (75)\nNA\n4583\n5833\nCDI +1 | Travail en journ√©e\nPlusieurs postes √† pourvoirVous retrouverez une *ESN ambitieuse port√©e par le go√ªt de l‚Äôexcellence.*. Au sein du d√©partement en charge d'automatisation transverse des besoins de la‚Ä¶\nPr√©sentiel\ndata scientist\n\n\nData Scientist Junior H/F / Freelance\nkarma partners\nRoissy-en-Brie (77)\nNA\n400\n550\nTemps plein +1\nLe profil recherch√© est un profil junior (0-2 ans d'exp√©rience) en data science, avec une app√©tence technique et des notions d'architecture logicielle et de‚Ä¶\nPr√©sentiel\ndata scientist junior\n\n\n\n\n\n\n\n\nVisualization of the proposed salaries\nLet‚Äôs see if we can get some insights about data science jobs by making some graphical representations. The first thing I wanted to know is how much the companies are willing to pay in order to recruit a data science candidate. I therefore decided to make some figures about the salary range depending on the company and the job title.\nBeware!\nThe following graphs must be taken with a grain of salt as they display a small sample of the data. Indeed, the salary was listed for only 14% of the job post. The insights or trends in these graphs may not be representative of companies that have not listed their proposed salary.\n\nSalary by company\nThe following graphic shows the monthly income listed by some companies (not all the companies list their proposed salary):\n\n# Function to make euro X scale \neuro &lt;- scales::label_dollar(\n  prefix = \"\",\n  suffix = \"\\u20ac\",\n  big.mark = \".\",\n  decimal.mark = \",\"\n)\n\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;% # To remove internships and freelance works\n  select(Company, Low_salary, High_salary) %&gt;%\n  group_by(Company) %&gt;%\n  summarize_if(is.numeric, mean) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n           Company = fct_reorder(Company, desc(-Mean_salary))) %&gt;%\n  ggplot(aes(x = Company)) +\n  geom_point(aes(y = Mean_salary), colour = \"#267266\") +\n  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +\n  geom_hline(aes(yintercept = median(Mean_salary)), lty=2, col='red', alpha = 0.7) +\n  scale_y_continuous(labels = euro) +\n  ylab(\"Monthly income\") +\n  xlab(\"\") +\n  coord_flip() +\n  theme_bw(base_size = 8)\n\n\n\n\n\n\n\n\nThe median monthly salary is around 3700 euros. As you can see the salaries can vary a lot depending on the company. This is partly due because I didn‚Äôt make distinction between the different data science jobs (data scientist, data analyst, data engineer, senior or lead).\n\n\nSalary by job title\nWe can plot the same graph but instead of grouping by company we can group by job title:\n\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;%  # To remove internships and freelance works\n  select(Job_title_c, Low_salary, High_salary, Job_type) %&gt;%\n  group_by(Job_title_c) %&gt;%\n  summarize_if(is.numeric, ~ mean(.x, na.rm = TRUE)) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n         Job_title_c = fct_reorder(Job_title_c, desc(-Mean_salary))) %&gt;%\n  ggplot(aes(x = Job_title_c, y = Mean_salary)) +\n  geom_point(aes(y = Mean_salary), colour = \"#267266\") +\n  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +\n  #geom_label(aes(label = n, Job_title_c, y = 1500), data = count_df) + \n  scale_y_continuous(labels = euro) +\n  theme_bw(base_size = 12) +\n  xlab(\"\") +\n  ylab(\"Monthly Income\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nWe clearly see the differences in proposed salaries depending on the job title: data scientists seem to earn slightly more in average than data analysts. The companies also seem to propose higher salaries for jobs with more responsibilities or requiring more experiences (senior, lead).\n\n\nSalary depending on location: full remote, hybrid, on site ?\nFinally we can plot the salaries depending on the location (full remote, hybrid, on site) to see if it has an impact:\n\n# Tidy the types and locations of listed jobs\nfinal_df &lt;- tidy_location(final_df)\ncount_df &lt;- count(final_df %&gt;% filter(Low_salary &gt; 1600), Job_type)\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;% \n  drop_na(Location) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n         Job_type = as.factor(Job_type)) %&gt;%\n    ggplot(aes(x = Job_type, y = Mean_salary)) +\n  geom_boxplot(na.rm = TRUE) +\n  geom_label(aes(label = n, Job_type, y = 5500), data = count_df) + \n  scale_y_continuous(labels = euro) + \n  theme_bw(base_size = 12) +\n  xlab(\"Job Type\") +\n  ylab(\"Income\")\n\n\n\n\n\n\n\n\nIt is worth noting that most of the jobs proposed in France are on site jobs. The median salary for this type of jobs is slightly lower than hybrid jobs. The salary distribution of full remote and hybrid jobs must be taken with care as it is only represented by 12 job posts.\n\n\n\nMapping job locations\nDuring my job search, I was frustrated not to see a geographical map regrouping the locations of all the proposed jobs. Such map could help me greatly in my search. Let‚Äôs do it !\nFirst, we must tidy and homogenize the locations for all the job posts. To this end, I made a custom function (tidy_location()) which includes some stringr functions, you can find more details about this function at the end of this post. It outputs the location in this format [Town]([Zip code]). Even though all the locations have been homogenized, it can not be plotted on a map (we need the longitude and latitude). To get the latitude and longitude with the town name and zip code I used the geocode() function from tidygeocoder package.\n\n# Extract coordinates from town name\nfinal_df &lt;- final_df %&gt;%\n  mutate(Loc_tidy_fr = paste(Loc_tidy, 'France')) %&gt;%\n  geocode(Loc_tidy_fr, method = 'arcgis', lat = latitude , long = longitude) %&gt;%\n  select(- Loc_tidy_fr)\n\n\nDistribution of Data Science jobs in France\nWe can now represent the number of Data Science jobs by departments:\n\n# Map of France from rnaturalearth package\nfrance &lt;- ne_states(country = \"France\", returnclass = \"sf\") %&gt;% \n  filter(!name %in% c(\"Guyane fran√ßaise\", \"Martinique\", \"Guadeloupe\", \"La R√©union\", \"Mayotte\"))\n\n# Transform location to st point \ntest &lt;- st_sf(final_df, geom= lapply(1:nrow(final_df), function(x){st_point(c(final_df$longitude[x],final_df$latitude[x]))}))\nst_crs(test) &lt;- 4326\n\n# St_join by departments \njoined &lt;- france %&gt;%\n  st_join(test, left = T)\n\n# Custom breaks for visual representation\nmy_breaks = c(0, 2, 5, 10, 30, 50, 100, 260)\n\njoined %&gt;% \n  mutate(region=as.factor(name)) %&gt;% \n  group_by(region) %&gt;% \n  summarize(Job_number=n()) %&gt;% \n  mutate(Job_number = cut(Job_number, my_breaks)) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill=Job_number), col='grey', lwd=0.2) + \n  scale_fill_brewer(\"Job number\",palette = \"GnBu\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nIt is really interesting to see that the distribution of jobs is quite heterogeneous in France. The majority of the jobs are concentrated in a few departments that include a large city. It is expected as most of the jobs are proposed by large company that are often installed in the proximity of important cities.\n\n\nInteractive map\nWe can go further and plot an interactive map with leaflet which allows us to search dynamically for a job post:\n\n# Plot leaflet map\nfinal_df %&gt;%\n  mutate(pop_up_text = sprintf(\"&lt;b&gt;%s&lt;/b&gt; &lt;br/&gt; %s\",\n                                     Job_title, Company)) %&gt;% # Make popup text\n  leaflet() %&gt;%\n  setView(lng = 2.36, lat = 46.31, zoom = 5.2) %&gt;% # Center of France\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addMarkers(\n    popup = ~as.character(pop_up_text),\n    clusterOptions = markerClusterOptions()\n  )"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#analyzing-job-descriptions",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#analyzing-job-descriptions",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Analyzing job descriptions",
    "text": "Analyzing job descriptions\nNowadays most of the resumes are scanned and interpreted by an applicant tracking system (ATS). To make things simple, this system looks for key words in your resume and assess the match with the job you are applying for. It is therefore important to describe your experiences with specific key words to improve the chances of getting to the next step of the hiring process.\nBut what key words should I include in my resume ? Let‚Äôs answer this question by analyzing the job descriptions of data scientist jobs.\n\nDownloading and cleaning each job description\nFirst we download the full description of each job by navigating through all the URL listed in our table. We then clean and homogenize the description with a custom function:\n\n# Loop through all the URLs\njob_descriptions &lt;- list()\npb &lt;- txtProgressBar(min = 1, max = length(final_df$url), style = 3)\nfor(i in 1:length(final_df$url)){\n  remDr$navigate(final_df$url[i])\n  web_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n  job_descriptions[[i]] &lt;- web_page %&gt;%\n        html_elements(css = \".jobsearch-JobComponent-description\") %&gt;%\n      html_text2()\n  Sys.sleep(2)\n  setTxtProgressBar(pb, i)\n}\n# Gathering in dataframe\njob_descriptions &lt;- as.data.frame(do.call(\"rbind\", job_descriptions))\nnames(job_descriptions) &lt;- c(\"Description\")\n\n# Binding to same table:\nfinal_df &lt;- cbind(final_df, job_descriptions)\n\n# Homogenize with custom function\nfinal_df$Description_c &lt;- lapply(final_df$Description, function(x){clean_job_desc(x)[[2]]})\nfinal_df$Language &lt;- textcat::textcat(final_df$Description)\n\n\n\nAnnotation procedure with udpipe Package\nThis part is inspired from this post.\nNow that the descriptions of all the listed jobs are imported and pre-cleaned, we can annotate the textual data with udpipe package. This package contains functions and models which can perform tokenisation, lemmatisation and key word extraction.\nWe first restrict this analysis to data scientist job post written in french, then we annotate all the descriptions:\n\n# Restricting the analysis to Data scientist post written in french\ndesc_data_scientist &lt;- final_df %&gt;%\n  filter((Job_title_c == \"data scientist\") & (Language == \"french\")) %&gt;%\n  select(Description_c)\n\nud_model &lt;- udpipe_download_model(language = \"french\") # Download the model if necessary\nud_model &lt;- udpipe_load_model(ud_model$file_model) \n\n# Annotate the descriptions \nx &lt;- udpipe_annotate(ud_model, x = paste(desc_data_scientist, collapse = \" \"))\nx &lt;- as.data.frame(x)\n\n\n\nMost common nouns\nWe can visualize the most employed word throughout the data scientist job posts written in french:\n\nstats &lt;- subset(x, upos %in% \"NOUN\")\nstats &lt;- txt_freq(x = stats$lemma)\n\nstats %&gt;%\n  top_n(50, freq) %&gt;%\n  mutate(key = as.factor(key),\n         key = fct_reorder(key, freq)) %&gt;%\n  ggplot(aes(x = key, y = freq)) +\n  geom_bar(stat = 'identity') +\n  coord_flip() + \n  ylab(\"Most common nouns\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nEven though, it gives us an idea of words to include it is not very informative as key words are often composed by two or more words.\n\n\nExtracting key words for resume writing\nThere are several methods implemented in udpipe to extract key words from a text. After testing several methods, I selected the Rapid Automatic Keyword Extraction (RAKE) which gives me the best results:\n\nstats &lt;- keywords_rake(x = x,\n                       term = \"token\",# Search on token\n                       group = c(\"doc_id\", \"sentence_id\"), # On every post \n                       relevant = x$upos %in% c(\"NOUN\", \"ADJ\"),  # Only among noun and adj.\n                       ngram_max = 2, n_min = 2, sep = \" \")\n\nstats &lt;- subset(stats, stats$freq &gt;= 5 & stats$rake &gt; 3)\n\nstats %&gt;% \n  arrange(desc(rake)) %&gt;% \n  head()\n\n                    keyword ngram freq     rake\n1 intelligence artificielle     2    9 9.368889\n2             tableaux bord     2    5 8.504274\n3      formation sup√©rieure     2    5 8.374725\n4        mod√®les pr√©dictifs     2   15 7.581294\n5         force proposition     2    6 7.190238\n6        production √©chelle     2    5 7.034038\n\nwordcloud(words = stats$keyword, freq = stats$freq, min.freq = 3,\n          max.words=100, random.order=FALSE, rot.per=0.35,\n          colors=brewer.pal(8, \"Dark2\"), scale = c(2.5, .5))\n\n\n\n\n\n\n\n\nWe can see that this method has selected important french key words related to the data scientist job ! In the first positions, we find the key words: ‚Äúartificial intelligence‚Äù, ‚Äúdashboards‚Äù, ‚Äúhigher education‚Äù, ‚Äúpredictive model‚Äù. I‚Äôd better check if these words appear on my resume !"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#conclusion",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#conclusion",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Conclusion",
    "text": "Conclusion\nI hope I convinced you that it is possible to optimize your job search with Data Science!\nIf this post has caught your interest and you are looking for a new Data Scientist, do not hesitate to contact me on my mail as I am currently looking for a job in France (Hybrid, Remote) or in Europe (Remote)."
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#custom-functions-to-clean-data-extracted-from-the-webpage",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#custom-functions-to-clean-data-extracted-from-the-webpage",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Custom functions to clean data extracted from the webpage",
    "text": "Custom functions to clean data extracted from the webpage\nThese functions use several methods such as regular expressions, stop words and conditional statements to clean the textual data.\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(httr)\nlibrary(tidystopwords)\nlibrary(textcat)\n\n# Function to tidy the data related to the company\ntidy_comploc &lt;- function(text){\n  lst &lt;- str_split(text, pattern = \"\\n\", simplify =T)\n  ext_str &lt;- substr(lst[1], nchar(lst[1])-2, nchar(lst[1]))\n  res &lt;- suppressWarnings(as.numeric(gsub(',', '.', ext_str)))\n  lst[1] &lt;- ifelse(is.na(res), lst[1], substr(lst[1], 1, nchar(lst[1])-3))\n  lst[3] &lt;- res\n  t(as.matrix(lst))\n}\n\n# Function to tidy the short job description provided with the job post\ntidy_job_desc &lt;- function(text){\n  stopwords &lt;- c(\"Candidature facile\", \"Employeur r√©actif\")\n  text &lt;- str_remove_all(text, paste(stopwords, collapse = \"|\"))\n  stopwords_2 &lt;- \"(Posted|Employer).*\"\n  text &lt;- str_remove_all(text, stopwords_2)\n  text\n}\n\n# Function to tidy the salary data if provided\ntidy_salary &lt;- function(text){\n  if(is.na(text)){\n    others &lt;- NA\n    sal_low &lt;- NA\n    sal_high &lt;- NA\n  }else{\n    text &lt;- str_split(text, \"\\n\", simplify = T)\n    others &lt;- paste(text[str_detect(text, \"‚Ç¨\", negate = T)], collapse = \" | \")\n    sal &lt;- text[str_detect(text, \"‚Ç¨\", negate = F)]\n    if(rlang::is_empty(sal)){\n      sal_low &lt;- NA\n      sal_high &lt;- NA\n    }else{\n      range_sal &lt;- as.numeric(str_split(str_remove_all(str_replace(sal, \"√†\", \"-\"), \"[^0-9.-]\"), \"-\", simplify = TRUE))\n      sal_low &lt;- sort(range_sal)[1]\n      sal_high &lt;- sort(range_sal)[2]\n\n      if(str_detect(sal, \"an\")){\n        sal_low &lt;- floor(sal_low/12)\n        sal_high &lt;- floor(sal_high/12)\n      }\n    }\n  }\n  return(c(as.numeric(sal_low), as.numeric(sal_high), others))\n}\n\n# Function to tidy the location of the job (Remote/Hybrid/Onsite) + homogenize \n# location and zip code\ntidy_location &lt;- function(final_df){\n  final_df$Job_type &lt;- ifelse(final_df$Location == \"T√©l√©travail\", \"Full Remote\", ifelse(str_detect(final_df$Location, \"T√©l√©travail\"), \"Hybrid\", \"On site\"))\n  final_df$Loc_possibility &lt;- ifelse(str_detect(final_df$Location, \"lieu\"), \"Plusieurs lieux\", NA)\n  stopwords &lt;- c(\"T√©l√©travail √†\", \"T√©l√©travail\", \"√†\", \"hybride\")\n  final_df$Loc_tidy &lt;- str_remove_all(final_df$Location, paste(stopwords, collapse = \"|\"))\n  final_df$Loc_tidy &lt;- str_remove_all(final_df$Loc_tidy, \"[+].*\")\n  final_df$Loc_tidy &lt;- str_trim(final_df$Loc_tidy)\n  final_df$Loc_tidy &lt;-  sapply(final_df$Loc_tidy,\n                               function(x){\n                                 if(!is.na(suppressWarnings(as.numeric(substr(x, 1, 5))))){\n                                   return(paste(substr(x, 7, 30), paste0('(', substr(final_df$Loc_tidy[2], 1, 2), ')')))\n                                 }else{\n                                   return(x)\n                                 }})\n  return(final_df)\n}\n\n# Function to keep only certain words in text\nkeep_words &lt;- function(text, keep) {\n  words &lt;- strsplit(text, \" \")[[1]]\n  txt &lt;- paste(words[words %in% keep], collapse = \" \")\n  return(txt)\n}\n\n# Homogenize the job title and class them in a few categories\nclean_job_title &lt;- function(job_titles){\n  job_titles &lt;- tolower(job_titles)\n  job_titles &lt;- gsub(\"[[:punct:]]\", \" \", job_titles, perl=TRUE)\n\n  words_to_keep &lt;- c(\"data\", \"scientist\", \"junior\", \"senior\", \"engineer\", \"nlp\",\n                     \"analyst\", \"analytics\", \"analytic\", \"science\", \"sciences\",\n                     \"computer\", \"vision\", \"ingenieur\", \"donn√©es\", \"analyste\",\n                     \"analyses\", \"lead\", \"leader\", \"dataminer\", \"mining\", \"chief\",\n                     \"miner\", \"analyse\", 'head')\n  job_titles_c &lt;- unlist(sapply(job_titles, function(x){keep_words(x, words_to_keep)}, USE.NAMES = F))\n  job_titles_c &lt;- unlist(sapply(job_titles_c, function(x){paste(unique(unlist(str_split(x, \" \"))), collapse = \" \")}, USE.NAMES = F))\n  table(job_titles_c)\n\n  data_analytics_ind &lt;-  job_titles_c %in% c(\"analyses data\", \"analyst data\", \"analyste data\", \"analyste data scientist\", \"data analyse\",\n                                             \"analyste donn√©es\", \"analytic data scientist\", \"analytics data\", \"analytics data engineer\", \"data analyst engineer\",\n                                             \"data analyst donn√©es\", \"data analyst scientist\", \"data analyst scientist donn√©es\", \"data analyste\", \"data analyst analytics\",\n                                             \"data analytics\", \"data analytics engineer\", \"data engineer analyst\", \"data scientist analyst\", \"data scientist analytics\")\n  job_titles_c[data_analytics_ind] &lt;- \"data analyst\"\n\n  data_analytics_j_ind &lt;-  job_titles_c %in% c(\"junior data analyst\", \"junior data analytics\", \"junior data scientist analyst\")\n  job_titles_c[data_analytics_j_ind] &lt;- \"data analyst junior\"\n\n  data_scientist_ind &lt;- job_titles_c %in% c(\"data computer science\", \"data science\", \"data science scientist\", \"data sciences\",\n                                            \"data sciences scientist\", \"data scientist donn√©es\", \"data scientist sciences\",\n                                            \"donn√©es data scientist\", \"scientist data\", \"science donn√©es\", \"scientist data\",\n                                            \"scientist data science\", \"computer data science\", \"data science donn√©es\", \"data scientist science\")\n  job_titles_c[data_scientist_ind] &lt;- \"data scientist\"\n\n  data_scientist_j_ind &lt;- job_titles_c %in% c(\"junior data scientist\")\n  job_titles_c[data_scientist_j_ind] &lt;- \"data scientist junior\"\n\n  data_engineer_ind &lt;- job_titles_c %in% c(\"data engineer scientist\", \"data science engineer\", \"data miner\", \"data scientist engineer\",\n                                           \"dataminer\", \"engineer data scientist\", \"senior data scientist engineer\", \"ingenieur data scientist\")\n  job_titles_c[data_engineer_ind] &lt;- \"data engineer\"\n\n  nlp_data_scientist_ind &lt;- job_titles_c %in% c(\"data scientist nlp\", \"nlp data science\",\n                                                \"nlp data scientist\", \"senior data scientist nlp\")\n  job_titles_c[nlp_data_scientist_ind] &lt;- \"data scientist NLP\"\n\n  cv_data_scientist_ind &lt;- job_titles_c %in% c(\"computer vision data scientist\", \"data science computer vision\",\n                                               \"data scientist computer vision\")\n  job_titles_c[cv_data_scientist_ind] &lt;- \"data scientist CV\"\n\n  lead_data_scientist_ind &lt;- job_titles_c %in% c(\"chief data\", \"chief data scientist\", \"data scientist leader\", \"lead data scientist\",\n                                                 \"data chief scientist\", \"lead data scientist senior\", \"head data science\")\n  job_titles_c[lead_data_scientist_ind] &lt;- \"data scientist lead or higher\"\n  senior_data_scientist_ind &lt;- job_titles_c %in% c(\"senior data scientist\")\n  job_titles_c[senior_data_scientist_ind] &lt;- \"data scientist senior\"\n\n  senior_data_analytics_ind &lt;- job_titles_c %in% c(\"senior analytics data scientist\", \"senior data analyst\", \"senior data scientist analytics\")\n  job_titles_c[senior_data_analytics_ind] &lt;- \"data analyst senior\"\n\n\n  lead_data_analyst_ind &lt;- job_titles_c %in% c(\"lead data analyst senior\", \"lead data analyst\")\n  job_titles_c[lead_data_analyst_ind] &lt;- \"data analyst lead\"\n  return(job_titles_c)\n}\n\n# Function to clean the full job description before word annotation\nclean_job_desc &lt;- function(text){\n  text &lt;- tolower(text)\n  text &lt;- str_replace_all(text, \"\\n\", \" \")\n  text &lt;- str_remove(text, pattern = \"d√©.*du poste \")\n  text &lt;- str_remove(text, pattern = \"analyse de recr.*\")\n  text &lt;- gsub(\"(?!&)[[:punct:]+‚Äô+‚Ä¶+¬ª+¬´]\", \" \", text, perl=TRUE)\n\n  language &lt;- textcat(text)\n\n  if(language == \"french\"){\n    text &lt;- str_replace_all(text, \"≈ì\", \"oe\")\n    stopwords &lt;- c(\"d√©tails\", \"poste\", \"description\", \"informations\", \"compl√©mentaires\", \"c\", generate_stoplist(language = \"French\"))\n  }else{\n    stopwords &lt;- c(\"description\", generate_stoplist(language = \"English\"))\n  }\n\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n\n  return(c(language, text))\n}"
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.html",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.html",
    "title": "The end of my PhD journey",
    "section": "",
    "text": "Two weeks ago, I defended my thesis entitled ‚ÄòStatistical learning for coastal risks assessment‚Äô in front of an academic jury. The defense went extremely well and the jury and the auditory were genuinely interested by the work I had done during my thesis. This thesis was an amazing and enriching journey in terms of knowledge and research, but also in terms of collaboration and exchange. I now am excited to start a new journey and discover new subjects to work on !\nYou can find my thesis manuscript here:  http://www.theses.fr/2021PAUU3016. If you don‚Äôt have the courage to read the entire manuscript (I understand :wink:) you can find below a short abstract of my thesis and the slides I presented during my PhD defense."
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.html#abstract-of-my-thesis",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.html#abstract-of-my-thesis",
    "title": "The end of my PhD journey",
    "section": "Abstract of my thesis",
    "text": "Abstract of my thesis\nOver the last decades, the quantity of data related to coastal risk has greatly increased with the installation of numerous monitoring networks. In this era of big data, the use of statistical learning methods (SLM) in the development of local predictive models becomes more legitimate and justified. The objective of this thesis is to demonstrate how SLM can contribute to the improvement of coastal risk assessment tools and to the development of an early warning system which aims to reduce coastal flooding risk.\nThree methodologies have been developed and tested on real study sites. The first methodology aims to improve the local wave forecast made by spectral wave model with machine learning methods and data from monitoring networks. We showed that data assimilation with machine learning methods improve significantly the forecast of wave parameters especially the wave height and period. The second methodology concerns the creation of storm impact databases. Even though these databases are essential for the disaster risk reduction process they are rare and sparse. We therefore proposed a methodology based on a deep learning method (convolutional neural networks) to generate automatically qualitative storm impact data from images provided by video monitoring stations installed on the coast. The last methodology is about the development of a storm impact model with a statistical method (bayesian network) based exclusively on data acquired with diverse monitoring networks. With this methodology we were able to predict qualitatively the storm impact on our study site, the Grande Plage of Biarritz.\n\n\n\nOrganization of my PhD manuscript"
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.html#my-presentation-for-the-phd-defense",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.html#my-presentation-for-the-phd-defense",
    "title": "The end of my PhD journey",
    "section": "My presentation for the PhD defense",
    "text": "My presentation for the PhD defense\nYou can find my presentation here"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "",
    "text": "My favorite hobby, in addition to R coding of course, is fishing. Most of the time, I fish European sea bass (Dicentrarchus labrax) in estuaries. The sea bass is a predatory fish that has a broad range of preys: crabs, sand eels, prawns, shrimps and other fish. To catch these predators, I don‚Äôt use live baits, I prefer to use artificial lures that imitate a specific prey.\nIn theory, it is quite easy to catch a fish:\nIn practice, it is an other story ! Indeed, the feeding activity, the position of the European sea bass in the estuary and their preys will vary depending on different parameters:\nAs you understand, there are many parameters potentially influencing the results of my fishing session. This is why I decided to create a shiny application to augment the number and the length of the fish caught during my sessions. To reach this objective, I need to better understand the activity, the position and the prey of the sea bass depending on the parameters described above."
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html#requirements-of-my-application",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html#requirements-of-my-application",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "Requirements of my application",
    "text": "Requirements of my application\n\nIt must store data about my fishing session:\n\n\n\n\n\n\n\n\n\nInformation needed\nDescription of the variables\nWhere do I get the data ?\n\n\n\n\nTime\nTime when a fish is caught, time since the beginning of the session\nR\n\n\nCatch\nSpecies and length of the fish caught\nGeolocation from smartphone?\n\n\nLures\nType, length, color of lure used\nWeather API\n\n\n\n\nIt must record data about my catch and the artificial lures used:\n\n\n\n\n\n\n\n\n\nInformation needed\nDescription of the variables\nWhere do I get the data ?\n\n\n\n\nTime\nTime when a fish is caught, time since the beginning of the session\nR\n\n\nCatch\nSpecies and length of the fish caught\nUser input\n\n\nLures\nType, length, color of lure used\nUser input\n\n\n\n\nIt must be adapted to small screens because I will always use the application on my phone.\nIt must remain free."
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html#collecting-the-data",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html#collecting-the-data",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "Collecting the data",
    "text": "Collecting the data\n\nGetting my gps location\nMy gps location is collected by using a bit of Javascript in the header of the shiny application. This code has been developed by AugusT and is available on his github repository.\n\n\nWeather API\nFor the weather data, I found a free API called Dark Sky. I made a function that takes as input the coordinates of a place and the API user key and returns the current weather conditions in a dataframe:\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(rvest)\n\nweather &lt;- function(x, API_key){\n  url &lt;- paste0(\"https://api.darksky.net/forecast/\",API_key,\n                \"/\", x[1], \",\", x[2],\n                \"?units=ca&exclude=hourly,alerts,flags\")\n  \n  rep &lt;- GET(url)\n  \n  table &lt;- fromJSON(content(rep, \"text\"))\n  \n  current.weather.info &lt;- with(table,\n                               data.frame(Air_temp = currently$temperature,\n                                     Weather = currently$summary,\n                                     Atm_pres = currently$pressure,\n                                     Wind_str = currently$windSpeed,\n                                     Wind_dir = currently$windBearing,\n                                     Cloud_cover = currently$cloudCover,\n                                     PrecipProb = currently$precipProbability,\n                                     PrecipInt = currently$precipIntensity,  \n                                     Moon = daily$data$moonPhase[1]))\n  return(current.weather.info)\n}\n\n\n\nWeb scrapping for Tide data\nI created a function to scrap information about the tide on a french website. The following function takes no argument and return the current water level, the tide status (going up or down) and time since the tide peak for the location I fish.\n\ntide &lt;- function(){\n  \n  # Set the current time and time zone \n  Sys.setenv(TZ=\"Europe/Paris\")\n  time &lt;- as.POSIXct(Sys.time())\n  url &lt;- \"https://services.data.shom.fr/hdm/vignette/grande/BOUCAU-BAYONNE?locale=en\"\n  \n  # Read the web page that contains the tide data \n  text &lt;- url %&gt;% \n    read_html() %&gt;%\n    html_text()\n  \n  # Clean the html data to get a dataframe  with two cols Time and water level: \n\n  text &lt;- as.character(sub(\".*var data = *(.*?) *\\\\;.*\", \"\\\\1\", text))\n  text &lt;- unlist(str_split( substr(text, 1, nchar(text)-2), \"\\\\],\"))\n  tidy_df &lt;- data.frame(hour=NA,Water=NA)\n  \n  for(i in 1:length(text)){\n    text_dat &lt;- unlist(str_split(text[i], '\"'))[c(2,3)]\n    text_dat[1] &lt;- substr(text_dat[1], 1, nchar(text_dat[1])-1)\n    text_dat[2] &lt;- as.numeric(substr(text_dat[2], 2, nchar(text_dat[2])))\n    tidy_df[i,] &lt;- text_dat\n  }\n  \n  tidy_df$hour &lt;- as.POSIXct(paste(format(Sys.time(),\"%Y-%m-%d\"), tidy_df$hour))\n  \n  # Some lines to get the tide status (going down or up) : \n  \n  n_closest &lt;- which(abs(tidy_df$hour - time) == min(abs(tidy_df$hour - time)))\n  \n  water_level &lt;- as.numeric(tidy_df[n_closest, 2])\n  \n  all_decrea &lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==\n                      cummin(tidy_df$Water[(n_closest-6):(n_closest+6)] ))\n  \n  all_increa &lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==\n                      cummax(tidy_df$Water[(n_closest-6):(n_closest+6)] ))\n  \n  maree &lt;- ifelse(all_decrea, \"Down\", ifelse(all_increa, \"Up\", \"Dead\"))\n  \n  \n  # Compute time since the last peak :\n  \n  last_peak &lt;- max(cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &gt; 0)$lengths)\n                   [cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &gt;0)$lengths) &lt; n_closest])\n  \n  \n  time_after &lt;- as.numeric(difftime(tidy_df$hour[n_closest], tidy_df$hour[last_peak], units = \"mins\"))\n  \n  \n  # Return the list with the results :\n  \n  return(list(Water_level = water_level,\n              Maree = maree,\n              Time_peak = time_after))\n  \n}"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html#the-shiny-application",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html#the-shiny-application",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "The shiny application",
    "text": "The shiny application\nThe main problem I encountered while developing this application was data storage. Shinyapps.io host freely your shiny application but there were some problems when I used the shiny application to modify the csv files. The solution I found was to store the data in my dropbox account, you can find here more details on the subject and alternatives solutions. I used the package rdrop2 to access and modify the data with the shiny application.\nHere are the main steps of this application :\n\nWhen the application is started, it reads a csv file stored on my dropbox to see if a fishing session is running or not. If not the user can start a fishing session.\nWhen starting a new session, a line with coordinates, weather conditions, and tide condition is added to the csv file previously mentioned.\nIf a fish is caught, the user can fill out a form to store the data in a second csv file. This file contains : the time, the species and length of the fish and information about the fishing lure used (type, color, length).\nThe user can end the fishing session by pushing a button. This will register the ending time, weather conditions, and tide condition in the first csv file.\n\nA simplified graph is showed below:\n\n\n\nSimplified workflow of the application\n\n\n\nUI side\nThe user interface of the application is built using the miniUI package. This package allows R user to develop shiny application adapted to small screens.\n\n# Load libraries \nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(googlesheets)\nlibrary(miniUI)\nlibrary(leaflet)\nlibrary(rdrop2)\nSys.setenv(TZ=\"Europe/Paris\")\n\n#Import the functions for weather API and webscrapping \nsuppressMessages(source(\"api_functions.R\"))\n\n\n# Load the dropbox token : \ntoken &lt;&lt;- readRDS(\"token.rds\")\n\n# Minipage for small screens\nui &lt;- miniPage(\n  # Javascript that give user location (input$lat,input$long)\n  tags$script('$(document).ready(function () {\n                           navigator.geolocation.getCurrentPosition(onSuccess, onError);\n                           \n                           function onError (err) {\n                           Shiny.onInputChange(\"geolocation\", false);\n                           }\n                           \n                           function onSuccess (position) {\n                           setTimeout(function () {\n                           var coords = position.coords;\n                           console.log(coords.latitude + \", \" + coords.longitude);\n                           Shiny.onInputChange(\"geolocation\", true);\n                           Shiny.onInputChange(\"lat\", coords.latitude);\n                           Shiny.onInputChange(\"long\", coords.longitude);\n                           }, 1100)\n                           }\n                           });'),\n  \n  gadgetTitleBar(\"Catch them all\", left = NULL, right = NULL),\n  \n  miniTabstripPanel(\n    #First panel depends if a fishing session is started or not \n    miniTabPanel(\"Session\", icon = icon(\"sliders\"),\n                 miniContentPanel(uiOutput(\"UI_sess\", align = \"center\"),\n                                  uiOutput(\"UI\", align = \"center\"))\n    ),\n    # Second panel displays the location of the previous fishing session with the number of fish caught \n    miniTabPanel(\"Map\", icon = icon(\"map-o\"),\n                 miniContentPanel(scrollable = FALSE,padding = 0,\n                                  div(style=\"text-align:center\",\n                                      prettyRadioButtons(\"radio\", inline = TRUE, label = \"\",\n                                                         choices = list(\"3 derni√®res sessions\" = 1,\n                                                                        \"3 Meilleures Sessions\" = 2,\n                                                                        \"Tout afficher\" = 3), \n                                                         selected = 1)),\n                                  leafletOutput(\"map\", height = \"93%\")\n                 ))\n  )\n  \n)\n\n\n\nServer side\nThe server side is mainly composed by observeEvent functions. The utility of each observeEvent is provided in the script as commentary.\n\nserver &lt;- function(input, output, session){\n  source(\"api_functions.R\")\n  \n  # Read the csv file containing information about fishing session. If a session is running,\n  # display the UI that allows the user to input data about the fish caught. If a session is not started,\n  # display a button to start the session.\n  \n  observeEvent(input$go ,{\n    \n    dat &lt;&lt;- drop_read_csv(\"/app_peche/session.csv\", header = T, stringsAsFactors = F, dtoken = token) \n    \n    output$UI&lt;- renderUI({\n      tagList(\n        if(rev(dat$Status)[1] == \"end\"){\n          actionButton(\"go\",\"Start session\")}\n        else{\n          actionButton(\"go\",\"End session\") \n        }\n      )\n    })\n    \n    output$UI_sess&lt;- renderUI({\n      if(rev(dat$Status)[1] == \"end\"){\n        tagList(textInput(\"comments\", label = h3(\"Commentaires\"), value = \"NA\"))\n      }else{\n        input$catch\n        \n        tagList(\n          selectInput(\"species\", label = h3(\"Esp√®ces\"), \n                      choices = list(\"Bar\" = \"bar\", \n                                     \"Bar mouchet√©\" = \"bar_m\", \n                                     \"Alose\" = \"alose\",\n                                     \"Alose Feinte\" = \"alose_f\",\n                                     \"Maquereau\" = \"maquereau\", \n                                     \"Chinchard\" = \"chinchard\"), selected = \"bar\"),\n          \n          sliderInput(\"length\",label = h3(\"Taille du poisson\"),value=25,min=0,max=80, step=1),\n          \n          selectInput(\"lure\", label = h3(\"Type de leurre\"), \n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"), selectize = FALSE),\n          \n          selectInput(\"color_lure\", label = h3(\"Couleur du leurre\"), \n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ), selectize = FALSE),\n          \n          selectInput(\"length_lure\", label = h3(\"Taille du leurre\"), \n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"), selectize = FALSE),\n          \n          actionButton(\"catch\",\"Rajoutez cette capture aux stats!\"),\n          \n          textInput(\"comments1\", label = h3(\"Commentaire avant la fin ?\"), value = \"NA\")\n          \n          \n        )\n        \n        \n      }\n      \n    })  \n    \n    \n  }, ignoreNULL = F)\n  \n  #If the button is pushed, create the line to be added in the csv file. \n  \n  observeEvent(input$go,{\n    \n    #Tide + geoloc + Weather\n    c_tide &lt;- unlist(tide())\n    geoloc &lt;- c(input$lat,input$long)\n    current.weather.info &lt;- weather(geoloc) \n    \n    # Two outcomes depending if the session starts or ends. This gives the possibility \n    # to the user to add a comment before starting the session or after ending the session\n    \n    if(rev(dat$Status)[1] == \"end\"){\n      \n      n_ses &lt;- c(rev(dat$Session)[1]+1)\n      stat_ses &lt;- c(\"beg\")\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment &lt;- input$comments\n      dat.f &lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment)\n      names(dat.f)&lt;-names(dat)\n      a &lt;- rbind(dat,dat.f)\n      \n    }else{\n      \n      n_ses &lt;- c(rev(dat$Session)[1])\n      stat_ses &lt;- c(\"end\")\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment1 &lt;- input$comments1\n      dat.f&lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment1)\n      names(dat.f)&lt;-names(dat)\n      a &lt;- rbind(dat,dat.f)\n    }\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(a), \"session.csv\")\n    \n    # Upload it to dropbox account \n    drop_upload(\"session.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  \n  # Add a line to the catch csv file whenever a fish is caught\n  observeEvent(input$catch,{\n    caugth &lt;- drop_read_csv(\"/app_peche/catch.csv\", header = T, stringsAsFactors = F, dtoken = token) \n    \n    n_ses &lt;- c(rev(dat$Session)[1])\n    time &lt;- as.POSIXct(Sys.time())\n    time_after_beg &lt;- round(as.numeric(difftime(time, rev(dat$Time)[1], units = \"mins\")), digits = 0)\n    \n    catch &lt;- data.frame(n_ses, \n                        time = as.character(time),\n                        min_fishing = as.character(time_after_beg),\n                        species = input$species,\n                        length = input$length,\n                        lure = input$lure,\n                        colour = input$color_lure,\n                        length_lure = input$length_lure)\n    \n    b &lt;- rbind(caugth,catch)\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(b), \"catch.csv\")\n    # Upload it to dropbox account \n    drop_upload(\"catch.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  # Create the map with the results of previous session depending on the choice of the user :\n  \n  observeEvent(input$radio,{\n    \n    output$map &lt;- renderLeaflet({\n      map_data &lt;- map_choice(input$radio)\n      leaflet(map_data) %&gt;% addTiles() %&gt;%\n        addPopups(lng = ~Long,\n                  lat = ~Lat, \n                  with(map_data,\n                       sprintf(\"&lt;b&gt;Session %.0f : %.1f h&lt;/b&gt; &lt;br/&gt; %s &lt;br/&gt; %.0f  poissons &lt;br/&gt; hauteur d'eau: %.0f m, %s, %.0f min apr√®s l'√©tal\",\n                               n_ses,\n                               duration,\n                               Time,\n                               nb,\n                               Water_level,\n                               Tide_status,\n                               Tide_time)),\n                  options = popupOptions(maxWidth = 100, minWidth = 50))\n    })\n    \n  })\n  \n}"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html#conclusion-and-future-improvments",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html#conclusion-and-future-improvments",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "Conclusion and future improvments",
    "text": "Conclusion and future improvments\nYou can find a dummy example of this application (not linked to the dropbox account) here. I have been using this application for 1 year without any problems! The data I collected will be presented in the next post.\nIn the coming months, I must find a new free API to replace the actual one. Indeed, the weather API I am using has been bought by Apple and the free requests will be stopped in the following year."
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "",
    "text": "In this previous post, I presented the shiny application I developed to record data about my fishing session. In today‚Äôs post, I will present briefly the changes and updates I made to improve the application. Here are the main changes:"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html#weather-api",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html#weather-api",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "Weather API",
    "text": "Weather API\nSmall changes were made to adapt the former weather function to the new weather API. As this new API do not furnish moon phase data, I decided to compute the moon phase with the oce package:\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(oce)\n\nweather &lt;- function(lat, lon, API_key){\n  url &lt;- paste0(\"api.openweathermap.org/data/2.5/weather?lat=\", lat, \"&lon=\", lon, \"&appid=\", API_key, \"&units=metric\")\n  \n  rep &lt;- GET(url)\n  \n  table &lt;- fromJSON(content(rep, \"text\"))\n  \n  # The weather API don't provide moon phase so I compute it with Oce package\n  moon_phase &lt;- round(moonAngle(t = Sys.Date(),\n                                longitude = as.numeric(lon),\n                                latitude = as.numeric(lat))$illuminatedFraction,\n                      3)\n  \n  \n  current.weather.info &lt;- data.frame(Air_temp = table$main$temp,\n                                     Weather = table$weather$main,\n                                     Atm_pres = table$main$pressure,\n                                     Wind_str = table$wind$speed,\n                                     Wind_dir = table$wind$deg,\n                                     Cloud_cover = table$clouds$all,\n                                     PrecipInt = ifelse(is.null(table$rains$`1h`), 0, table$rains$`1h`),  \n                                     Moon = moon_phase)\n  return(current.weather.info)\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html#river-flow",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html#river-flow",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "River flow",
    "text": "River flow\nI wrote functions to scrap information about the flows of the rivers in which I fish the most on a french website:\n\n# Get and prepare the flow data\nget_Qdata &lt;- function(link){\n  table &lt;- fromJSON(content(GET(link), \"text\"))\n  table &lt;- table$Serie$ObssHydro\n  table &lt;- as.data.frame(table)\n  table$DtObsHydro &lt;- sub(\"T\", \" \", table$DtObsHydro)\n  table$DtObsHydro &lt;- substr(table$DtObsHydro, start = 1, stop = 19)\n  ts &lt;- data.frame(Date = seq.POSIXt(as.POSIXct(range(table$DtObsHydro)[1],'%m/%d/%y %H:%M:%S'), \n                                     as.POSIXct(range(table$DtObsHydro)[2],'%m/%d/%y %H:%M:%S'), by=\"hour\"))\n  \n  table$DtObsHydro &lt;- as.POSIXct(table$DtObsHydro, format = \"%Y-%m-%d %H:%M:%S\")\n  \n  table &lt;- full_join(table, ts, by = c(\"DtObsHydro\" = \"Date\")) %&gt;% arrange(DtObsHydro)\n  return(table)\n}\n\n# Main function to collect river flow \n\nriver_flow &lt;- function(){\n  # Url of website to scrap:\n  url_index &lt;- \"https://www.vigicrues.gouv.fr/services/station.json/index.php\"\n  \n  rep &lt;- GET(url_index)\n  \n  table_index &lt;- fromJSON(content(rep, \"text\"))$Stations%&gt;% \n    na.omit()\n  \n  # I need to add the flow of several rivers to get the flow of the rivers I am interested in:\n  stations &lt;- table_index %&gt;% \n    filter(LbStationHydro %in% c(\"Pontonx-sur-l'Adour\", \"St-Pandelon\", \"Artiguelouve\", \"Escos\",\n                                 \"A√Øcirits [St-Palais]\", \"Cambo-les-Bains\"))\n  \n  base_url &lt;- \"http://www.vigicrues.gouv.fr/services/observations.json?CdStationHydro=\"\n  height_url &lt;- \"&FormatDate=iso\"\n  Q_url &lt;- \"&GrdSerie=Q\"\n  \n  stations &lt;- stations %&gt;% \n    mutate(WL_link = paste0(base_url, CdStationHydro, height_url),\n           Q_link = paste0(WL_link, Q_url))\n  \n  data_Q &lt;- lapply(stations$Q_link, \n                   function(x){get_Qdata(x)})\n  \n  data_Q &lt;- suppressWarnings(Reduce(function(...) merge(..., all = TRUE, by = \"DtObsHydro\"),\n                   data_Q))\n  \n  names(data_Q) &lt;- c(\"Date\", stations$LbStationHydro) \n  \n  data_Q &lt;- data_Q  %&gt;% \n    mutate(hour_of_day = format(Date, \"%Y-%m-%d %H\"))\n  \n  \n  data_Q &lt;- aggregate(.~hour_of_day, data = data_Q, mean, na.rm = TRUE, na.action = na.pass)\n  \n  data_Q &lt;- imputeTS::na_interpolation(data_Q, option = \"linear\")\n  \n  final_data &lt;- data_Q %&gt;% \n    mutate(Adour = `Pontonx-sur-l'Adour` +  `A√Øcirits [St-Palais]` + Artiguelouve + Escos + `St-Pandelon`,\n           Date = as.POSIXct(hour_of_day, tryFormats = \"%Y-%m-%d %H\")) %&gt;% \n    select(Date, `Cambo-les-Bains`, Adour) %&gt;% \n    rename(Nive = `Cambo-les-Bains`)\n  \n  Cur_flow &lt;- data.frame(\"Nive_c\" = final_data[nrow(final_data), 2],\n                         \"Adour_c\" = final_data[nrow(final_data), 3])\n  \n  \n  final_data &lt;- cbind(Cur_flow, final_data) %&gt;% \n    nest(Ts_flow = c(Date, Nive, Adour)) %&gt;% \n    mutate(Ts_flow = paste(Ts_flow))\n\n  return(final_data)\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html#shiny-application",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html#shiny-application",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "Shiny application",
    "text": "Shiny application\nA simplified graph of the new application is showed below:\n\n\n\nSimplified workflow of the new version of application\n\n\n\nUI side\nThe UI side did not change that much, I only removed the tab that displayed fishing data on a map because I wasn‚Äôt using this feature too much:\n\n# Load libraries \nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(googlesheets)\nlibrary(miniUI)\nlibrary(leaflet)\nlibrary(rdrop2)\nSys.setenv(TZ=\"Europe/Paris\")\n\n#Import the functions for weather API and webscrapping \nsuppressMessages(source(\"api_functions.R\"))\n\n\n# Load the dropbox token : \ntoken &lt;&lt;- readRDS(\"token.rds\")\n\n# Minipage for small screens\nui &lt;- miniPage(tags$script('$(document).ready(function () {\n                           navigator.geolocation.getCurrentPosition(onSuccess, onError);\n\n                           function onError (err) {\n                           Shiny.onInputChange(\"geolocation\", false);\n                           }\n\n                           function onSuccess (position) {\n                           setTimeout(function () {\n                           var coords = position.coords;\n                           console.log(coords.latitude + \", \" + coords.longitude);\n                           Shiny.onInputChange(\"geolocation\", true);\n                           Shiny.onInputChange(\"lat\", coords.latitude);\n                           Shiny.onInputChange(\"long\", coords.longitude);\n                           }, 1100)\n                           }\n                           });'),\n               \n               gadgetTitleBar(\"Catch them all\", left = NULL, right = NULL),\n               \n               miniTabstripPanel(\n                 \n                 miniTabPanel(\"Session\", icon = icon(\"sliders\"),\n                              \n                              miniContentPanel(uiOutput(\"UI_sess\", align = \"center\"),\n                                               uiOutput(\"UI\", align = \"center\"))\n                              \n                 )\n               )\n               \n)\n\n\n\nServer side\nSeveral changes were made in the server side to collect data about the lures I used. Now, each time I change my fishing lure, I fill a small form to collect the lure characteristics and it adds a line in a third csv file:\n\nserver &lt;- function(input, output, session){\n  \n  observeEvent(input$go ,{\n    \n  # Read the csv file containing information about fishing session. If a session is running,\n  # display the UI that allows the user to input data about the fish caught. If a session is not started,\n  # display a button to start the session and small survey on lure characteristics.\n    \n    dat &lt;&lt;- drop_read_csv(\"/app_peche/session1.csv\", header = T, stringsAsFactors = F, dtoken = token)\n    \n    # Reactive UI\n    \n    output$UI &lt;- renderUI({\n      \n      if(!is.na(rev(dat$End)[1])){\n        # We now indicate what type of lure we use at the beginning of the session:\n        tagList(\n          selectInput(\"lure1\", \n                      label = \"Type de leurre\",\n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"),\n                      selected = \"shad\",\n                      selectize = FALSE),\n          \n          selectInput(\"color_lure1\", \n                      label = \"Couleur du leurre\",\n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ),\n                      selected = \"naturel\",\n                      selectize = FALSE),\n          \n          selectInput(\"length_lure1\",\n                      label = \"Taille du leurre\",\n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"),\n                      selected = \"petit\",\n                      selectize = FALSE),\n          \n          actionButton(\"go\",\"Commencer session !\"))\n      }else{\n        \n        tagList(actionButton(\"go\",\"End session\"))\n      }\n      \n    })\n    \n    output$UI_sess &lt;- renderUI({\n      \n      if(!is.na(rev(dat$End)[1])){\n        \n        tagList(textInput(\"comments\", label = \"Commentaire avant le d√©but?\", value = \"NA\"))\n        \n      }else{\n        input$catch\n        input$lure\n        tagList(\n          \n          selectInput(\"lure_type\", \n                      label = \"Type de leurre\",\n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"),\n                      selected = \"shad\",\n                      selectize = FALSE),\n          \n          selectInput(\"color_lure\", \n                      label = \"Couleur du leurre\",\n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ),\n                      selected = \"naturel\",\n                      selectize = FALSE),\n          \n          selectInput(\"length_lure\",\n                      label = \"Taille du leurre\",\n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"),\n                      selected = \"petit\",\n                      selectize = FALSE),\n          \n          actionButton(\"lure\",\n                       label = \"Changer de leurre!\"),\n          \n          br(), \n          br(), \n          \n          h4(\"Ajouter une capture\"),\n          \n          selectInput(\"species\", \n                      label = \"Esp√®ces\",\n                      choices = list(\"Bar\" = \"bar\",\n                                     \"Bar mouchet√©\" = \"bar_m\",\n                                     \"Alose\" = \"alose\",\n                                     \"Maquereau\" = \"maquereau\",\n                                     \"Chinchard\" = \"chinchard\"),\n                      selected = \"bar\"),\n          \n          sliderInput(\"length\",\n                      label = \"Taille du poisson\",\n                      value = 25, \n                      min = 0, \n                      max = 80, \n                      step = 1),\n          \n          actionButton(\"catch\",\"Rajoutez cette capture aux stats!\"),\n          \n          br(), \n          br(), \n          \n          textInput(\"comments1\", label = h4(\"Commentaire avant la fin ?\"), value = \"NA\")\n        )\n      }\n    })\n  }, ignoreNULL = F)\n  \n  \n  #If the button is pushed, create the line to be added in the csv file. \n  \n  observeEvent(input$go,{\n    \n    # Two outcomes depending if the session starts or ends. This gives the possibility \n    # to the user to add a comment before starting the session or after ending the session\n    \n    if(!is.na(rev(dat$End)[1])){\n      \n      #Tide + geoloc + Weather\n      c_tide &lt;- tide()\n      geoloc &lt;- c(input$lat,input$long)\n      current.weather.info &lt;- weather(lat = geoloc[1], lon = geoloc[2])\n      river.flow &lt;- river_flow()\n      \n      n_ses &lt;- c(rev(dat$Session)[1] + 1)\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment &lt;- input$comments\n      dat.f &lt;&lt;- cbind(data.frame(n_ses,\n                                 time_beg,\n                                 NA,\n                                 geoloc[2],\n                                 geoloc[1]),\n                      current.weather.info,\n                      c_tide,\n                      river.flow,\n                      comment)\n      names(dat.f) &lt;- names(dat)\n      print(dat.f)\n      final_dat &lt;- rbind(dat, dat.f)\n      \n      lure &lt;- drop_read_csv(\"/app_peche/lure.csv\",\n                            header = T,\n                            stringsAsFactors = F,\n                            dtoken = token)\n      \n      new_lure &lt;- data.frame(n_ses = n_ses,\n                             time = as.character(as.POSIXct(Sys.time())),\n                             type_lure = input$lure1,\n                             color_lure = input$color_lure1,\n                             length_lure = input$length_lure1)\n      \n      new_df &lt;- rbind(lure, \n                      new_lure)\n      \n      write_csv(as.data.frame(new_df), \"lure.csv\")\n      drop_upload(\"lure.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n      \n\n    }else{\n      \n      dat$End[nrow(dat)] &lt;- as.character(as.POSIXct(Sys.time()))\n      dat$Comments[nrow(dat)] &lt;- paste(dat$Comments[nrow(dat)], \"/\", input$comments1)\n      final_dat &lt;- dat \n    }\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(final_dat), \"session1.csv\")\n    \n    # Upload it to dropbox account \n    drop_upload(\"session1.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  # Add a line to the catch csv file whenever a fish is caught\n  observeEvent(input$catch,{\n    caugth &lt;- drop_read_csv(\"/app_peche/catch1.csv\", header = T, stringsAsFactors = F, dtoken = token)\n    \n    catch &lt;- data.frame(n_ses = dat$Session[nrow(dat)],\n                        time = as.character(as.POSIXct(Sys.time())),\n                        species = input$species,\n                        length = input$length)\n    \n    b &lt;- rbind(caugth,catch)\n    \n    write_csv(as.data.frame(b), \"catch1.csv\")\n    drop_upload(\"catch1.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  \n  observeEvent(input$lure,{\n    lure &lt;- drop_read_csv(\"/app_peche/lure.csv\",\n                          header = T,\n                          stringsAsFactors = F,\n                          dtoken = token)\n    \n    new_lure &lt;- data.frame(n_ses = dat$Session[nrow(dat)],\n                        time = as.character(as.POSIXct(Sys.time())),\n                        type_lure = input$lure_type,\n                        color_lure = input$color_lure,\n                        length_lure = input$length_lure)\n    \n    new_df &lt;- rbind(lure, \n               new_lure)\n    \n    write_csv(as.data.frame(new_df), \"lure.csv\")\n    drop_upload(\"lure.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html#conclusion",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html#conclusion",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "Conclusion",
    "text": "Conclusion\nI have tested this new application during two fishing sessions and it has been working like a charm. I can‚Äôt wait to present you my findings at the end of this fishing season !"
  },
  {
    "objectID": "posts/2024-12-17-rainfall-interpolation-deep-learning/index.html",
    "href": "posts/2024-12-17-rainfall-interpolation-deep-learning/index.html",
    "title": "Rainfall with a Precision of 1km¬≤‚ÄîA Myth Becoming Reality?",
    "section": "",
    "text": "Once again, I published a new article on Medium! This time I explore an exciting challenge in agriculture and meteorology: Can we achieve the accuracy of rain gauges while benefiting from the broad coverage of remote sensing? üåßüì°\n\nWhy does spatial rainfall data matter?\nRainfall influences everything from irrigation planning to crop health management. While traditional rain gauges provide highly accurate, localized measurements, they don‚Äôt capture rainfall patterns across larger regions‚Äîcreating blind spots for decision-making. Weather radars and satellites help fill these gaps, but their estimates of ground-level rainfall lack precision.\n\n\nThe challenge and our approach\nAt Sencrop, we‚Äôre tackling this challenge with deep learning. In this post, I introduce an innovative methodology called densification, which merges: - Sparse but precise rainfall observations from our weather station network - Dense but less accurate rainfall estimates from radar and satellite data\nThe goal? Provide high-resolution (1km¬≤) rainfall data anywhere in Europe, with accuracy equal to or better than our station network.\nCurious to see how we‚Äôre making this a reality? Check out the full article here: Rainfall with a precision of 1km¬≤: a myth becoming reality?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.html",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.html",
    "title": "Can R and Shiny make me a better fisherman? Part 2",
    "section": "",
    "text": "In the previous blog article, I described in details how I built a shiny application that stores data about my fishing sessions. In this post, I will explore the data I have collected during the last year.\nTo sum up, my application store the data in two csv files. The first one contains variables related to the fishing conditions at the beginning and at the end of the session such as :\nThe second one contains information about my catches :"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.html#importing-and-cleaning-my-fishing-data",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.html#importing-and-cleaning-my-fishing-data",
    "title": "Can R and Shiny make me a better fisherman? Part 2",
    "section": "Importing and cleaning my fishing data",
    "text": "Importing and cleaning my fishing data\nThe first step of this analysis is to import both csv files and make some transformations.\n\n\nShow the code\n# Change character variables to factor\nsession_data %&lt;&gt;% \nmutate_at(vars(Weather, Tide_status), as.factor)\n\n# Change character variables to factor\ncatch_data %&lt;&gt;% \nmutate_at(vars(species, lure, colour, length_lure), as.factor)\n\n\n¬†\nAfter cleaning and rearranging the data (code hidden below), we are ready to explore graphically our data!\n\n\nShow the code\n# Compute mean conditions (between beg and end session) \n\nmean_weather_cond &lt;- session_data %&gt;% \ngroup_by(Session) %&gt;% \nselect(-c(Long, Lat, Water_level, Tide_time)) %&gt;% \nsummarise_if(is.numeric, mean) \n\n\n# Extract fixed conditions and comments + join with mean cond \n\nfixed_cond_com &lt;- session_data %&gt;% \ngroup_by(Session) %&gt;% \nselect(Session, Comments, Long, Lat, Weather) %&gt;% \nmutate(Comments_parsed = paste(na.omit(Comments), collapse = \"\")) %&gt;% \nselect(-Comments) %&gt;% \nslice(1) %&gt;% \ninner_join(mean_weather_cond, by = \"Session\")\n\n# Create end and beg variables for WL, Time , Tide_time, Tide_status\n\nbeg_end_vars &lt;- session_data %&gt;% \nselect(Session, Status, Water_level, Time, Tide_time, Tide_status) %&gt;% \npivot_wider(names_from = Status,\nvalues_from = c(Time, Water_level,  Tide_time, Tide_status))\n\n\n# Assemble both file and calculate duration\n\ndat_ses &lt;-  inner_join(beg_end_vars,\nfixed_cond_com,\nby = \"Session\")\n\n# Calculate duration of the sessions\n\ndat_ses %&lt;&gt;% \nmutate(duration = round(difftime(Time_end,  Time_beg,  units = \"hours\"),\ndigits = 1))\n\ncatch_cond &lt;- full_join(dat_ses,\ncatch_data, by = c( \"Session\" = \"n_ses\" )) %&gt;% \nmutate(Session = factor(Session, levels = 1:length(dat_ses$Session)))\n\ncatch_cond %&lt;&gt;%\nmutate(Tide_status_ses = paste0(Tide_status_beg, \"_\", Tide_status_end))\n\n# Simplify the Tide status variable\n\ncatch_cond$Tide_status_ses &lt;- sapply(catch_cond$Tide_status_ses , function(x){switch(x, \n\"Up_Dead\" = \"Up\",\n\"Up_Up\" = \"Up\",\n\"Up_Down\" = \"Dead\",\n\"Down_Dead\" = \"Down\",\n\"Down_Up\" = \"Dead\",\n\"Down_Down\"  = \"Down\",\n\"Dead_Dead\" = \"Dead\",\n\"Dead_Up\" = \"Up\",\n\"Dead_Down\" = \"Down\"\n)}, USE.NAMES = F)"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.html#graphical-exploration",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.html#graphical-exploration",
    "title": "Can R and Shiny make me a better fisherman? Part 2",
    "section": "Graphical exploration",
    "text": "Graphical exploration\n\nWhere did I fish ?\nWe can visualize the locations I fished the most by using the leaflet package:\n\n\nShow the code\n# Calculate the number of fish caught by session \nfish_number &lt;-  catch_cond  %&gt;% na.omit() %&gt;% group_by(Session) %&gt;%  summarise(nb = length(Session))\n\n# Dataframe with variables we want to show on the map\nmap_data &lt;- catch_cond %&gt;% \ngroup_by(Session) %&gt;%\nselect(Session, Time_beg, Time_end, Long,\nLat, Water_level_beg, Tide_status_beg, Tide_time_beg, duration) \n\nmap_data &lt;- full_join(map_data, fish_number)\n\nmap_data$nb[is.na(map_data$nb)] &lt;- 0\n\n# Interactive map with Popup for each session\nlibrary(leaflet)\n\nleaflet(map_data, width = \"100%\") %&gt;% addTiles() %&gt;%\naddPopups(lng = ~Long, lat = ~Lat, \nwith(map_data, sprintf(\"&lt;b&gt;Session %.0f : %.1f h&lt;/b&gt; &lt;br/&gt; %s &lt;br/&gt; %.0f  fish &lt;br/&gt; Water level: %.0f m, %s, %.0f min since last peak\",                                         Session, duration,  Time_beg, nb, Water_level_beg, Tide_status_beg, Tide_time_beg)), \noptions = popupOptions(maxWidth = 100, minWidth = 50))\n\n\n\n\n\n\nAs you see I fish mostly in the Nive river that is flowing through Bayonne city.\n\n\nWhen it is best to fish ?\n\nTime of the year\nThe following graph shows the number of fish caught depending on the time of the year :\n\n\nShow the code\ncatch_cond %&gt;% \ngroup_by(Session, Time_beg, .drop = F) %&gt;% \nna.omit() %&gt;% \nsummarise(n_catch = n()) %&gt;% \nright_join(unique(catch_cond[, c(\"Session\", \"Time_beg\")])) %&gt;% \nmutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;%\nggplot(aes(y = n_catch, x =Time_beg)) +\ngeom_point( size = 2) + \n  theme_minimal() + labs(x = \"Date\", y = \"Number of catch\") + scale_x_datetime(date_labels = \"%d/%m/%y\", date_breaks = \"3 months\") \n\n\n\n\n\n\n\n\n\nFrom this graph we see that I didn‚Äôt go fishing during the autumn and winter of 2019, I don‚Äôt have any data. Unfortunately for me, autumn is known to be a great period for sea bass fishing, I must go fishing this year to compensate the lack of data in this season. During winter, fishing is really complicated because the large majority of sea bass are returned to the ocean.\n\n\nTime of the day\nThis graph shows the number of fish I catch depending on the hour of the day :\n\n\nShow the code\ncatch_cond %&gt;% \ngroup_by(Session, Time_beg, .drop = F) %&gt;% \nna.omit() %&gt;% \nsummarise(n_catch = n()) %&gt;% \nright_join(unique(catch_cond[, c(\"Session\", \"Time_beg\")])) %&gt;% \nmutate(n_catch = ifelse(is.na(n_catch), 0, n_catch ), \nhour = format(Time_beg, \"%H\")) %&gt;%\nggplot(aes(y = n_catch, x =hour)) +\ngeom_point( size = 2)  + labs(x = \"Hour\", y = \"Number of catch\")+\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nI mostly fish after work or during evenings. To draw relevant conclusions about the influence of the fishing hour, I have to go fishing at different hours of the day (in the morning for example).\n\n\nThe tide\nThe tide is an important parameter for fishing in estuaries. Let‚Äôs see the effect of the tide current on my catches:\n\n\nShow the code\nlibrary(ggpubr)\n\ngg1 &lt;- catch_cond %&gt;% \n  group_by(Session, Tide_status_ses, .drop = F)  %&gt;%  \n  drop_na() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Tide_status_ses\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;%\n  ggplot(aes(y = n_catch, x = Tide_status_ses, fill = Tide_status_ses)) +\n  geom_boxplot() +\n  labs(x = \"Status of tide current\", y = \"Number of catch\") +\n  theme_minimal()+ theme(legend.position=\"None\")\n\ngg2 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(y = length,x = Tide_status_ses, fill = Tide_status_ses)) +\n  geom_boxplot()+\n  labs(x = \"Status of tide current\", y = \"Length of the fish\") +\n  theme_minimal()+ theme(legend.position=\"None\")\n\nggarrange(gg1, gg2)\n\n\n\n\n\n\n\n\n\nIt seems that the status of the tide current does not influence the number of my catch but influences the length of the fish. I tend to catch bigger fish when the current is going down.\n\n\n\nDoes the moon affect my fishing results?\nA widespread belief among fishermen is that the moon influences greatly the behavior of the fish. Data about the moon phase were available thanks to the weather API, I decided to record this variable to investigate if the belief was true. The two graphs show the number and length of fish depending on the phase of moon (0 corresponding to new moon and 1 to full moon):\n\n\nShow the code\ngg3 &lt;- catch_cond %&gt;% \n  group_by(Session, Moon, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Moon\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Moon)) +\n  geom_point( size = 2) +\n  labs(x = \"Moon phase\", y = \"Number of catch\")+\n  theme_minimal()\n\ngg4 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Moon)) +\n  geom_point( size = 2) +\n  geom_smooth(method=\"lm\", se=T) + \n  labs(x = \"Moon phase\", y = \"Length of the fish\")+\n  theme_minimal()\n\nggarrange(gg3, gg4)\n\n\n\n\n\n\n\n\n\nThe phase of the moon does not seem to influence the number of fish I catch during a session. However, I tend to catch bigger fish the closer we are to the full moon. To confirm this observation, I need to keep going fishing to get more data !\n\n\nDoes the weather affect my fishing results?\nWe can look at the number of fish caught during different weather conditions:\n\n\nShow the code\n# precipitation probability \n\ngg5 &lt;- catch_cond %&gt;% \n  group_by(Session, Preci_prob, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Preci_prob\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Preci_prob)) +\n  geom_point()+\n  labs(x = \"Precipitation prob.\", y = \"Number of catch\")+\n  theme_minimal()\n\n# Atm pressure \n\ngg6 &lt;- catch_cond %&gt;% \n  group_by(Session, Atm_pres, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Atm_pres\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Atm_pres)) +\n  geom_point() +\n  labs(x = \"Atm. pressure\", y = \"Number of catch\")+\n  theme_minimal()\n\n#Air temp\n\ngg7 &lt;- catch_cond %&gt;% \n  group_by(Session, Air_temp, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Air_temp\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Air_temp)) +\n  geom_point() +\n  labs(x = \"Air temp.\", y = \"Number of catch\")+\n  theme_minimal()\n\n\n#Cloud cover\n\ngg8 &lt;- catch_cond %&gt;% \n  group_by(Session, Cloud_cover, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Cloud_cover\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Cloud_cover)) +\n  geom_point() +\n  labs(x = \"Cloud cover\", y = \"Number of catchh\")+\n  theme_minimal()\n\nggarrange(gg5, gg6, gg7, gg8)\n\n\n\n\n\n\n\n\n\nAnd to their length :\n\n\nShow the code\ngg15 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Preci_prob)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\ngg16 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Atm_pres)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\ngg17 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Air_temp)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\n\ngg18  &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Cloud_cover)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\nggarrange(gg15, gg16, gg17, gg18)\n\n\n\n\n\n\n\n\n\nBecause we have limited data and not all weather conditions are covered, it is difficult to draw any conclusions.\n\n\nWhat are the best lures to catch fish ?\nEach time I catch a fish, I fill a form with my shiny application in order to record the characteristics of the lure used. There are different types of lures that have specific swimming patterns, different colors and size. We can represent the number of fish caught depending on the lure characteristics:\n\n\nShow the code\nlevels(catch_cond$colour) &lt;- c(\"clear\", \"natural\", \"dark\")\nlevels(catch_cond$length_lure) &lt;- c(\"large\", \"medium\", \"small\")\n\ngg9 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot( aes(x=lure, fill = lure)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Type of lure\", y = \"\")+\n  theme_minimal()+ \n  theme(legend.position=\"None\")\n\ngg10 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot( aes(x=colour, fill = colour)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Color of the lures\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"BuPu\")+ \n  theme(legend.position=\"None\")\n\ngg11 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot( aes(x=length_lure, fill = length_lure)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Size of the lure\", y = \"\")+\n    scale_fill_brewer(palette=\"Dark2\")+\n  theme_minimal()+ theme(legend.position=\"None\")\n\nannotate_figure(ggarrange(gg9, gg10, gg11, ncol = 3),\n                left = text_grob(\"Number of catch\", rot = 90)\n)\n\n\n\n\n\n\n\n\n\nWe can do the same for the length of fish caught:\n\n\nShow the code\ngg12 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(y = length, x = lure, fill=lure)) +\n  geom_boxplot()+\n  labs(x = \"Type of lure\", y = \"\")+\n  theme_minimal()+ theme(legend.position=\"None\")\n\ngg13 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(y = length, x = colour, fill= colour)) +\n  geom_boxplot()+\n  labs(x = \"Color of the lures\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"BuPu\")+ theme(legend.position=\"None\")\n\ngg14 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(y = length, x = length_lure, fill=length_lure)) +\n  geom_boxplot()+\n  labs(x = \"Size of the lure\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"Dark2\")+ theme(legend.position=\"None\")\n\nannotate_figure(ggarrange(gg12, gg13, gg14, ncol = 3),\n                left = text_grob(\"Length of fish\", rot = 90)\n)\n\n\n\n\n\n\n\n\n\nWith these 6 graphs, we can see that the most successful types of lures for me are the shad and slug types. An honorable mention is the jerkbait type: it only accounts for 2 fish, but 2 big fish (median around 47cm). The colors that worked best for me were clear and natural. For the size of the lure, bigger lures tend to catch bigger fish in average. These conclusions must be taken with a grain of salt because we do not know the time spent with each lure before catching a fish. In addition, I tend to use the same types and colors of lures (habits), I should vary more."
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.html#conclusion",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.html#conclusion",
    "title": "Can R and Shiny make me a better fisherman? Part 2",
    "section": "Conclusion",
    "text": "Conclusion\nAnalyzing my fishing data was very interesting and it brought me some insights on my fishing style! I understood that I was fishing almost the same way with the same habits. Although it seems to be working for me, I have a biased view on how to catch European sea bass. I must use bigger lures to catch bigger fish and I must vary the types of lures used. Indeed, I fish most of the times with slug or shad lures, hence the higher number of fish caught with these types of lures.\nI will keep using the application to gather more data and have a better understanding on my fishing session. I will keep you updated on the results ! :wink:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aur√©lien Callens",
    "section": "",
    "text": "Welcome! I‚Äôm Aur√©lien Callens, a Data Scientist specialized in environmental and geospatial data. Passionate about data science, development, and nature, I enjoy exploring new fields and combining different approaches to solve complex problems using data.\nAfter completing a PhD in applied mathematics on coastal risk modeling, I led several R&D projects, ranging from computer vision for depth prediction on beaches to the fusion of multi-source precipitation data in the meteorology and AgTech sectors. Today, as a freelancer, I assist my clients in solving their data-related challenges.\nMy services:\n\nAnalysis and modeling of environmental & geospatial data\n\nDevelopment of machine learning & deep learning algorithms\n\nFusion and enhancement of multi-source data\n\nIndustrialization and deployment of models\n\nMy goal? To transform your data into concrete and impactful solutions for your business!\nDo you have a project related to geospatial or environmental data? ‚úâÔ∏è Contact me!\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.fr.html",
    "href": "projects/2022-04-01-my-fishing-application/index.fr.html",
    "title": "Mon application de p√™che",
    "section": "",
    "text": "Dans ce projet, j‚Äôai cr√©√© une application Shiny pour enregistrer mes sessions de p√™che dans le but de trouver les conditions m√©t√©orologiques optimales pour attraper plus de poissons. J‚Äôai r√©dig√© plusieurs articles de blog li√©s √† ce projet, n‚Äôh√©sitez pas √† les consulter !"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.fr.html#r√©sum√©",
    "href": "projects/2022-04-01-my-fishing-application/index.fr.html#r√©sum√©",
    "title": "Mon application de p√™che",
    "section": "",
    "text": "Dans ce projet, j‚Äôai cr√©√© une application Shiny pour enregistrer mes sessions de p√™che dans le but de trouver les conditions m√©t√©orologiques optimales pour attraper plus de poissons. J‚Äôai r√©dig√© plusieurs articles de blog li√©s √† ce projet, n‚Äôh√©sitez pas √† les consulter !"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.fr.html#productions",
    "href": "projects/2022-04-01-my-fishing-application/index.fr.html#productions",
    "title": "Mon application de p√™che",
    "section": "Productions",
    "text": "Productions\n\nArticles de blog :\n\nCr√©ation d‚Äôune application Shiny pour stocker mes donn√©es de p√™che\n\nAnalyse exploratoire de mes donn√©es de p√™che\n\nMise √† jour de mon application\n\nAnalyse exploratoire de mes donn√©es de p√™che (Shiny et Plotly)\n\nApplications Shiny\n\nExemple simplifi√© de l‚Äôapplication de p√™che que j‚Äôutilise\n\nApplication Shiny pr√©sentant les r√©sultats de ma saison de p√™che 2021"
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.fr.html",
    "href": "projects/2018-09-01-robust-regression-time-series/index.fr.html",
    "title": "R√©gression robuste pour les s√©ries temporelles pr√©sentant de l‚Äôh√©t√©rosc√©dasticit√©",
    "section": "",
    "text": "Lors de ce stage, j‚Äôai travaill√© sur une nouvelle m√©thode statistique permettant d‚Äôeffectuer une r√©gression robuste pour des s√©ries temporelles pr√©sentant de l‚Äôh√©t√©rosc√©dasticit√©. Nous avons d√©velopp√© et test√© cette m√©thode sur un jeu de donn√©es contenant des mesures de concentration en chlorophylle dans un petit affluent de la Tamise (Royaume-Uni)."
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.fr.html#r√©sum√©",
    "href": "projects/2018-09-01-robust-regression-time-series/index.fr.html#r√©sum√©",
    "title": "R√©gression robuste pour les s√©ries temporelles pr√©sentant de l‚Äôh√©t√©rosc√©dasticit√©",
    "section": "",
    "text": "Lors de ce stage, j‚Äôai travaill√© sur une nouvelle m√©thode statistique permettant d‚Äôeffectuer une r√©gression robuste pour des s√©ries temporelles pr√©sentant de l‚Äôh√©t√©rosc√©dasticit√©. Nous avons d√©velopp√© et test√© cette m√©thode sur un jeu de donn√©es contenant des mesures de concentration en chlorophylle dans un petit affluent de la Tamise (Royaume-Uni)."
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.fr.html#productions",
    "href": "projects/2018-09-01-robust-regression-time-series/index.fr.html#productions",
    "title": "R√©gression robuste pour les s√©ries temporelles pr√©sentant de l‚Äôh√©t√©rosc√©dasticit√©",
    "section": "Productions",
    "text": "Productions\n\nMon m√©moire de Master 2 :\n\nR√©gression robuste pour les s√©ries temporelles pr√©sentant de l‚Äôh√©t√©rog√©n√©it√©\n\nD√©veloppement dans le package rlmDataDriven :\n\nrlmDD_het.R : cette fonction r√©alise une r√©gression robuste prenant en compte les corr√©lations temporelles et l‚Äôh√©t√©rog√©n√©it√©.\n\nwhm.R : cette fonction est l‚Äôimpl√©mentation en R de l‚Äôestimation M pond√©r√©e.\n\nUn article publi√© dans une revue √† comit√© de lecture :\n\nCallens, A., Wang, Y., Fu, L. et al.¬†(2020). Robust Estimation Procedure for Autoregressive Models with Heterogeneity. Environmental Modeling & Assessment, (10.1007/s10666-020-09730-w)"
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.fr.html",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.fr.html",
    "title": "Apprentissage statistique pour l‚Äô√©valuation des risques c√¥tiers",
    "section": "",
    "text": "L‚Äôobjectif de mon travail √©tait de d√©montrer comment les m√©thodes d‚Äôapprentissage statistique peuvent contribuer √† l‚Äôam√©lioration des outils d‚Äô√©valuation des risques c√¥tiers et au d√©veloppement d‚Äôun syst√®me d‚Äôalerte pr√©coce visant √† r√©duire le risque d‚Äôinondation c√¥ti√®re. J‚Äôai travaill√© sur 4 sujets :\n\nRegroupement des r√©gimes m√©t√©orologiques\n\nM√©thode d‚Äôapprentissage automatique pour corriger les pr√©visions de vagues\n\nApprentissage profond pour cr√©er une base de donn√©es d‚Äôimpacts de vagues √† partir d‚Äôimages de surveillance\n\nR√©seau bay√©sien pour am√©liorer la prise de d√©cision en mati√®re de risques c√¥tiers"
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.fr.html#r√©sum√©",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.fr.html#r√©sum√©",
    "title": "Apprentissage statistique pour l‚Äô√©valuation des risques c√¥tiers",
    "section": "",
    "text": "L‚Äôobjectif de mon travail √©tait de d√©montrer comment les m√©thodes d‚Äôapprentissage statistique peuvent contribuer √† l‚Äôam√©lioration des outils d‚Äô√©valuation des risques c√¥tiers et au d√©veloppement d‚Äôun syst√®me d‚Äôalerte pr√©coce visant √† r√©duire le risque d‚Äôinondation c√¥ti√®re. J‚Äôai travaill√© sur 4 sujets :\n\nRegroupement des r√©gimes m√©t√©orologiques\n\nM√©thode d‚Äôapprentissage automatique pour corriger les pr√©visions de vagues\n\nApprentissage profond pour cr√©er une base de donn√©es d‚Äôimpacts de vagues √† partir d‚Äôimages de surveillance\n\nR√©seau bay√©sien pour am√©liorer la prise de d√©cision en mati√®re de risques c√¥tiers"
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.fr.html#productions",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.fr.html#productions",
    "title": "Apprentissage statistique pour l‚Äô√©valuation des risques c√¥tiers",
    "section": "Productions",
    "text": "Productions\n\nMa th√®se de doctorat :\n\nApprentissage statistique pour l‚Äô√©valuation des risques c√¥tiers\n\nPr√©sentation :\n\nPr√©sentation pour ma soutenance de th√®se\n\nDeux articles publi√©s dans des revues √† comit√© de lecture :\n\nCallens, A., Morichon, D., Liria, P., Epelde, I., & Liquet, B. (2021). Automatic Creation of Storm Impact Database Based on Video Monitoring and Convolutional Neural Networks. Remote Sensing, 13(10), 1933, (10.3390/rs13101933).\n\nCallens, A., Morichon, D., Abadie, S., Delpey, M., Liquet, B. (2020). Using Random forest and Gradient boosting trees to improve wave forecast at a specific location. Applied Ocean Research, 104, (10.1016/j.apor.2020.102339).\n\nArticle de blog :\n\nLa fin de mon parcours doctoral"
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.fr.html",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.fr.html",
    "title": "Analyse des for√ßages physiques et climatiques impactant le transport des micropolluants dans l‚Äôestuaire de l‚ÄôAdour",
    "section": "",
    "text": "L‚Äôobjectif de ce travail √©tait de fournir une synth√®se descriptive des for√ßages physiques et climatiques de la plume de l‚ÄôAdour (un fleuve fran√ßais), √† savoir la houle, le vent et le d√©bit. Cette synth√®se vise √† estimer les param√®tres initiaux d‚Äôun mod√®le hydrodynamique de la plume de l‚ÄôAdour, qui sera utilis√© pour mieux comprendre le transport des micropolluants dans cet estuaire."
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.fr.html#r√©sum√©",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.fr.html#r√©sum√©",
    "title": "Analyse des for√ßages physiques et climatiques impactant le transport des micropolluants dans l‚Äôestuaire de l‚ÄôAdour",
    "section": "",
    "text": "L‚Äôobjectif de ce travail √©tait de fournir une synth√®se descriptive des for√ßages physiques et climatiques de la plume de l‚ÄôAdour (un fleuve fran√ßais), √† savoir la houle, le vent et le d√©bit. Cette synth√®se vise √† estimer les param√®tres initiaux d‚Äôun mod√®le hydrodynamique de la plume de l‚ÄôAdour, qui sera utilis√© pour mieux comprendre le transport des micropolluants dans cet estuaire."
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.fr.html#productions",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.fr.html#productions",
    "title": "Analyse des for√ßages physiques et climatiques impactant le transport des micropolluants dans l‚Äôestuaire de l‚ÄôAdour",
    "section": "Productions",
    "text": "Productions\n\nMon m√©moire de Master :\n\nAnalyse des for√ßages physiques et climatiques impactant le transport des micropolluants dans l‚Äôestuaire de l‚ÄôAdour (version fran√ßaise).\n\nApplication Shiny :\n\nD√©couvrez ici ma premi√®re application Shiny (impressionnante mais lente), qui r√©sume une partie de mon premier stage.\n\nUn article publi√© dans une revue √† comit√© de lecture :\n\nMorichon, D., de Santiago, I., Delpey, M., Somdecoste, T., Callens, A., Liquet, B., ‚Ä¶ & Arnould, P. (2018). Assessment of flooding hazards at an engineered beach during extreme events: Biarritz, SW France. Journal of Coastal Research, 85(sp1), 801-805, (10.2112/SI85-161.1)."
  },
  {
    "objectID": "blog.fr.html",
    "href": "blog.fr.html",
    "title": "Mon blog",
    "section": "",
    "text": "Trier par\n       Ordre par d√©faut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus r√©cent\n        \n         \n          Auteur¬∑rice\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPr√©cipitations avec une pr√©cision de 1 km¬≤ : un mythe qui devient r√©alit√© ?\n\n\n\n\n\n\nResearch\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n17 d√©c. 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nExplorer la valeur du t√©l√©d√©tection en agriculture\n\n\n\n\n\n\nResearch\n\n\nEDA\n\n\n\n\n\n\n\n\n\n19 sept. 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôimportance du controle de la qualit√© des donn√©es en M√©t√©orologie\n\n\n\n\n\n\nResearch\n\n\nAnomaly detection\n\n\n\n\n\n\n\n\n\n17 juil. 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nOptimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\nEDA\n\n\nNLP\n\n\n\n\n\n\n\n\n\n21 sept. 2022\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyse des packages R les plus t√©l√©charg√©s\n\n\n\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\n19 juil. 2022\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nR et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 4\n\n\nAnalyse exploratoire de mes donn√©es (shiny et plotly)\n\n\n\nR\n\n\nShiny\n\n\nEDA\n\n\n\n\n\n\n\n\n\n12 avr. 2022\n\n\nAur√©lien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nLa fin de ma th√®se\n\n\n\n\n\n\n\n\n\n\n\n2 oct. 2021\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nR et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 3\n\n\nMise √† jour de l‚Äôapplication\n\n\n\nR\n\n\nShiny\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n1 juin 2021\n\n\nAur√©lien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping sur Aliexpress avec Rselenium\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n18 nov. 2020\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nEst-ce que R et Shiny peuvent faire de moi un meilleur p√™cheur ? Partie 2\n\n\nAnalyse exploratoire de mes donn√©es de p√™che\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\n25 sept. 2020\n\n\nAur√©lien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nR et Shiny peuvent-ils faire de moi un meilleur p√™cheur ? Partie 1\n\n\nCr√©er une application Shiny pour stocker mes donn√©es de p√™che\n\n\n\nR\n\n\nShiny\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n12 sept. 2020\n\n\nAur√©lien Callens\n\n\n\n\n\n\nAucun article correspondant\n\n Retour au sommet"
  }
]